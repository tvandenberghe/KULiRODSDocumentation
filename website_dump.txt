"id","title","body"
1,"","<p>© FWO</p>"
2,"","<p>The VSC-infrastructure consists of two layers. The central Tier-1 infrastructure is designed to run large parallel jobs. It also contains a small accelerator testbed to experiment with upcoming technologies. The Tier-2 layer runs the smaller jobs, is spread over a number of sites, is closer to users and more strongly embedded in the campus networks. The Tier-2 clusters are also interconnected and integrated with each other.
</p>"
3,"","<p>This infrastructure is accessible to all scientific research taking place in Flemish universities and public research institutes. In some cases a small financial contribution is required. Industry can use the infrastructure for a fee to cover the costs associated with this.
</p>"
4,"What is a supercomputer?","<p>A         supercomputer is a very fast and extremely parallel computer. Many of its technological properties are comparable to those of your laptop or even smartphone. But there are also important differences. </p>"
5,"The VSC in Flanders","<p>The VSC is a partnership of five Flemish university associations. The Tier-1 and Tier-2 infrastructure is spread over four locations: Antwerp, Brussels, Ghent and Louvain. There is also a local support office in Hasselt.
</p>"
6,"Tier-1 infrastructure","<p>Central infrastructure for large parallel compute jobs and an experimental accelerator system.</p>"
7,"Tier-2 infrastructure","<p>An integrated distributed infrastructure for smaller supercomputing jobs with varying hardware needs.<br></p>"
8,"Getting access","<p>Who can access, and how do I get my account?</p>"
9,"Tier-1 starting grant","<p>A programme to get a free allocation on the Tier-1 supercomputer to perform the necessary tests to prepare a regular Tier-1 project application.</p>"
10,"Project access Tier-1","<p>A programme to get a compute time allocation on the Tier-1 supercomputers based on an scientific project with evaluation.</p>"
11,"Buying compute time","<p>Without an awarded scientific project, it is possible to buy compute time. We also offer a free try-out so you can test if our infrastructure is suitable for your needs.</p>"
12,"","<p>Need help ? Have more questions ?</p>"
13,"User portal","<p>On these pages, you will find everything that is useful for users of our infrastructure: the user documentation, server status, upcoming training programs and links to other useful information on the web.<br></p>"
15,"","<p>Below we give information about current downtime (if applicable) and planned maintenance of the various VSC clusters.</p>"
23,"","<p>There is no clear agreement on the exact definition of the term ‘supercomputer’. Some say a supercomputer is a computer with at least 1% of the computing power of the fastest computer in the world. But according to this definition, there are currently only a few hundred supercomputers in the world. <a href=\"https://www.top500.org/\">The TOP500 list</a> is a list of the supposedly 500 fastest computers in the world, updated twice a year.
</p><p>One could take 1‰ of the performance of the fastest computer as the criterion, but it is an arbitrary criterion. Stating that a supercomputer should perform at least X trillion computations per second, is not a useful definition. Because of the fast evolution of the technology, this definition would be outdated in a matter of years. The first smartphone of a well-known manufacturer launched in 2007 had about the same computing power and more memory than the computer used to predict the weather in Europe 30 years earlier.
</p><p>So what is considered as a ‘supercomputer’ is very time-bound, at least in terms of absolute compute power. So let us just agree that a supercomputer is a computer that is hundreds or thousands times faster than your smartphone or laptop.
</p><p>But is a supercomputer so different from your laptop or smartphone? Yes and no. Since roughly 1975 the key word in supercomputing is parallelism. But this also applies for your PC or smartphone. PC processor manufacturers started to experiment with simple forms of parallelism at the end of the nineties. A few years later the first processors appeared with multiple cores that could perform calculations independently from each other. A laptop has mostly 2 or 4 cores and modern smartphones have 2, 4 or in some rare cases 8 cores.  Although it must be added that they are a little slower than the ones on a typical laptop.
</p><p>Around 1975 manufacturers started to experiment with vector processors. These processors perform the same operation to a set of numbers simultaneously. Shortly thereafter supercomputers with multiple processors working independently from each other, appeared on the market. Similar technologies are nowadays used in the processor chips of laptops and smartphones. In the eighties, supercomputer designers started to experiment with another kind of parallelism. Several rather simple processors - this was sometimes just standard PC processors like the venerable Intel 80386 were linked together with fast networks and collaborated to solve large problems. These computers were cheaper to develop, much simpler to build, but required frequent changes to the software.
</p><p>In modern supercomputers, parallelism is pushed to extremes. In most supercomputers, all forms of parallelism mentioned above are combined at an unprecedented scale and can take on extreme forms. All modern supercomputers rely on some form of vector computing or related technologies and consist of building blocks - <i>nodes</i> - uniting tens of cores and interconnecting through a fast network to a larger whole. Hence the term ‘compute cluster’ is often used.
</p><p>Supercomputers must also be able to read and interpret data is ‘at a very high speed. Here the key word is also parallellism. Many supercomputers have several network connections to the outside world. Their permanent storage system consists of hundreds or even thousands of hard disks or SSDs linked together to one extremely large and extremely fast storage system. This type of technology has probably not influenced significantly the development of laptops as it would not be very practical to carry a laptop around with 4 hard drives. Yet this technology does appear to some extent in modern, fast SSD drives in some laptops and smartphones. The faster ones use several memory chips in parallel to increase their performance and it is a standard technology in almost any server storing data.
</p><p>As we have already indicated to some extent in the text above, a supercomputer is more than just hardware. It also needs properly written software. or Java program you wrote during your student years will not run a 10. 000 times faster because you run it on a supercomputer. On the contrary, there is a fair chance that it won't run at all or run slower than on your PC. Most supercomputers - and all supercomputers at the VSC - use a variant of the Linux operating system enriched with additional software to combine all compute nodes in one powerful supercomputer. Due to the high price of such a computer, you're rarely the only user but will rather share the infrastructure with others.
</p><p>So you may have to wait a little before your program runs. Furthermore your monitor is not directly connected to the supercomputer. Proper software is also required here with your application software having to be adapted to run well on a supercomputer. Without these changes, your program will not run much faster than on a regular PC. You may of course still run hundreds or thousands copies simultaneously, when you for example wish to explore a parameter space. This is called ‘capacity computing’.
</p><p>If you wish to solve truly large problems within a reasonable timeframe, you will have to adapt your application software to maximize every form of parallellism within a modern supercomputer and use several hundreds, or even thousands, of compute cores simultaneously to solve one large problem. This is called ‘capability computing’. Of course, the problem you wish to solve has to be large enough for this approach to make sense. Every problem has an intrinsic limit to the speedup you can achieve on a supercomputer. The larger the problem, the higher speedup you can achieve.
</p><p>This also implies that a software package that was cutting edge in your research area 20 years ago, is unlikely to be so anymore because it is not properly adapted to modern supercomputers, while new applications exploit supercomputers much more efficiently and subsequently generate faster, more accurate results.
</p><p>To some extent this also applies to your PC. Here again you are dealing with software  exploiting the parallelism of a modern PC quite well or with software that doesn't. As a ‘computational scientist’ or supercomputer user you constantly have to be open to new developments within this area. Fortunately, in most application domains, a lot of efficient software already exists which succeeds in using all the parallellism that can be found in modern supercomputers.
</p>"
25,"","<p>The successor of Muk is expected to be installed in the spring 2016.</p><p>There is also a small test cluster for experiments with accellerators (GPU and Intel Xeon Phi) with a view to using this technology in future VSC clusters.</p><h2>The Tier-1 cluster Muk</h2><p>The Tier-1 cluster Muk has 528 computing nodes, each with two 8-core Intel Xeon processors from the Sandy Bridge generation (E5-2670, 2.6 GHz). Each node features 64 GiB RAM, for a total memory capacity of more than 33 TiB. The computing nodes are connected by an FDR InfiniBand interconnect with a fat tree topology. This network has a high bandwidth (more than 6,5GB / s per direction per link) and a low latency. The storage is provided by a disk system with a total disk capacity of 400 TB and a peak bandwidth of 9.5 GB / s.</p><p>The cluster achieves a peak performance of more than 175 Tflops and a Linpack performance of 152.3 Tflops. With this result, the cluster was for 5 consecutive periods in the Top500 list of fastest supercomputers in the world:</p><table> <tbody><tr> <td> <p>List</p> </td> <td> <p>06/2012</p> </td> <td> <p>11/2012</p> </td> <td> <p>06/2013</p> </td> <td> <p>11/2013</p> </td> <td> <p>06/2014</p> </td> </tr> <tr> <td> <p>Position</p> </td> <td> <p>118</p> </td> <td> <p>163</p> </td> <td> <p>239</p> </td> <td> <p>306</p> </td> <td> <p>430</p> </td> </tr> </tbody></table><p>In November 2014 the cluster fell just outside the list but still took 99% of the performance of the system in place 500.</p><h2>Accellerator testbed</h2><p>In addition to the tier-1 cluster Muk, the VSC has an experimental GPU / Xeon Phi cluster. 8 nodes in this cluster have 2 K20x nVidia GPUs with accompanying software stack, and 8 nodes are equipped with two Intel Xeon Phi 5110P (\"Knight's Corner\" generation) boards. The nodes are interconnected by means of a QDR InfiniBand network. For practical reasons, these nodes were integrated into the KU Leuven / Hasselt University Tier-2 infrastructure.</p><h2>Software</h2><p>Like on all other VSC-clusters, the operating system of Muk is a variant of Linux, in this case Scientific Linux which in turn based on Red Hat Linux. The system also features a comprehensive stack of software development tools which includes the GNU and Intel compilers, debugger and profiler for parallel applications and different versions of OpenMPI and Intel MPI.</p><p>There is also an extensive set of freely available applications installed on the system. More software can be installed at the request of the user. Users however have to take care of the software licenses when the software is not freely available, and therefore also for the financing of that license.</p><p><a href=\"/cluster-doc/software/tier1-muk\">Detailed overview of the installed software</a></p><h2>Access to the Tier-1 system</h2><p>Academic users can access the Tier-1 cluster Muk through a project application. There are two types of project applications</p><ul><li>The Tier-1 starting grant of up to 100 node days to test and / or to optimize software, typically with a view to a regular request for computing time. There is a continuous assessment process for this project type.<br><a href=\"/en/access-and-infrastructure/tier1-starting-grant\">Learn more</a></li> <li>The regular project application, for allocations between 500 and 5000 node days. The applications are assessed on scientific excellence and technical feasibility by an evaluation committee of foreign experts. There are three cut-off dates a year at which the submitted project proposals are evaluated. The users are also expected to pay a small contribution towards the cost.<br><a href=\"/en/access-and-infrastructure/project-access-tier1\">Learn more</a></li></ul><p>To use the GPU / Xeon Phi cluster it is sufficient to contact the <a href=\"/en/about-vsc/contact\">HPC coordinator of your institution</a>.</p><p>Industrial users and non-Flemish research institutions and not-for-profit organizations can also <a href=\"/en/access-and-infrastructure/access-industry\">purchase computing time on the Tier-1 Infrastructure</a>. For this you can contact the <a href=\"/en/about-vsc/contact\">Hercules Foundation</a>.</p>"
27,"","<p>The VSC does not only rely on the Tier-1 supercomputer to respond to the need for computing capacity. The HPC clusters of the University of Antwerp, VUB, Ghent University and KU Leuven constitute the VSC Tier-2 infrastructure, with a total computing capacity of 416.2 TFlops. Hasselt University invests in the HPC cluster of Leuven. Each cluster has its own specificity and is managed by the university’s dedicated HPC/ICT team. The clusters are interconnected with a 10 Gbps BELNET network, ensuring maximal cross-site access to the different cluster architectures. For instance, a VSC user from Antwerp can easily log in to the infrastructure at Leuven.<br>
</p><h2>Infrastructure</h2><ul>
	<li><a href=\"/infrastructure/hardware/hardware-ua\">The Tier-2 of the University of Antwerp</a> consists of a cluster with 168 nodes, accounting for 3.360 cores (336 processors) and 75 TFlops. Storage capacity is 100 TB. By the spring of 2017 a new cluster will gradually becoming available, containing 152 regular compute nodes and some facilities for visualisation and to test GPU-computing and Xeon Phi computing.</li>
	<li><a href=\"/infrastructure/hardware/hardware-vub\">The Tier-2 of VUB (Hydra)</a> consists of 3 clusters of successive generations of processors with a peak capacity of 75 TFlops (estimated). The total storage capacity is 446 TB. It has a relatively large memory per computing node and is therefore best fit for computing jobs that require a lot of memory per node or per core. This configuration is complemented by a High Troughput Computing (HTC) grid infrastructure.</li>
	<li><a href=\"/infrastructure/hardware/hardware-ugent\">The Tier-2 of Ghent University (Stevin)</a> represents a capacity of 226 TFlops (11.328 cores over 568 nodes) and a storage capacity of 1,430 TB. It is composed of several clusters, 1 of which is intended for single-node computing jobs and 4 for multi-node jobs. One cluster has been optimized for memory-intensive computing jobs and BigData problems.</li>
	<li><a href=\"/infrastructure/hardware/hardware-kul\">The joint KU Leuven/UHasselt Tier-2</a> housed by KU Leuven focuses on small capability computing and tasks requiring a fairly high disk bandwidth. The infrastructure consists of a thin node cluster with 7.616 cores and a total capacity of  230 TFlops. A shared memory system with 14 TB of RAM and 640 cores yields an additional 12 TFlops. A total storage of 280 TB provides the necessary I/O capacity. Furthermore, there are a number of nodes with accellerators (including the GPU/Xeon Phi cluster purchased as an experimental tier-1 setup) and 2 visualization nodes.</li></ul><h2>More information</h2><p>A more detailed description of the complete infrastructure is available in the \"<a href=\"/en/infrastructure/hardware\">Available hardware</a>\" section of the <a href=\"/en/user-portal\">user portal</a>.</p><ul>
</ul><!--<h2>Further detail</h2>
<p>This page only presents a high-level view of the infrastructure. More details, including all technical data needed to log in to the machines or starting jobs can be found in the section “<a href=\"/infrastructure/hardware\">Available hardware</a>” of the documentation on the <a href=\"/en/user-portal\">User Portal</a>.
</p>-->"
37,"","<p>Computational science has - alongside experiments and theory -  become the fully fledged third pillar of science. Supercomputers offer unprecedented opportunities to simulate complex models and as such to test theoretical models against reality. They also make it possible to extract valuable knowledge from massive amounts of data.
</p>
<p>For many calculations, a laptop or workstation is no longer sufficient. Sometimes dozens or hundreds of CPU cores and hundreds of gigabytes or even terabytes of RAM-memory are necessary to produce an acceptable solution within a reasonable amount of time.
</p>
<h2>Our offer</h2>
<p>An overview of our services:
</p>
<ul>
	<li>Access to a variety of <strong>supercomputing infrastructure</strong>, suited for many applications.
	</li>
	<li><strong>Guidance and advice</strong> when determining whether your software is suited to our infrastructure.
	</li>
	<li><strong>Training</strong> (from beginner to advanced level) on the use of supercomputers. In this training all aspects are covered: how to run a program on a supercomputer, how to develop software, and for some application domains even how to use a couple of popular packages.
	</li>
	<li><strong>Support
	</strong>with optimizing the use of your infrastructure.
	</li>
	<li><strong>A wide range of free software.</strong> When using commercial software it is the responsibility of the user to take care of a license with a number of packages as an exception to this. For these packages we ourselves are responsible to ensure optimal running.
	</li>
</ul>
<h2>More information?</h2>
<p>More information can be found in our <a href=\"/en/education-and-trainings\">training section</a> and <a href=\"/en/user-portal\">user portal</a>.
</p>"
41,"","<p>Not only have supercomputers changed scientific research in a fundamental way, they also enable the development of new, affordable products and services which have a major impact on our daily lives.
</p><h2>Not only have supercomputers changed scientific research in a fundamental way  ...</h2><p>Supercomputers are indispensable for scientific research and for a modern R&D environment. ‘Computational Science’ is - alongside theory and experiment - the third fully fledged pillar of science. For centuries, scientists used pen and paper to develop new theories based on scientific experiments. They also set up new experiments to verify the predictions derived from these theories (a process often carried out with pen and paper). It goes without saying that this method was slow and cumbersome. </p><p>As an astronomer you can not simply make Jupiter a little bigger to see what effect this would lager size would have on our solar system. As a nuclear scientist it would be difficult to deliberately lose control over a nuclear reaction to ascertain the consequences of such a move. (Super)computers can do this and are indeed revolutionizing science. </p><p>Complex theoretical models - too advanced for ‘pen and paper’ results - are simulated on computers. The results they deliver, are then compared with reality and used for prediction purposes. Supercomputers have the ability to handle huge amounts of data, thus enabling experiments that would not be achievable in any other way. Large radio telescopes or the LHC particle accelerator at CERN could not function without supercomputers processing mountains of data.</p><h2>… but also the industry and out society</h2><p>But supercomputers are not just an expensive toy for researchers at universities. Numerical simulation also opens up new possibilities in industrial R&D. For example in the search for new medicinal drugs, new materials or even the development of a new car model. Biotechnology also requires the large data processing capacity of a supercomputer. The quest for clean energy, a better understanding of the weather and climate evolution, or new technologies in health care all require a powerful supercomputer. </p><p>Supercomputers have a huge impact on our everyday lives. Have you ever wondered why the showroom of your favourite car brand contains many more car types than 20 years ago? Or how each year a new and faster smartphone model is launched on the market? We owe all of this to supercomputers.</p>"
45,"","<p>In the past few decades supercomputers have not only revolutionized scientific research but have also been used increasingly by businesses all over the world to accelerate design, production processes and the development of innovative services.
</p><h2>Situation</h2><p>Modern         microelectronics has created many new opportunities. Today powerful supercomputers enable us to collect and process huge amounts of data. Complex systems can be studied through numerical simulation without having to build a prototype or set up a scaled experiment beforehand. All this leads to a quicker and cheaper design of new products, cost-efficient processes and innovative services. To support this development in Flanders, the Flemish Government founded in late 2007 the Flemish Supercomputer Center (VSC) as a partnership between the government and Flemish university associations. The accumulated expertise and infrastructure are assets we want to make available to the industry.
</p><h2>Technology Offer</h2><p>A collaboration with the VSC offers your company a good number of benefits.
</p><ul>
	<li>Together
we will identify which expertise within the Flemish universities and their
associations is appropriate for you when rolling out High Performance Computing
(HPC) within your company.
	</li>
	<li>We
can also assist with the technical writing of a project proposal for financing for example through  the IWT (Agency for
Innovation by Science and Technology).
	</li>
	<li>You
can participate in courses on HPC, including tailor-made courses provided by the VSC.
	</li>
	<li>You
will have access to a supercomputer infrastructure with a dedicated, on-site
team assisting you during the start-up phase.
	</li>
	<li>As
a software developer, you can also deploy HPC software technologies to develop
more efficient software which makes better use of modern hardware.
	</li>
	<li>A
shorter turnaround time for your simulation or data study boosts productivity
and increases the responsiveness of your business to new developments.
	</li>
	<li>The
possibility to carry out more detailed simulations or to analyse larger amounts
of data can yield new insights which in turn lead to improved products and more
efficient processes.
	</li>
	<li>A
quick analysis of the data collected during a production process helps to
detect and correct abnormalities early on.
	</li>
	<li>Numerical
simulation and virtual engineering reduce the number of prototypes and
accelerate the discovery of potential design problems. As a result you are able
to market a superior product faster and cheaper. 
	</li>
</ul><h2>About the VSC</h2><p>The VSC was launched in late 2007 as a collaboration between the Flemish Government and five Flemish university associations. Many of the VSC employees have a strong technical and scientific background. Our team also collaborates with many research groups at various universities and helps them and their industrial partners with all aspects of infrastructure usage.
</p><p>Besides a competitive infrastructure, the VSC team also offers full assistance with the introduction of High Performance Computing within your company.
</p>"
49,"","<p>The Flemish Supercomputer Centre (<strong>VSC</strong>) is a virtual centre making supercomputer infrastructure available for both the <strong>academic</strong> and <strong>industrial</strong> world. This centre is managed by the Research Foundation - Flanders (FWO) in partnership with the five Flemish university associations.<br></p>"
51,"HPC for academics","<p>With HPC-technology you can refine your research and gain new insights to take your research to new heights.
<br><br><br></p>"
57,"","<p>You can fix this yourself in a few easy steps via the <a href=\"https://account.vscentrum.be/\">account management web site</a>.</p><p>There are two ways in which you may have messed up your keys:</p><ol class=\"list--ordered\"><li>The keys that were stored in the <code>.ssh</code> subdirectory of your home directory on the cluster were accidentally deleted, or the <code>authorized_keys</code> file was accidentally deleted:<ol><li>Go to <a href=\"https://account.vscentrum.be/\">account.vscentrum.be</a></li><li>Choose your institute and log in.</li><li>At the top of the page, click 'Edit Account'.</li><li>Press the 'Update' button on that web page.</li><li>Exercise some patience, within 30 minutes, your account should be accessible again.</li></ol></li><li>You deleted your (private) keys on your own computer, or don't know the passphrase anymore<ol><li>Generate a new public/private key pair. Follow the procedure outlined in the client sections for <a href=\"/client/linux/keys-openssh\" class=\"internal-link\">Linux</a>, <a href=\"/client/windows/keys-putty\" class=\"internal-link\">Windows</a> and <a href=\"/client/macosx/keys-openssh\" class=\"internal-link\">macOS (formerly  OS X)</a>.</li><li>Go to <a href=\"https://account.vscentrum.be/\">account.vscentrum.be</a></li><li>Choose your institute and log in.</li><li>At the top of the page, click 'Edit Account'.</li><li>Upload your new public key adding it in the 'Add Public Key' section of the page. Use 'Browse...' to find your public key, press 'Add' to upload it.</li><li>You may now delete the entry for the \"lost\" key if you know which one that is, but this is not crucial.</li><li>Exercise some patience, within 30 minutes, your account should be accessible again.</li></ol></li></ol>"
59,"","<p>Before you can really start using one of the clusters, there are several things you need to do or know:</p><ol>
    <li>You need to log on to the cluster via an ssh-client to one of the login nodes. This will give you a command line. The software you'll need to use on your client system depends on its operating system:
    <ul>
        <li><a href=\"/client/windows#connecting\">Windows</a></li>
        <li><a href=\"/client/linux\">Linux</a></li>
        <li><a href=\"/client/macosx\">macOS/OS X</a></li>
    </ul>
    </li>
    <li>Your account also comes with a certain amount of data storage capacity in at least three subdirectories on each cluster. You'll need to familiarise yourself with
    <ul>
        <li>the storage policies: <a href=\"/cluster-doc/access-data-transfer/where-store-data\">where should which data be stored?</a>,</li>
        <li>and the <a href=\"/cluster-doc/account-management/managing-disk-usage\">tools to check your disk usage</a>.</li>
    </ul>
    </li>
    <li>Before you can do some work, you'll have to transfer the files that you need from your desktop or department to the cluster. At the end of a job, you might want to transfer some files back. The preferred way to do that, is by using an sftp client. It again requires some software on your client system which depends on its operating system:
    <ul>
        <li><a href=\"/client/windows#connecting\">Windows</a></li>
        <li><a href=\"/client/linux\">Linux</a></li>
        <li><a href=\"/client/macosx\">macOS/OS X</a></li>
    </ul>
    </li>
    <li>Optionally, if you wish to use programs with a graphical user interface, you'll need an X server on your client system. Again, this depends on the latter's operating system:
    <ul>
        <li><a href=\"/client/windows\">Windows</a></li>
        <li><a href=\"/client/linux\">Linux</a></li>
        <li><a href=\"/client/macosx\">macOS/OS X</a></li>
    </ul>
    </li>
    <li>Often several versions of software packages and libraries are installed, so you need to select the ones you need. To manage different versions efficiently, the VSC clusters use so-called modules, so you'll need to <a href=\"/cluster-doc/software/modules\">select and load the modules that you need</a>.</li>
</ol><p>Logging in to the login nodes of your institute's cluster may not work if your computer is not on your institute's network (e.g., when you work from home). In those cases you will have to <a href=\"/cluster-doc/access-data-transfer/vpn\">set up a VPN (Virtual Private Network) connection</a> if your institute provides this service.</p>"
61,"","<h2>What is a group?</h2><p>The concept of group as it is used here is that of a POSIX group and is a user management concept from the Linux OS (and many other OSes, not just UNIX-like systems). Groups are a useful concept to control access to data or programs for groups of users at once, using so-called group permissions. Three important use cases are:</p><ol><li>Controlling access to licensed software, e.g., when one or only some research groups pay for the license</li><li>Creating a shared subdirectory to collaborate with several VSC-users on a single project</li><li>Controlling access to a project allocation on clusters implementing a credit system (basically all clusters at KU Leuven)</li></ol><p>VSC groups are managed without any interaction from the system administrators. This provides a highly flexible way for users to organise themselves. Each VSC group has members and moderators:
</p><ul>
	<li>A user can become a member of a group after a moderator approves it. As a regular user, you can check all groups you belong to on <a href=\"https://account.vscentrum.be/\">the VSC account management web site account.vscentrum.be</a>.</li>
	<li>A moderator can add/delete members <em>and</em> moderators
	<ul>
		<li>When you create a new group, you become both the first member and moderator of that group.</li>
	</ul>
	</li>
</ul><p><strong>Warning:</strong> You should not exaggerate in creating new groups. Mounting file systems over NFS doesn't work properly if you belong to more than 32 different groups, and so far we have not found a solution. This happens when you log on to a VSC cluster at a different site.</p><h2>Managing groups</h2><h3><a name=\"view-groups\"></a>Viewing the groups you belong to</h3><ul>
	<li>Go to <a href=\"https://account.vscentrum.be/\">the VSC account management web site</a></li>
	<li>Click on \"View groups\"</li>
</ul><p>You will in fact see that you always belong to at least two groups depending on the institution from which you have your VSC account.
</p><h3><a name=\"join-group\"></a>Join an existing group</h3><ul>
	<li>Go to <a class=\"free external\" href=\"https://account.vscentrum.be/\" rel=\"nofollow\">the VSC account management web site</a></li>
	<li>Click on \"New group\"</li>
	<li>Fill in the name of the group
	<ul>
		<li>The name of the group will automatically begin with the first letter of the hosting institute (a for Antwerp, b for Brussels, g for Ghent, l for Leuven)</li>
		<li>If the name is wrong, it will treat the request as a new group</li>
	</ul>
	</li>
	<li>In the message field, describe who you are to motivate the request, so the moderator knows who is making the request
	<ul>
		<li>Moderators will deny all unclear requests</li>
	</ul>
	</li>
</ul><h3><a name=\"create-group\"></a>Create new group</h3><ul>
	<li>Go to <a class=\"free external\" href=\"https://account.vscentrum.be/\" rel=\"nofollow\">the VSC account management web site</a></li>
	<li>Click on \"New group\"</li>
	<li>Fill in the group name</li>
	<li>You will receive a confirmation email</li>
	<li>After the confirmation, you are now member and moderator of the new group</li>
</ul><h2>Working with file and directory permissions</h2><ul>
	<li>The <code>chgrp</code> (from <strong>change group</strong>) command is used by users on Unix-like systems to change the group associated with a computer file. General syntax:
	<pre>chgrp [options] group target1 [target2 ..]</pre>
	</li>
	<li>The <code>chmod</code> command (abbreviated from <strong>ch</strong>ange <strong>mod</strong>e) can change file system modes of files and directories. The modes include permissions and special modes. General syntax:
	<pre>chmod [options] mode[,mode] file1 [file2 ...]</pre>
	</li>
	<li>Hints:
	<ul>
		<li>To view what the permissions currently are, type:
		<pre>$ ls -l file</pre>
		</li>
		<li><tt>-R</tt>: Changes the modes of directories and files recursively.</li>
		<li>
		Setting the setgid permission on a directory (chmod g+s) causes new files and subdirectories created within it to inherit its groupID, rather than the primary groupID of the user who created the file (the ownerID is never affected, only the groupID). Newly created subdirectories inherit the setgid bit. Note that setting the setgid permission on a directory only affects the groupID of new files and subdirectories created after the setgid bit is set, and is not applied to existing entities. Setting the setgid bit on existing subdirectories must be done manually, with a command such as the following:
		<pre>[user@foo]# find /path/to/directory -type d -exec chmod g+s '{}' \\;</pre>
		</li>
	</ul>
	</li>
</ul>"
63,"","<table>
<tbody>
<tr>
	<td>
		<h2>Total disk space used on filesystems with quota</h2>
		<p>On filesystems with 'quota enabled', you can check the amount of disk space that is available for you, and the amount of disk space that is in use by you. Unfortunately, there is not a single command that will give you that information for all file systems in the VSC.
		</p>
		<ul>
			<li><code>quota</code> is the standard command to request your disk quota. Its output is in 'blocks', but can also be given in MB/GB if you use the '-s' option.</li>
			<li>But it does not work on GPFS file systems. On those you have to use <code>mmlsquota</code>. This is the case for the scratch space at the KU Leuven or on the Tier-1.</li>
			<li>On some clusters, these commands are currently disabled.</li>
			<li>Also, using these commands on another cluster than the one in your home institution, will fail to return information about the quota on your VSC_HOME and VSC_DATA directories and will show you the quota for your VSC_SCRATCH directory on that system.</li>
		</ul>
		<pre>quota -s
Disk quotas for user vsc31234 (uid 123456):
  Filesystem  blocks   quota   limit   grace   files   quota   limit   grace
nas2-ib1:/mnt/home
                648M   2919M   3072M            3685       0       0
nas2-ib1:/mnt/data
              20691M  24320M  25600M            134k       0       0
nas1-ib1:/mnt/site_scratch
                   0  24320M  25600M               1       0       0<br>
		</pre>
		<p>Each line represents a file system you have access to, $VSC_HOME, $VSC_DATA, and, for this particular example, $VSC_SCRATCH_SITE. The blocks column shows your current usage, quota is the usage above which you will be warned, and limit is \"hard\", i.e., when your usage reaches this limit, no more information can be written to the file system, and programs that try will fail.</p><p>Some file systems have limits on the number of files that can be stored, and those are represented by the last four columns. The number of files you currently have is listed in the column files, quota and limit represent the soft and hard limits for the number of files.</p>

		<h2>Diskspace used by individual directories</h2>
		<p>The command to check the size of all subdirectories in the current directory is \"du\":
		</p>
		<pre>$ du -h
4.0k      ./.ssh
0       ./somedata/somesubdir
52.0k   ./somedata
56.0k   .
		</pre>
		<p>This shows you first the aggregated size of all subdirectories, and finally the total size of the current directory \".\" (this includes files stored in the current directory). The -h option ensures that sizes are displayed in human readable form, omitting it will show sizes in bytes.</p>
		<p>If the number of lower level subdirectories starts to grow too big, you may not want to see the information at that depth; you could just ask for a summary of the current directory:
		</p>
		<pre>du -s 
54864 .<br>
		</pre>
		<p>If you want to see the size of any file or top level subdirectory in the current directory, you could use the following command:
		</p>
		<pre>du -s *
12      a.out
3564    core
4       mpd.hosts
51200   somedata
4       start.sh
4       test
		</pre>
		<p>Finally, if you don't want to know the size of the data in your current directory, but in some other directory (eg. your data directory), you just pass this directory as a parameter. If you also want this size to be \"human readable\" (and not always the total number of kilobytes), you add the parameter \"-h\":
		</p>
		<pre>du -h -s $VSC_DATA/*
50M     /data/leuven/300/vsc30001/somedata
		</pre>
	</td>
	<td>
		<dl>
		</dl>
	</td>
</tr>
</tbody>
</table>"
65,"","<ul>
    <li><a href=\"/support/tut-book/hpc-glossary\">HPC glossary</a>: Terms often used in HPC</li><li><a href=\"/support/tut-book/vsc-tutorials\">VSC tutorials</a>: Our own tutorial texts, used in some of the introductory courses</li>
    <li><a href=\"/support/tut-book/books\">A list of books</a> from general introduction to specific technologies</li>
    <li><a href=\"/support/tut-book/web-tutorials\">Freely available tutorials on the web</a></li>
</ul>"
67,"","<ul>
    <li><strong>HPC cluster</strong>: relatively tightly coupled collection of compute nodes, the interconnect typically allows for high bandwidth, low latency communication. Access to the cluster is provided through a login node. A resource manager and scheduler provide the logic to schedule jobs efficiently on the cluster. A detailed description of the <a href=\"/infrastructure/hardware\">VSC clusters and other hardware</a> is available.</li>
    <li><strong>Compute node</strong>: an individual computer, part of an HPC cluster.  Currently most compute node have two sockets, each with a single CPU, volatile working memory (RAM), a hard drive, typically small, and only used to store temporary files, and a network card. The hardware specifications for the various <a href=\"/infrastructure/hardware\">VSC compute nodes</a> is available.</li>
    <li><strong>CPU</strong>: Central Processing Unit, the chip that performs the actual computation in a compute node.  A modern CPU is composed of numerous cores, typically 8 or 10.  It has also several cache levels that help in data reuse.</li>
    <li><strong>Core</strong>: part of a modern CPU.  A core is capable of running processes, and has its own processing logic and floating point unit.  Each core has its own level 1 and level 2 cache for data and instructions. Cores share last level cache.</li>
    <li><strong>Cache</strong>: a relatively small amount of (very) fast memory (when compared to regular RAM), on the CPU chip.  A modern CPU has three cache level, L1 and L2 are specific to each core, while L3 (also referred to as Last Level Cache, LLC) is shared among all the cores of a CPU.</li>
    <li><strong>RAM</strong>: Random Access Memory used as working memory for the CPUs.  On current hardware, the size of RAM is expressed in gigabytes (GB). The RAM is shared between the two CPUs on each of the sockets.  This is volatile memory in the sense that once the process that creates the data ends, the data in the RAM is no longer available. The complete RAM can be accessed by each core.</li>
    <li><strong>Walltime</strong>: the actual time an application runs (as in clock on the wall), or is expected to run. When submitting a job, the walltime refers to the maximum amount of time the application can run, i.e., the requested walltime.  For accounting purposes, the walltime is the amount of time the application actually ran, typically less than the requested walltime.</li>
    <li><strong>Node-hour</strong>: unit of work indicating that an application ran for a time <em>t</em> on <em>n</em> nodes, such that <em>n</em>*<em>t</em> = 1 hour. Using 1 node for 1 hour is 1 node-hour. This is irrespective of the number of cores on the node you actually use.</li>
    <li><strong>Core-hour</strong>: unit of work indicating that an application ran for a time <em>t</em> on <em>p</em> cores, such that <em>p</em>*<em>t</em> = 1 hour. Using 20 cores, no matter on how many nodes, for 1 hour results in 20 core-hours.</li>
    <li><strong>Node-day</strong>: unit of work indicating that an application ran for a time <em>t</em> on <em>n</em> nodes such that <em>n</em>*<em>t</em> = 24 hours. Using 3 nodes for 8 hours results in 1 node day.</li>
    <li><strong>Memory requirement</strong>: the amount of RAM needed to successfully run an application.  It can be specified per process for a distributed application, expressed in GB.</li>
    <li><strong>Storage requirement</strong>: the amount of disk space needed to store the input and output of an application, expressed in GB or TB.</li>
    <li><strong>Temporary storage requirement</strong>: the amount of disk space needed to store temporary files during the run of an application, expressed in GB or TB.</li>
    <li><strong>Single user per node policy</strong>: indicates that when a process of user <em>A</em> runs on a compute node, no process of another user will run on that compute node concurrently, i.e., the compute node will be exclusive to user <em>A</em>.  However, if one or more processes of user <em>A</em> are running on a compute node, and that node's capacity in terms of available cores and memory is not exceeded, processes part of another job submitted by user <em>A</em> may start on that compute node.</li>
    <li><strong>Shared memory application</strong>: an application that uses multiple cores for its computations, concurrent computations are executed by threads, typically one per core.  Each thread has access to the application's global memory space (hence the name), and has some thread-private memory. A shared memory application runs on a single compute node. See also multi-core application.</li>
    <li><strong>Multi-core application</strong>: a multi-core application uses more than one core during its execution by running multiple threads, also called a shared memory application.</li>
    <li><strong>Distributed application</strong>: an application that uses multiple compute nodes for its computations, concurrent computations are executed as processes.  These processes communicate by exchanging messages, typically implemented by calls to an MPI library.  Messages can be used to exchange data and coordinate the execution.</li>
    <li><strong>Serial application</strong>: a program that runs a single process, with a single thread.  All computations are done sequentially, i.e., one after the other, no explicit parallelism is used.</li>
    <li><strong>Process</strong>: an independent computation running on a computer.  It may interact with other processes, and it may run multiple threads.  A serial and shared memory application run as a single process, while a distributed application consists of multiple, coordinated processes.</li>
    <li><strong>Threads</strong>: a process can perform multiple computations, i.e., program flows, concurrently.  In scientific applications, threads typically process their own subset of data, or a subset of loop iterations.</li>
    <li><strong>MPI:</strong> Message passing interface, a de-facto standard that defines functions for inter-process communication. Many implementations in the form of libraries exist for C/C++/Fortran, some vendor specific.</li>
    <li><strong>OpenMP:</strong> a standard for shared memory programming that makes abstraction of explicit threads.</li>
</ul>"
69,"","<p>This is a very incomplete list, permantently under construction, of books about parallel computing.
</p><h2>General</h2><ul>
	<li>V. Eijkhout. <a href=\"https://insidehpc.com/2010/12/download-introduction-to-high-performance-scientific-computing/\" target=\"_blank\">Introduction to High Performance Scientific Computing</a>. 2011. This is a textbook that teaches the bridging topics between numerical analysis, parallel computing, code performance, large scale applications. It can be freely downloaded from <a href=\"http://pages.tacc.utexas.edu/~eijkhout/istc/istc.html\" target=\"_blank\">the author's page on the book</a> (though you have to respect the copyright of course).</li>
	<li>A. Grama, A. Gupta, G. Kapyris, and V. Kumar. <a href=\"http://www.pearsoned.co.uk/Bookshop/detail.asp?item=100000000005961\" target=\"_blank\">Introduction to Parallel Computing (2nd edition)</a>. Pearson Addison Wesley, 2003. ISBN 978-0-201-64865-2. A somewhat older book, but still used a lot as textbook in academic courses on parallel computing.</li>
	<li>C. Lin and L. Snyder. <a href=\"http://www.pearsoned.co.uk/bookshop/detail.asp?WT.oss=Principles%20of%20parallel%20programming&WT.oss_r=1&item=100000000247080\" target=\"_blank\">Principles of Parallel Programming</a>. Pearson Addison Wesley, 2008. ISBN 978-0-32148790-2. This books discusses parallel programming both from a more abstract level and a more practical level, touching briefly threads programming, OpenMP, MPI and PGAS-languages (using ZPL).</li>
	<li>M. McCool, A.D. Robinson, and J. Reinders. <a href=\"https://www.elsevier.com/books/structured-parallel-programming/mccool/978-0-12-415993-8\">Structured Parallel Programming: Patterns for Efficient Computation</a>. Morgan Kaufmann, 2012. ISBN 978-0-12-415993-8</li>
</ul><h2><a name=\"grid\"></a>Grid computing</h2><ul>
	<li>
	F. Magoules, J. Pan, K.-A. Tan, and A. Kumar. <a href=\"https://www.crcpress.com/Introduction-to-Grid-Computing/Magoules-Pan-Tan-Kumar/p/book/9781420074062\" target=\"_blank\">Introduction to Grid Computing</a>. CRC Press, 2009. ISBN 9781420074062.
	</li>
</ul><h2><a name=\"MPI\"></a>MPI</h2><ul>
	<li>A two-volume set in tutorial style:
	<ul>
		<li>W. Gropp, E. Lusk, and A. Skjellum. <a href=\"https://mitpress.mit.edu/?q=using-MPI-3ed\" target=\"_blank\">Using MPI: Portable Parallel Programming with the Message-Passing Interface, third edition</a>. MIT Press, 2014. ISBN 978-0-262-57139-2 (paperback) or 978-0-262-32659-9 (ebook). This edition of the book is based on the MPI-3.0 specification.</li>
		<li>W. Gropp, T. Hoeffler, R. Thakur and E. Lusk. <a href=\"https://mitpress.mit.edu/?q=using-advanced-MPI\" target=\"_blank\">Using Advanced MPI: Modern Features of the Message-Passing Interface</a>. MIT Press, 2014. ISBN 978-0-262-52763-7 (paperback) or 978-0-262-32662-9 (ebook).</li>
	</ul>
	The books replace the earlier editions of \"Using MPI: Portable Parallel Programming with the Message-Passing Interface\" and the book \"Using MPI-2: Advanced Features of the Message-Passing Interface\".
	</li>
	<li>
	A two-volume set in reference style, but somewhat outdated:
	<ul>
		<li>
		M. Snir, S.W. Otto, S. Huss-Lederman, D.W. Walker, and J. Dongarra. <a href=\"https://mitpress.mit.edu/?q=books/mpi-complete-reference-0\" target=\"_blank\">MPI: The Complete Reference. Volume 1: The MPI Core (2nd Edition)</a>. MIT Press, 1998. ISBN 978-0-262-69215-1.
		</li>
		<li>
		W. Gropp, S. Huss-Lederman, A. Lumsdaine, E. Lusk, B. Nitzberg, W. Saphir, and M. Snir. <a href=\"https://mitpress.mit.edu/?q=books/mpi-complete-reference-1\" target=\"_blank\">MPI: The Complete Reference, Volume 2: The MPI-2 Extensions</a>. MIT Press, 1998. ISBN 978-0-262-57123-4.
		</li>
	</ul>
	The two volumes are also available as one set with <a href=\"https://mitpress.mit.edu/?q=books/mpi-complete-reference\" target=\"_blank\">ISBN number 978-0-262-69216-8</a>.
	</li>
</ul><h2><a name=\"OpenMP\"></a>OpenMP</h2><ul>
	<li>B. Chapman, G. Jost, and R. van der Pas. <a href=\"https://mitpress.mit.edu/?q=books/using-openmp\" target=\"_blank\">Using OpenMP - Portable Shared  Memory Parallel Programming</a>. The MIT Press, 2008. ISBN 978-0-262-53302-7. </li>
	<li>R. Chandra, L. Dagum, D. Kohr, D. Maydan, J. McDonald, and R. Menon. <a href=\"https://www.elsevier.com/books/parallel-programming-in-openmp/chandra/978-1-55860-671-5\" target=\"_blank\">Parallel Programming in OpenMP</a>. Academic Press, 2000. ISBN 978-1-55860-671-5.</li>
</ul><h2>GPU computing</h2><ul>
	<li>M. Scarpino. <a href=\"https://www.manning.com/books/opencl-in-action\" target=\"_blank\">OpenCL in Action</a>. Manning Publications Co., 2012. ISBN 978-1-617290-17-6</li>
	<li>D.R. Kaeli, P. Mistry, D. Schaa, and D.P. Zhang. <a href=\"https://www.elsevier.com/books/heterogeneous-computing-with-opencl-20/kaeli/978-0-12-801414-1\" target=\"_blank\">Heterogeneous Computing with OpenCL 2.0, 1st Edition</a>. Morgan Kaufmann, 2015. ISBN 978-0-12-801414-1 (print) or 978-0-12-801649-7 (eBook). A thourough rewrite of the earlier well-selling book for OpenCL 1.2 that saw 2 editions.</li>
</ul><h2>Xeon Phi computing</h2><ul>
	<li>R. Rahman. <a href=\"https://www.apress.com/gp/book/9781430259268\">Intel Xeon Phi Coprocessor Architecture and Tools: The Guide for Application Developers</a>. Apress, 2013. SBN13: 978-1-4302-5926-8.  This is a free book that is aimed at the Knights Corner generation of Xeon Phi processors. The newer Knights Landing generation has a reworked vector instruction set, but most principles explained in this book remain valid also for the newer generation(s).</li>
	<li>J. Jeffers, J. Reinders, and A. Sodani. <a href=\"https://www.elsevier.com/books/intel-xeon-phi-processor-high-performance-programming/jeffers/978-0-12-809194-4\" target=\"_blank\">Intel Xeon Phi Processor High Performance Programming, 2nd Edition (Knights Landing Edition)</a>. Morgan Kaufmann, 2016. ISBN 978-0-12-809194-4. Errata and downloadable code examples for this book and other books by Jeffers and Reinders are maintained on the blog <a href=\"http://lotsofcores.com/\" target=\"_blank\">lotsofcores.com</a>.</li>
</ul><h2>Case studies and examples of programming paradigms</h2><ul>
	<li>J. Reinders and J. Jeffers (editors). <a href=\"https://www.elsevier.com/books/high-performance-parallelism-pearls-volume-one/reinders/978-0-12-802118-7\" target=\"_blank\">High Performance Parallelism Pearls Volume 1: Multicore and Many-Core Programming Approaches</a>. Morgan Kaufmann, 2014. ISBN 978-0-12-802118-7</li>
	<li>J. Reinders and J. Jeffers (editors). <a href=\"https://www.elsevier.com/books/high-performance-parallelism-pearls-volume-two/jeffers/978-0-12-803819-2\">High Performance Parallelism Pearls Volume 2: Multicore and Many-Core Programming Approaches</a>. Morgan Kaufmann, 2015. ISBN 978-0-12-803819-2</li>
</ul><p><em>Please mail further suggestions to <a href=\"mailto:Kurt.Lust@uantwerpen.be\">Kurt.Lust@uantwerpen.be</a>.</em>
</p>"
71,"","<h2>PRACE</h2><p>The <a href=\"http://www.training.prace-ri.eu/\" target=\"_blank\">PRACE Training Portal</a> has a number of <a href=\"http://www.training.prace-ri.eu/tutorials/index.html\" target=\"_blank\">training videos</a> online from their courses.<br>
</p><h2>LLNL - Lawrence Livermore National Laboratory (USA)<br></h2><p><a href=\"https://hpc.llnl.gov/training/tutorials\" target=\"_blank\">LLNL provides several tutorials.</a> Not all are applicable to the VSC clusters, but some are. E.g.,
</p><ul>
	<li><a href=\"https://computing.llnl.gov/tutorials/parallel_comp/\" target=\"_blank\">Introduction to Parallel Computing</a></li>
	<li><a href=\"https://computing.llnl.gov/tutorials/openMP/\" target=\"_blank\">OpenMP</a></li>
	<li><a href=\"https://computing.llnl.gov/tutorials/mpi_advanced/DavidCronkSlides.pdf\" target=\"_blank\">Advanced MPI</a></li>
</ul><p>There are also some tutorials on Python.
</p><h2>NCSA - National Center for Supercomputing Applications (USA)</h2><p>NCSA runs the <a href=\"https://www.citutor.org/browse.php\" target=\"_blank\">CI-Tutor (Cyberinfrastructure Tutor)</a> service that also contains a number of interesting tutorials. At the moment of writing, there is no fee and everybody can subscribe.
</p>"
73,"","<h2>Getting ready to request an account</h2><ul><li>Before requesting an account, you need to generate a pair of ssh keys. One popular way to do this on Windows is<a href=\"https://vscentrum.be/neutral/documentation/client/windows/keys-putty\" class=\"internal-link\"> using the freely available PuTTY client</a> which you can then also use to log on to the clusters.</li></ul><h2><a name=\"connecting\"></a>Connecting to the cluster</h2><ul><li>Open a text-mode session using an ssh client<ul><li><a href=\"https://vscentrum.be/neutral/documentation/client/windows/console-putty\" class=\"internal-link\">PuTTY</a> is a simple-to-use and freely available GUI SSH client for Windows.</li><li><a href=\"https://vscentrum.be/neutral/documentation/client/windows/using-pageant\">pageant</a> can be used to manage active keys for PuTTY, WinSCP and FileZilla so that you don't need to enter the passphrase all the time.</li><li><a href=\"https://vscentrum.be/neutral/documentation/client/windows/PuTTY-proxy\">Setting up a SSH proxy with PuTTY</a> to log on to a node protected by a firewall through another login node, e.g., to access the tier-1 system muk.</li><li><a href=\"https://vscentrum.be/neutral/documentation/client/windows/creating-an-ssh-tunnel\" class=\"internal-link\">Creating a SSH tunnel using PuTTY</a> to establish network communication between your local machine and the cluster otherwise blocked by firewalls.</li></ul></li><li><a id=\"data-transfer\" name=\"data-transfer\"></a>Transfer data using Secure FTP (SFTP) clients:<ul><li><a href=\"https://vscentrum.be/neutral/documentation/client/windows/filezilla\" class=\"internal-link\">FileZilla</a></li><li><a href=\"https://vscentrum.be/neutral/documentation/client/windows/winscp\" class=\"internal-link\">WinSCP</a></li></ul></li><li><a id=\"X-programs\" name=\"X-programs\"></a>Display graphical programs:<ul><li>You can install a so-called X server: <a href=\"https://vscentrum.be/neutral/documentation/client/windows/xming\" class=\"internal-link\">Xming</a>. X is the protocol that is used by most Linux applications to display graphics on a local or remote screen.</li><li>On the KU Leuven/UHasselt clusters it is also possible to <a href=\"https://vscentrum.be/neutral/documentation/client/multiplatform/nx-start-guide\">use the NX Client</a> to log on to the machine and run graphical programs. Instead of an X-server, another piece of client software is needed. That software is currently available for Windows, OS X, Linux, Android and iOS. </li></ul></li><li>If you install the free <a href=\"http://www.cygwin.com/\">UNIX emulation layer cygwin</a> with the necessary packages, you can use the same OpenSSH client as on Linux systems and all pages about ssh and data transfer from <a href=\"https://vscentrum.be/neutral/documentation/client/linux\">the Linux client pages</a> apply.</li></ul><h2>Programming tools</h2><ul><li>By installing the <a href=\"http://www.cygwin.com/\">UNIX emulation layer cygwin</a> with the appropriate packages you can mimic very well the VSC cluster environment (at least with the foss toolchain). Cygwin supports the GNU compilers and also contains packages for OpenMPI (<a href=\"http://cygwin.com/cgi-bin2/package-grep.cgi?grep=openmpi&amp;arch=x86_64\">look for \"openmpi\"</a>) and some other popular libraries (FFTW, HDF5, ...). As such it can turn your Windows PC in a computer that can be used to develop software for the cluster if you don't rely on too many external libraries (which may be hard to install). This can come in handy if you sometimes need to work off-line. If you have a 64-bit Windows system (which most recent computers have), it is best to go for the 64-bit version of Cygwin. After all, the VSC-clusters are also running a 64-bit OS.</li><li><a href=\"https://vscentrum.be/neutral/documentation/client/windows/microsoft-visual-studio\" class=\"internal-link\">Microsoft Visual Studio</a> can also be used to develop OpenMP or MPI programs. If you do not use any Microsoft-specific libraries but stick to plain C or C++, the programs can be recompiled on the VSC clusters. Microsoft is slow in implementing new standards though. As of January 2015, OpenMP support is still stuck at version 2.0 of the standard.</li><li>Eclipse is a popular multi-platform Integrated Development Environment (IDE) very well suited for code development on clusters. <ul><li>Read our <a href=\"https://vscentrum.be/neutral/documentation/client/multiplatform/eclipse-intro\">Eclipse introduction</a> to find out why you should consider using Eclipse if you develop code and how to get it.</li><li>You can use <a href=\"https://vscentrum.be/neutral/documentation/client/multiplatform/eclipse-remote-editor\" class=\"internal-link\">Eclipse on the desktop as a remote editor for the cluster</a>.</li><li>You can use <a href=\"https://vscentrum.be/neutral/documentation/client/multiplatform/eclipse-VSC-subversion\" class=\"internal-link\">Eclipse on the desktop to access files in a subversion repository on the cluster</a>.</li><li>You can combine the remote editor feature with version control from Eclipse, but some care is needed, and <a href=\"https://vscentrum.be/neutral/documentation/client/multiplatform/eclipse-PTP-versioncontrol\">here's how to do it</a>.</li></ul>On Windows Eclipse relies by default on the cygwin toolchain for its compilers and other utilities, so you need to install that too.</li><li>There are also other ways to access subversion repositories on the VSC clusters or other subversion servers:<ul><li><a href=\"https://vscentrum.be/neutral/documentation/client/windows/TortoiseSVN\" class=\"internal-link\">TortoiseSVN</a> is a convenient client in fact, a Windows explorer shell extension) to access SVN repositories on the VSC clusters from your Windows desktop.</li><li>If you have the <a class=\"external-link\" href=\"http://www.cygwin.com/\">Cygwin unix emulation layer</a> installed, you can also use the <a href=\"https://vscentrum.be/neutral/documentation/client/multiplatform/desktop-access-VSC-subversion\" class=\"internal-link\">UNIX-style command line tools</a>.</li></ul></li></ul>"
75,"","<h2>Prerequisite: PuTTY</h2><p>By default, there is no ssh client software available on Windows, so you will typically have to install one yourself.  We recommend to use <a href=\"https://www.chiark.greenend.org.uk/~sgtatham/putty/\" target=\"_blank\">PuTTY</a>, which is freely available.  You do not even need to install; just download the executable and run it!  Alternatively, an installation package (MSI) is also available <a href=\"https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html\" target=\"_blank\">from the download site</a> that will install all other tools that you might need also.
</p><p>You can copy the PuTTY executables together with your private key on a USB stick to connect easily from other Windows computers.
</p><h2>Generating a public/private key pair</h2><p>To generate a public/private key pair, you can use the <a href=\"https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html\">PuTTYgen key generator</a>.  Start it and follow the following steps. Alternatively, you can follow a <a target=\"_blank\" href=\"https://www.vscentrum.be/assets/1189\">short video</a> explaining step-by-step the process of generating a new key pair and saving it in a format required by different VSC login nodes.
</p><ol>
	<li>
	In 'Parameters' (at the bottom of the window), choose 'SSH-2 RSA' and set the number of bits in the key to 2048:<br>
	<img src=\"/assets/123\" alt=\"PuTTYgen Parameters\"></li>
	<li>
	Click on 'Generate'.  To generate the key, you must move the mouse cursor over the PuTTYgen window (this generates some random data that PuTTYgen uses to generate the key pair).  Once the key pair is generated, your public key is shown in the field 'Public key for pasting into OpenSSH authorized_keys file'.
	</li>
	<li>
	Next, you should specify a passphrase in the 'Key passphrase' field and retype it in the 'Confirm passphrase' field. Remember, the passphrase protects the private key against unauthorized use, so it is best to choose one that is not too easy to guess.  Additionally, it is adviced to fill in the 'Key comment' field to make it easier identifiable afterwards.<br>
	<img src=\"/assets/125\" alt=\"PuTTYgen Passphrases\"></li>
	<li>
	Finally, save both the public and private keys in a secure place (i.e., a folder on your personal computer, or on your personal USB stick, ...) with the buttons 'Save public key' and 'Save private key'.  We recommend to use the name \"id_rsa.pub\" for the public key, and \"id_rsa.ppk\" for the private key.
	</li>
</ol><p><span class=\"visualHighlight\">If you use another program to generate a key pair, please remember that they need to be in the OpenSSH format to access the VSC clusters.</span>
</p><h2><a id=\"PuTTY_to_OpenSSH\" name=\"PuTTY_to_OpenSSH\"></a>Converting PuTTY keys to OpenSSH format</h2><p>OpenSSH is a very popular command-line SSH client originating from the Linux world but now available on many operating systems. Therefore its file format is a very popular one. Some applications, such as Eclipse's SSH components, can not handle private keys generated with PuTTY, only OpenSSH compliant private keys. However, PuTTY's key generator 'PuTTYgen' (that was used to generate the public/private key pair in the first place) can be used to convert the PuTTY private key to one that can be used by Eclipse.
</p><ol>
	<li>Start PuTTYgen.</li>
	<li>From the 'Conversions' menu, select 'Import key' and choose the file containing your PuTTY private key that is used to authenticate on the VSC cluster.</li>
	<li>When prompted, enter the appropriate passphrase.</li>
	<li>From the 'Conversions' menu, select 'Export OpenSSH key' and save it as 'id_rsa' (or any other name if the former already exists). Remember the file name and its location, it will have to be specified in the configuration process of, e.g.,  Eclipse.</li>
	<li>Exit PuTTYgen.</li>
</ol>"
79,"","<p>2) Click on 'Generate'. To generate the key, you must move the mouse cursor over the PuTTYgen window (this generates some random data that PuTTYgen uses to generate the key pair). Once the key pair is generated, your public key is shown in the field 'Public key for pasting into OpenSSH authorized_keys file'.</p><p><span></span>3) Next, you should specify a passphrase in the 'Key passphrase' field and retype it in the 'Confirm passphrase' field. Remember, the passphrase protects the private key against unauthorized use, so it is best to choose one that is not too easy to guess. Additionally, it is adviced to fill in the 'Key comment' field to make it easier identifiable afterwards.</p>"
81,"","<p><br>4) Finally, save both the public and private keys in a secure place (i.e., a folder on your personal computer, or on your personal USB stick, ...) with the buttons 'Save public key' and 'Save private key'. We recommend to use the name \"id_rsa.pub\" for the public key, and \"id_rsa.ppk\" for the private key.</p><p><strong>If you use another program to generate a key pair, please remember that they need to be in the OpenSSH format to access the VSC clusters.</strong></p><h2><a id=\"PuTTY_to_OpenSSH\" name=\"PuTTY_to_OpenSSH\"></a>Converting PuTTY keys to OpenSSH format</h2><p>OpenSSH is a very popular command-line SSH client originating from the Linux world but now available on many operating systems. Therefore its file format is a very popular one. Some applications, such as Eclipse's SSH components, can not handle private keys generated with PuTTY, only OpenSSH compliant private keys. However, PuTTY's key generator 'PuTTYgen' (that was used to generate the public/private key pair in the first place) can be used to convert the PuTTY private key to one that can be used by Eclipse.</p><ol><li>Start PuTTYgen.</li><li>From the 'Conversions' menu, select 'Import key' and choose the file containing your PuTTY private key that is used to authenticate on the VSC cluster.</li><li>When prompted, enter the appropriate passphrase.</li><li>From the 'Conversions' menu, select 'Export OpenSSH key' and save it as 'id_rsa' (or any other name if the former already exists). Remember the file name and its location, it will have to be specified in the configuration process of, e.g., Eclipse.</li><li>Exit PuTTYgen.</li></ol>"
83,"","<p>Each of the major VSC-institutions has its own user support:
</p><ul>
	<li>KU Leuven/Hasselt University cluster: E-mail <a href=\"mailto:HPCinfo@kuleuven.be\">HPCinfo@kuleuven.be</a></li>
	<li>Ghent University: E-mail <a href=\"mailto:hpc@ugent.be\">hpc@ugent.be</a> , info <a class=\"external-link\" href=\"https://www.ugent.be/hpc/en\" target=\"_blank\">hpc.ugent.be</a></li>
	<li>Antwerp University: E-mail <a href=\"mailto:hpc@uantwerpen.be\">hpc@uantwerpen.be</a>, further info on the <a href=\"https://www.uantwerpen.be/en/research-and-innovation/expertise/core-facilities/core-facilities/calcua/\" target=\"_blank\">CalcUA Core Facility web page</a></li>
	<li>VUB: E-mail <a href=\"mailto:hpc@vub.ac.be\">hpc@vub.ac.be</a> for information about accessing the VSC infrastructure or help with preparing a tier-1 proposal</li>
</ul><h2><a id=\"information-for-support\" name=\"information-for-support\"></a>What information should I provide when contacting user support?</h2><p>When you submit a support request, it helps if you always provide:
</p><ol>
	<li>your VSC user ID (or VUB netID),</li>
	<li>contact information - it helps to specify your preferred mail address and phone number for contact,</li>
	<li>an informative subject line for your request,</li>
	<li>the time the problem occurred,</li>
	<li>the steps you took to resolve the problem.</li>
</ol><p>Below, you will find more useful information you can provide for various categories of problems you may encounter. Although it may seem like more work to you, it will often save a few iterations and get your problem solved faster.
</p><h3>If you have problems logging in to the system</h3><p>then provide the following information:
</p><ol>
	<li>your operating system (e.g., Linux, Windows, MacOS X, ...),</li>
	<li>your client software (e.g., PuTTY, OpenSSH, ...),</li>
	<li>your location (e.g., on campus, at home, abroad),</li>
	<li>whether the problem is systematic (how many times did you try, over which period) or intermittent,</li>
	<li>any error messages shown by the client software, or an error log if it is available.</li>
</ol><h3>If installed software malfunctions/crashes</h3><p>then provide the following information:
</p><ol>
	<li>the name of the application (e.g., Ansys, Matlab, R, ...),</li>
	<li>the module(s) you load to use the software (e.g., R/3.1.2-intel-2015a),</li>
	<li>the error message the application produces,</li>
	<li>whether the error is reproducible,</li>
	<li>if possible, a procedure and data to reproduce the problem,</li>
	<li>if the application was run as a job, the jobID(s) of (un)successful runs.</li>
</ol><h3>If your own software malfunctions/crashes</h3><p>then provide the following information:
</p><ol>
	<li>the location of the source code,</li>
	<li>the error message produced at build time or runtime,</li>
	<li>the toolchain and other module(s) you load to build the software (e.g., intel/2015a with HDF5/1.8.4-intel-2015a),</li>
	<li>if possible and applicable, a procedure and data to reproduce the problem,</li>
	<li>if the software was run as a job, the jobID(s) of (un)successful runs.</li>
</ol>"
85,"","<p>The best way to get a complete list of all available software in a particular cluster can be obtained by typing:</p><pre>$ module av</pre><p>In order to use those software packages, the user should work with the <a href=\"/cluster-doc/software/modules\">module system</a>. On the newer systems, we use the same naming conventions for packages on all systems. Due to the ever expanding list of packages, we've also made some adjustments and don't always show all packages, so be sure to check out the page on <a href=\"/cluster-doc/software/modules\">the module system</a> again to <a href=\"/cluster-doc/software/modules#getting-more-software\">learn how you can see more packages</a>.</p><p><em>Note: Since August 2016, a <a href=\"/cluster-doc/software/modules-lmod\">different implementation of the module system has been implemented on the UGent and VUB Tier-2 systems</a>, called Lmod. Though highly compatible with the system used on the other clusters, it offers <a href=\"/cluster-doc/software/modules-lmod#commands\">a lot of new commands</a>, and <a href=\"/cluster-doc/software/modules-lmod#Advantages-differences\">some key differences</a>.</em><br></p><ul><li>As not everybody has access to the Tier-1 system Muk, here is an <a href=\"/cluster-doc/software/tier1-muk\">overview of the currently installed software on that system</a>.</li></ul><h2>Packages with additional documentation</h2><ul>
    <li>Matlab (a package by <a href=\"https://nl.mathworks.com\" target=\"_blank\">The MathWorks</a>)

    <ul>
        <li><a href=\"/cluster-doc/software/matlab\">Starting Matlab</a></li>
        <li><a href=\"/cluster-doc/software/matlab-dc\">Parallel computing with Matlab</a></li>
    </ul>
    </li>
    <li>R
    <ul>
        <li><a href=\"/cluster-doc/software/r-cla-in-scripts\">Accessing command line arguments in R scripts</a></li>
        <li><a href=\"/cluster-doc/software/r-integrate-c-functions\">Integrating C functions in R</a></li>
    </ul>
    </li>
    <li>Some programming languages have an extensive standard library, but optionally allow to install extra packages. For most languages, the user can install packages in his own home directory without system administrator intervention. Some documentation for doing this for <a href=\"/cluster-doc/development/perl-packages\">Perl</a> and <a href=\"/cluster-doc/development/python-packages\">Python</a> is provided.</li>
</ul>"
87,"","<h2>Software stack</h2><p>Software installation and maintenance on HPC infrastructure such as the VSC clusters poses a number of challenges not encountered on a workstation or a departemental cluster. For many libraries and programs, multiple versions have to installed and maintained as some users require specific versions of those. And those libraries or executable sometimes rely on specific versions of other libraries, further complicating the matter.
</p><p>The way Linux finds the right executable for a command, and a program loads the right version of a library or a plug-in, is through so-called environment variables. These can, e.g., be set in your shell configuration files (e.g., <code>.bashrc)</code>, but this requires a certain level of expertise. Moreover, getting those variables right is tricky and requires knowledge of where all files are on the cluster. Having to manage all this by hand is clearly not an option.
</p><p>We deal with this on the VSC clusters in the following way. First, we've defined the concept of a <a href=\"/cluster-doc/development/toolchains\">toolchain</a> on most of the newer clusters. They consist of a set of compilers, MPI library and basic libraries that work together well with each other, and then a number of applications and other libraries compiled with that set of tools and thus often dependent on those. We use tool chains based on the Intel and GNU compilers, and refresh them twice a year, leading to version numbers like 2014a, 2014b or 2015a for the first and second refresh of a given year. Some tools are installed outside a toolchain, e.g., additional versions requested by a small group of users for specific experiments, or tools that only depend on basic system libraries. Second, we use the module system to manage the environment variables and all dependencies and possible conflicts between various programs and libraries., and that is what this page focuses on.
</p><p><em>Note: Since August 2016, a <a href=\"https://www.vscentrum.be/cluster-doc/software/modules-lmod\">different implementation of the module system has been implemented on the UGent and VUB Tier-2 systems</a>, called Lmod. Though highly compatible with the system used on the other clusters, it offers <a href=\"https://www.vscentrum.be/cluster-doc/software/modules-lmod#commands\">a lot of new commands</a>, and <a href=\"https://www.vscentrum.be/cluster-doc/software/modules-lmod#Advantages-differences\">some key differences</a>. Most of the commands below will still work though.</em></p><h2>Basic use of the module system</h2><p>Many software packages are installed as modules. These packages include compilers, interpreters, mathematical software such as Matlab and SAS, as well as other applications and libraries. This is managed with the <code>module</code> command.
</p><p>To view a list of available software packages, use the command <code>module av</code>. The output will look similar to this:
</p><pre>$ module av
----- /apps/leuven/thinking/2014a/modules/all ------
Autoconf/2.69-GCC-4.8.2
Autoconf/2.69-intel-2014a
Automake/1.14-GCC-4.8.2
Automake/1.14-intel-2014a
BEAST/2.1.2
...
pyTables/2.4.0-intel-2014a-Python-2.7.6
timedrun/1.0.1
worker/1.4.2-foss-2014a
zlib/1.2.8-foss-2014a
zlib/1.2.8-intel-2014a
</pre><p>This gives a list of software packages that can be loaded. Some packages in this list include <code>intel-2014a</code> or <code>foss-2014a</code> in their name. These are packages installed with the 2014a versions of the toolchains based on the Intel and GNU compilers respectively. The other packages do not belong to a particular toolchain. The name of the packages also includes a version number (right after the /) and sometimes other packages they need.
</p><p>Often, when looking for some specific software, you will want to filter the list of available modules, since it tends to be rather large. The module command writes its output to standard error, rather than standard output, which is somewhat confusing when using pipes to filter. The following command would show only the modules that have the string 'python' in their name, regardless of the case.</p><pre>$ module av |& grep -i python
</pre><p>A module is loaded using the command <code>module load</code> with the name of the package. E.g., with the above list of modules,
</p><pre>$ module load BEAST
</pre><p>will load the <code>BEAST/2.1.2</code> package.
</p><p>For some packages, e.g., <code>zlib</code> in the above list, multiple versions are installed; the <code>module load</code> command will automatically choose the lexicographically last, which is typically, but not always, the most recent version. In the above example,
</p><pre> $ module load zlib
</pre><p>will load the module <code>zlib/1.2.8-intel-2014a</code>. This may not be the module that you want if you're using the GNU compilers. In that case, the user should specify a particular version, e.g.,
</p><pre>$ module load zlib/1.2.8-foss-2014a
</pre><p>Obviously, the user needs to keep track of the modules that are currently loaded. After executing the above two load commands, the list of loaded modules will be very similar to:
</p><pre>$ module list
Currently Loaded Modulefiles:
  1) /thinking/2014a
  2) Java/1.7.0_51
  3) icc/2013.5.192
  4) ifort/2013.5.192
  5) impi/4.1.3.045
  6) imkl/11.1.1.106
  7) intel/2014a
  8) beagle-lib/20140304-intel-2014a
  9) BEAST/2.1.2
 10) GCC/4.8.2
 11) OpenMPI/1.6.5-GCC-4.8.2
 12) gompi/2014a
 13) OpenBLAS/0.2.8-gompi-2014a-LAPACK-3.5.0
 14) FFTW/3.3.3-gompi-2014a
 15) ScaLAPACK/2.0.2-gompi-2014a-OpenBLAS-0.2.8-LAPACK-3.5.0
 16) foss/2014a
 17) zlib/1.2.8-foss-2014a
</pre><p>It is important to note at this point that, e.g., <code>icc/2013.5.192</code> is also listed, although it was not loaded explicitly by the user. This is because <code>BEAST/2.1.2</code> depends on it, and the system administrator specified that the <code>intel</code> toolchain module that contains this compiler should be loaded whenever the <code>BEAST</code> module is loaded. There are advantages and disadvantages to this, so be aware of automatically loaded modules whenever things go wrong: they may have something to do with it!
</p><p>To unload a module, one can use the <code>module unload</code> command. It works consistently with the <code>load</code> command, and reverses the latter's effect. One can however unload automatically loaded modules manually, to debug some problem.
</p><pre>$ module unload BEAST
</pre><p>Notice that the version was not specified: the module system is sufficiently clever to figure out what the user intends. However, checking the list of currently loaded modules is always a good idea, just to make sure...
</p><p>In order to unload all modules at once, and hence be sure to start with a clean slate, use:
</p><pre>$ module purge
</pre><p>It is a good habit to use this command in PBS scripts, prior to loading the modules specifically needed by applications in that job script. This ensures that no version conflicts occur if the user loads module using his <code>.bashrc</code> file.
</p><p>Finally, modules need not be loaded one by one; the two 'load' commands can be combined as follows:
</p><pre>$ module load  BEAST/2.1.2  zlib/1.2.8-foss-2014a
</pre><p>This will load the two modules and, automatically, the respective toolchains with just one command.
</p><p>To get a list of all available module commands, type:
</p><pre>$ module help
</pre><h2><a name=\"getting-more-software\"></a>Getting even more software</h2><p>The list of software available on a particular cluster can be unwieldingly long and the information that <code>module av</code> produces overwhelming. Therefore the administrators may have chose to only show the most relevant packages by default, and not show, e.g., packages that aim at a different cluster, a particular node type or a less complete toolchain. Those additional packages can then be enabled by loading another module first. E.g., on hopper, the most recent UAntwerpen cluster when we wrote this text, the most complete and most used toolchains were the 2014a versions. Hence only the list of packages in those releases of the <code>intel</code> and <code>foss</code> (GNU) toolchain were shown at the time. Yet
</p><pre>$ module av
</pre><p>returns at the end of the list:
</p><pre>...
ifort/2015.0.090                   M4/1.4.16-GCC-4.8.2
iimpi/7.1.2                        VTune/2013_update10
----------------------- /apps/antwerpen/modules/calcua ------------------------
hopper/2014a hopper/2014b hopper/2015a hopper/2015b hopper/2016a hopper/2016b 
hopper/all   hopper/sl6   perfexpert   turing
</pre><p>The packages such as <code>hopper/2014b</code> enable additional packages when loaded.
</p><p>Similarly, on ThinKing, the KU Leuven cluster:
</p><pre>$ module av
...
-------------------------- /apps/leuven/etc/modules/ --------------------------
cerebro/2014a   K20Xm/2014a     K40c/2014a      M2070/2014a     thinking/2014a
ictstest/2014a  K20Xm/2015a     K40c/2015a      phi/2014a       thinking2/2014a
</pre><p>shows modules specifically for the thin node cluster ThinKing, the <a href=\"/infrastructure/hardware/hardware-kul#Cerebro\">SGI shared memory system Cerebro</a>, three types of NVIDIA GPU nodes and the Xeon Phi nodes. Loading one of these will show the appropriate packages in the list obtained with <code>module av</code>. E.g.,
</p><pre>module load cerebro/2014a
</pre><p>will make some additional modules available for Cerebro, including two additional toolchains with the SGI MPI libraries to take full advantage of the interconnect of that machine.
</p><h2>Explicit version numbers</h2><p>As a rule, once a module has been installed on the cluster, the executables or libraries it comprises are never modified. This policy ensures that the user's programs will run consistently, at least if the user specifies a specific version. Failing to specify a version may result in unexpected behavior.
</p><p>Consider the following example: the user decides to use the GSL library for numerical computations, and at that point in time, just a single version 1.15, compiled with the foss toolchain is installed on the cluster. The user loads the library using:
</p><pre>$ module load GSL
</pre><p>rather than
</p><pre>$ module load GSL/1.15-foss-2014a
</pre><p>Everything works fine, up to the point where a new version of GSL is installed, e.g., 1.16 compiled with both the <code>intel</code> and the <code>foss</code> toolchain. From then on, the user's load command will load the latter version, rather than the one he intended, which may lead to unexpected problems.
</p>"
97,"HPC for industry","<p>The collective expertise, training programs and infrastructure of VSC together with participating university associations have the potential to create significant added value to your business.<br>
</p>"
99,"What is supercomputing?","<p>Supercomputers have an immense impact on our daily lives. Their scope extends far beyond the weather forecast after the news.<br><br>
</p>"
109,"Projects and cases","<p>The VSC infrastructure being used by many academic and industrial users. Here are just a few case studies of work involving the VSC infrastructure and an overview of actual projects run on the tier-1 infrastructure.<br></p>"
113,"","<h2>Technical support<br></h2><ul>
	<li>KU Leuven/Hasselt University cluster: E-mail <a href=\"mailto:HPCinfo@kuleuven.be\">HPCinfo@kuleuven.be</a></li>
	<li>Ghent University: E-mail <a href=\"mailto:hpc@ugent.be\">hpc@ugent.be</a> , info <a class=\"external-link\" href=\"https://www.ugent.be/hpc/en\">hpc.ugent.be</a></li>
	<li>Antwerp University: E-mail <a href=\"mailto:hpc@uantwerpen.be\">hpc@uantwerpen.be</a>, further info on the <a href=\"https://www.uantwerpen.be/en/research-and-innovation/expertise/core-facilities/core-facilities/calcua/\">CalcUA Core Facility web page</a></li>
	<li>VUB: E-mail <a href=\"mailto:hpc@vub.ac.be\">hpc@vub.ac.be</a> for information about accessing the VSC infrastructure or help with preparing a tier-1 proposal</li>
</ul><p>Please also take a look at our <a href=\"/support/contact-support\">web page about technical support</a>. It contains a lot of tips about the information that you can pass to us with your support question so that we can provide a helpful answer faster.
</p><h2>General enquiries</h2><p>For non-technical questions about the VSC, you can contact the FWO or one of the coordinators from participating universities. This may include questions on admission requirements to questions about setting up a course or other questions that are not directly related to technical problems.<br>
</p>"
115,"FWO","<p>Research Foundation - Flanders (FWO)<br>Egmontstraat 5<br>1000 Brussel
</p><p>Tel. +32 (2) 512 91 10<br>E-mail: <a href=\"mailto:post@fwo.be\">post@fwo.be</a><br><a href=\"http://www.fwo.be/en/\">Web page of the FWO</a>
</p>"
117,"Antwerp University Association","<p><strong>Stefan Becuwe</strong>
<br>Antwerp University<br>
	Department of Mathematics and Computer Science<br>Middelheimcampus M.G 310<br>Middelheimlaan 1<br>2020 Antwerpen
</p><p>Tel.: +32 (3) 265 3860<br>E-mail: <a href=\"mailto:Stefan.Becuwe@uantwerpen.be\">Stefan.Becuwe@uantwerpen.be</a><br><a href=\"https://www.uantwerpen.be/nl/personeel/stefan-becuwe/\">Contact page on the UAntwerp site</a></p>"
119,"KU Leuven Association","<p><strong>Leen Van Rentergem</strong><br>KU Leuven, Directie ICTS<br>Willem de Croylaan 52c - bus 5580<br>3001 Heverlee</p><p>Tel.:+32 (16) 32 21 55 or +32 (16) 32 29 99<br>E-mail: <a href=\"mailto:leen.vanrentergem@kuleuven.be\">leen.vanrentergem@kuleuven.be</a><br><a href=\"https://www.kuleuven.be/wieiswie/nl/person/00025349\">Contact page on the KU Leuven site</a></p>"
121,"Universitaire Associatie Brussel","<p><strong>Stefan Weckx</strong><br>VUB, Research Group of Industrial Microbiology and Food Biotechnology<br>Pleinlaan 2<br>1050 Brussel</p><p>Tel.: +32 (2) 629 38 63<br>E-mail: <a href=\"mailto:Stefan.Weckx@vub.ac.be\">Stefan.Weckx@vub.ac.be</a><br><a href=\"http://we.vub.ac.be/nl/stefan-weckx\">Contact page on the VUB site</a></p>"
123,"Ghent University Association","<p><strong>Ewald Pauwels</strong><br>Ghent University, ICT Department<br>Krijgslaan 281 S89<br>9000 Gent
</p><p>Tel: +32 (9) 264 4716<br>E-mail: <a href=\"mailto:Ewald.Pauwels@ugent.be\">Ewald.Pauwels@ugent.be</a><br><a href=\"https://telefoonboek.ugent.be/nl/people/801001384834\">Contact page on the UGent site</a>
</p>"
125,"Associatie Universiteit-Hogescholen Limburg","<p><strong>Geert Jan Bex<br><em>VSC course coordinator</em></strong><br>UHasselt, Dienst Onderzoekscoördinatie<br>Campus Diepenbeek<br>Agoralaan Gebouw D<br>3590 Diepenbeek</p><p>Tel.: +32 (11) 268231 or +32 (16) 322241<br>E-mail: <a href=\"mailto:GeertJan.Bex@uhasselt.be\">GeertJan.Bex@uhasselt.be</a><br><a href=\"https://www.uhasselt.be/fiche?voornaam=geertjan&naam=bex\">Contact page on the UHasselt site</a> and <a href=\"http://alpha.uhasselt.be/~gjb/\">personal web page</a></p>"
127,"Contact us","<p>You can also contact the coordinators by filling in the form below.</p>"
129,"Technical problems?","<p>Don't use this form, but contact your support team directly using <a href=\"/support/contact-support\">the contact information in the user portal</a>.</p>"
131,"","<p>Need help? Have more questions? <a href=\"/en/about-vsc/contact\">Contact us</a>!</p>"
133,"","<p> The VSC is a partnership of five Flemish university associations. The Tier-1 and Tier-2 infrastructure is spread over four locations: Antwerp, Brussels, Ghent and Louvain. There is also a local support office in Hasselt.
</p>"
135,"","<h2>Ghent</h2><p>The recent data center of UGhent (2011)         on Campus Sterre features a room which is especially equipped to accommodate the VSC framework. This room currently houses the majority of the Tier-2 infrastructure of Ghent University and the first VSC Tier-1 capability system. The adjacent building of the ICT Department hosts the Ghent University VSC -employees, including support staff for the Ghent University Association (AUGent).</p><h2>Louvain</h2><p>The         KU Leuven equiped its new data center (2012) in Heverlee with a separate room for the VSC framework. This room currently houses the joint Tier-2 infrastructure of KU Leuven and Hasselt University and an experimental GPU / Xeon Phi cluster. This space will also house the next VSC Tier-1 computer. The nearby building of ICTS houses the KU Leuven VSC employees, including the support team for the KU Leuven Association.</p><h2>Hasselt</h2><p>The         VSC does not feature a computer room in Hasselt, but there is a local user support office for the Association University-Colleges Limburg (AU-HL) at Campus Diepenbeek.</p><h2>Brussels</h2><p>The VUB shares a data center with the ULB on Solbosch Campus also housing the VUB Tier-2 cluster and a large part of the BEgrid infrastructure. The VSC also has a local team responsible for the management of this infrastructure and for the user support within the University Association Brussels (UAB) and for BEgrid.</p><h2>Antwerp</h2><p>The         University of Antwerp features a computer room equipped for HPC infrastructure in the building complex Campus Groenenborger. A little further, on the Campus Middelheim, the UAntwerpen VSC members have their offices in the Mathematics and Computer Science building. This team also handles user support for the Association Antwerp University (AUHA).</p>"
137,"","<p>The VSC is a consortium of five Flemish universities. This consortium has no legal personality. Its objective is to build a Tier-1 and Tier-2 infrastructure in accordance with the European pyramid model. Staff appointed at five Flemish universities form an integrated team dedicated to training and user support.
</p><p>For specialized support each institution can appeal to an expert independent of where he or she is employed. The universities also invest in HPC infrastructure and the VSC can appeal to the central services of these institutions. In addition, embedment in an academic environment creates opportunities for cooperation with industrial partners.
</p><p>The VSC project is managed by the Research Foundation - Flanders (FWO), that receives the necessary financial resources for this task from the Flemish Government.
</p><p>Operationally, the VSC is controlled by the HPC workgroup consisting of employees from the FWO and HPC coordinators from the various universities. The HPC workgroup meets monthly. During these meetings operational  issues are discussed and agreed upon and strategic advice is offered to the Board of Directors of the FWO.<br>
</p><p>In addition, four committees are involved in the operation of the VSC: the Tier-1 user committee, the Tier-1 evaluation committee, the Industrial Board and the Scientific Advisory Board.
</p><h2>VSC users' committee</h2><p>The VSC user's committee was established to provide advise on the needs of users and ways to improve the services, including the training of users. The user's committee also plays a role in maintaining contact with users by spreading information about the VSC, making (potential) users aware of the possibilities offered by HPC and organising the annual user day.
</p><p>These members of the committee are given below in alphabetical order, according to which university they are associated with:
</p><ul>
	<li>AUHA: Wouter Herrebout, substitute Bart Partoens</li>
	<li>UAB: Frank De Proft, substitute Wim Thiery</li>
	<li>AUGent: Marie-Françoise Reyniers or Veronique Van Speybroeck</li>
	<li>AU-HL: Sorin Pop, substitute Sofie Thijs</li>
	<li>KU Leuven association: Dirk Roose, substitute Nele Moelans</li>
</ul><p>The members representing the strategic research institutes are
</p><ul>
	<li>VIB: Steven Maere, substitute Frederik Coppens</li>
	<li>imec: Wilfried Verachtert</li>
	<li>VITO: Clemens Mensinck, substitute Katrijn Dirix</li>
	<li>Flanders Make: Mark Engels, substitute Paola Campestrini</li>
</ul><p>The representation of the Industrial Board:
</p><ul>
	<li>Benny Westaedt, substitute Mia Vanstraelen</li>
</ul><h2><a name=\"tier1-evaluation\"></a>Tier-1 evaluation committee</h2><p>This committee evaluates applications for computing time on the Tier-1. Based upon admissibility and other evaluation criteria the committee grants the appropriate computing time.
</p><p>This committee is composed as follows:
</p><ul>
	<li>Walter Lioen, chairman (SURFsara, The Netherlands);</li>
	<li>Derek Groen (Computer Science, Brunel University London, UK);</li>
	<li>Sadaf Alam (CSCS, Switzerland);</li>
	<li>Nicole Audiffren (Cines, France);</li>
	<li>Gavin Pringle (EPCC, UK).</li>
</ul><p>The FWO provides the secretariat of the committee.
</p><h2>Industrial Board</h2><p>The Industrial Board serves as a communication channel between the VSC and the industry in Flanders. The VSC offers a scientific/technical computing infrastructure to the whole Flemish research community and industry. The Industrial Board can facilitate the exchange of ideas and expertise between the knowledge institutions and industry.
</p><p>The Industrial Board also develops initiatives to inform companies and non-profit institutions about the added value that HPC delivers in the development and optimisation of services and products and promotes the services that the VSC delivers to companies, such as consultancy, research collaboration, training and compute power.
</p><p>The members are:
</p><ul>
	<li>Mia Vanstraelen (IBM)</li>
	<li>Charles Hirsch (Numeca)</li>
	<li>Herman Van der Auweraer (Siemens Industry Software NV)</li>
	<li>Benny Westaedt (Van Havermaet)</li>
	<li>Marc Engels (Flanders Make)</li>
	<li>Marcus Drosson (Umicore)</li>
	<li>Sabien Vulsteke (BASF Agricultural Solutions)</li>
	<li>Birgitta Brys (Worldline)</li>
</ul>"
141,"","<p>A         supercomputer is a very fast and extremely parallel computer. Many of its technological properties are comparable to those of your laptop or even smartphone but there are important differences. </p>"
145,"","<h2>The VSC account</h2><p>In order to use the infrastructure of the VSC, you need a VSC-userid, also called a VSC account. The only exception are users of the VUB who just want to use the VUB Tier-2 infrastructure. For them their VUB userid is sufficient. You can then use the same userid on all VSC infrastructure to which you have access.
</p><p>Your account also includes two “blocks” of disk space: your home directory and data directory. Both are accessible from all VSC clusters. When you log in to a particular cluster, you will also be assigned one or more blocks of temporary disk space, called scratch directories. Which directory should be used for which type of data, is explained in the <a href=\"/en/user-portal\">user documentation</a>.
</p><p>You do not automatically have access to all VSC clusters with your VSC account. For the main Tier-1 compute cluster you need to submit a project application (or you should be covered by a project application within your research group). For some more specialised hardware you have to ask access separately, typically to the coordinator of your institution, because we want to be sure that that (usually rather expensive hardware) is used efficiently for the type of applications for which it was purchased. Also, you do not simply get automatic access to all available software. You can use all free software and a number of compilers and other development tools, but for most commercial software, you must first prove that you have a valid license (or the person who has paid the license on the cluster must allow you to use the license). For this you can contact your local support team.
</p><p>Before you can apply for your account, you will usually have to install an extra piece of software on your computer, called a ssh client. How the actual account application should be made and where you can find the software, is explained in the user documentation on the user portal.
</p><h2>Who can get access?</h2><ul>
	<li>All researchers at the Flemish university associations can get a VSC account. In many cases, this is done through a fully automated application process, but in some cases you must submit a request to your local support team. Specific details about these procedures can be found on the \"<a href=\"/cluster-doc/account-request\">Account request</a>\" page in the user documentation.</li>
	<li>Also Master students can get access to the Tier-2 infrastructure in the framework of their master thesis if supercomputing is needed for the thesis. For this, you will first need the approval of your supervisor. The details about the procedure can again be found on the \"<a href=\"/cluster-doc/account-request\">Account request</a>\" page in the user documentation.</li>
	<li>At the University of Leuven and Hasselt University lecturers can also use the local Tier-2 infrastructure in the context of some courses (when the software cannot run in the PC classes or the computers in those classes are not powerful enough). Again, you can find all the details about the application process on the \"<a href=\"/cluster-doc/account-request\">Account request</a>\" page in the user documentation. It is important that the application is submitted on time, at least two weeks before the start of the computer sessions.</li>
	<li>Researchers from minds and VIB can also get access. The application is made through your host university. The same applies to researchers at the university hospitals and research institutes under the direction or supervision of a university or a university college, such as the special university institutes mentioned in Article 169quater of the Decree of 12 June 1991 concerning universities in the Flemish Community.</li>
	<li>Researchers at other Flemish public research institutions can compute on the Tier-1 infrastructure through a project application or can contact one of the coordinators of the university associations to access Tier-2 infrastructure. For larger amounts of computing time a fair financial compensation may be asked because universities also co-finance the operation of the VSC from their own.</li>
	<li>Businesses, non-Flemish public knowledge institutions and not-for-profit organisations can also gain access to the infrastructure. The procedures are explained on the page \"<a href=\"/en/access-and-infrastructure/access-industry\">Access for industry</a>\".</li>
</ul><h2>Additional information</h2><p>Before you apply for VSC account, it is useful to first check whether the infrastructure is suitable for your application. Windows or OS X programs for instance cannot run on our infrastructure as we use the Linux operating system on the clusters. The infrastructure also should not be used to run applications for which the compute power of a good laptop is sufficient. The pages on the <a href=\"/en/access-and-infrastructure/tier-1-clusters\">Tier-1</a> and <a href=\"/en/access-and-infrastructure/tier-2-clusters\">Tier-2 infrastructure</a> in this part of the website give a high-level description of our infrastructure. You can find more detailed information in the user documentation on the user portal. When in doubt, you can also contact your local support team. This does not require a VSC account.
</p><p>Furthermore, you should first check the page \"<a href=\"/cluster-doc/account-request\">Account request</a>\" in the <a href=\"/en/user-portal\">user documentation</a> and install the necessary software on your PC. You can also find links to information about that software on the “Account Request” page.
</p><p>Furthermore, it can also be useful to take one of the introductory courses that we organise periodically at all universities. However, it is best to apply for your VSC account before the course since you also can then also do the exercises during the course. We strongly urge people who are not familiar with the use of a Linux supercomputer to take such a course. After all, we do not have enough staff to help everyone individually for all those generic issues.
</p>"
149,"","<p>We offer you the opportunity of a free trial of the Tier-1 to prepare a future regular Tier-1 project application. You can test if your software runs well on the Tier-1 and do the scalability tests that are required for a project application.
</p><p>If you want to check if buying compute time on our infrastructure is an option, we offer a <a href=\"/en/access-and-infrastructure/access-industry\">very similar free programme for a test ride</a>.</p><h2>Characteristics of a Starting Grant</h2><ul>
	<li>The maximum amount is 100 nodedays.</li>
	<li>The maximal allowed period to use the compute time is 2 months.</li>
	<li>The allocation is personal and can't be transferred or shared with other researchers.</li>
	<li>Requests can be done at any time, there are no cutoff days.</li>
	<li>The use of this compute time is free of charge.</li>
</ul><h2>Procedure to apply and grant the request</h2><ol>
	<li>Download the <u><a href=\"/assets/1331\">application form for a starting grant version 2018 (docx, 31 kB)</a>.</u></li>
	<li>Send the completed application by e-mail to the Tier-1 contact address (<a href=\"mailto:hpcinfo@icts.kuleuven.be\">hpcinfo@icts.kuleuven.be</a>), with your local VSC coordinator in cc.</li>
	<li>The request will be judged for its validity by the Tier-1 coordinator.</li>
	<li>After approval the Tier-1 coordinator will give you access and compute time.<br>If not approved, you will get an answer with a motivation for the decision.</li>
	<li>The granted requests are published on the VSC website. Therefore you need to provide a short abstract in the application.<span></span></li>
</ol>"
153,"","<h2>The application</h2><p>The designated way to get access to the Tier-1 for research purposes is through a project application.
</p><p>You have to submit a proposal to get compute time on the Tier-1 cluster BrENIAC.
</p><p>You should include a realistic estimate of the compute time needed in the project in your application. These estimations can best be endorsed by Tier-1 benchmarks. To be able to perform these tests for new codes, you can request a <a href=\"/en/access-and-infrastructure/tier1-starting-grant\">starting grant</a> through a short and quick procedure.
</p><p>You can submit proposals continuously, but they will be gathered, evaluated and resources allocated at a number of cut-off dates. There are 3 cut-off dates in 2018 :
</p><ul>
	<li>February 5, 2018</li>
	<li>June 4, 2018</li>
	<li>October 1, 2018</li>
</ul><p>Proposals submitted since the last cut-off and before each of these dates are reviewed together.
</p><p>The FWO appoints an evaluation commission to do this.
</p><p>Because of the international composition of the <a href=\"/en/about-vsc/organisation-structure#tier1-evaluation\">evaluation commission</a>, the preferred language for the proposals is English. If a proposal is in Dutch, you must also sent an English translation. Please have a look at the documentation of standard terms like: CPU, core, node-hour, memory, storage, and use these consistently in the proposal.
</p><p>You can submit you application <a href=\"https://easychair.org/conferences/?conf=tier12017\" target=\"_blank\">via EasyChair</a> using the application forms below.<br>
</p><h2>Relevant documents - 2018</h2><p>As was already the case for applications for computing time on the Tier-1 granted in 2016 and 2017 and coming from researchers at universities, the Flemish SOCs and the Flemish public knowledge institutions, applicants do not have to pay a contribution in the cost of compute time and storage. Of course, the applications have to be of outstanding quality. The evaluation commission remains responsible for te review of the applications. For industry the price for compute time is 13 EURO per node day including VAT and for storage 15 EURO per TB per month including VAT.
</p><p>The adjusted Regulations for 2018 can be found in the links below.
</p><ul>
	<li><a href=\"/assets/1327\">Reglement betreffende aanvragen voor het gebruik van de Vlaamse supercomputer (Dutch only, applicable as of 1 January 2018) (PDF, 791 kB)</a><i></i></li>
	<li>Enclosure 1: <a href=\"/assets/1329\">The application form for 2018 (docx, 82 kB, last update March 2018)</a></li>
	<li><a href=\"/support/tut-book/hpc-glossary\">An overview of standard terms used in HPC</a></li>
	<li><a href=\"/support/tut-book/hpc-glossary\"></a><a href=\"/en/access-and-infrastructure/project-access-tier1/domains\">The list of scientific domains</a></li>
	<li>Submission is done via <a href=\"#easychair\">EasyChair</a></li>
</ul><p>If you need help to fill out the application, please consult your local support team.
</p><h2>Relevant documents - 2017</h2><p>As was already the case for applications for computing time on the Tier-1 granted in 2016 and coming from researchers at universities, the Flemish SOCs and the Flemish public knowledge institutions, applicants do not have to pay a contribution in the cost of compute time and storage. Of course, the applications have to be of outstanding quality. The evaluation commission remains responsible for te review of the applications. For industry the price for compute time is 13 EURO per node day including VAT and for storage 15 EURO per TB per month including VAT.
</p><p>The adjusted Regulations for 2017 can be found in the links below.
</p><ul>
	<li><a href=\"/assets/1171\">Reglement betreffende aanvragen voor het gebruik van de Vlaamse supercomputer (Dutch only, applicable as of 1 January 2017) (PDF, 215 kB)</a><i></i></li>
	<li>Enclosure 1: <a href=\"/assets/1193\">The application form (docx, 54 kB, last update May 2017)</a>. There is only a single category of projects in 2017. Research projects that have not yet been evaluated scientifically, should get an approval of the proposed research project by the university of the promotor. See §5 of the Regulations.</li>
	<li><a href=\"/support/tut-book/hpc-glossary\">An overview of standard terms used in HPC</a></li>
	<li><a href=\"/support/tut-book/hpc-glossary\"></a><a href=\"/en/access-and-infrastructure/project-access-tier1/domains\">The list of scientific domains</a></li>
	<li>Submission is done via <a href=\"#easychair\">EasyChair</a></li>
</ul><h2><a name=\"easychair\"></a>EasyChair procedure<br></h2><p>You have to submit your proposal on <a href=\"https://easychair.org/conferences/?conf=tier12018\">EasyChair for the conference Tier12018</a>. This requires the following steps:<br>
</p><ol>
	<li>If you do not yet have an EasyChair account, you first have to create one:
	<ol>
		<li>Complete the CAPTCHA</li>
		<li>Provide first name, name, e-mail address</li>
		<li>A confirmation e-mail will be sent, please follow the instructions in this e-mail (click the link)</li>
		<li>Complete the required details.</li>
		<li>When the account has been created, a link will appear to log in on the TIER1 submission page.</li>
	</ol></li>
	<li>Log in onto the EasyChair system.</li>
	<li>Select ‘New submission’.</li>
	<li>If asked, accept the EasyChair terms of service.</li>
	<li>Add one or more authors; if they have an EasyChair account, they can follow up on and/or adjust the present application.</li>
	<li>Complete the title and abstract.</li>
	<li>You must specify at least three keywords: Include the institution of the promoter of the present project and the field of research.</li>
	<li><span></span>As a paper, submit a PDF version of the completed Application form. You must submit the complete proposal, including the enclosures, as 1 single PDF file to the system.</li>
	<li>Click \"Submit\".</li>
	<li>EasyChair will send a confirmation e-mail to all listed authors.</li>
</ol>"
155,"","<p>The VSC infrastructure is can also be used by industry and non-Flemish research institutes. Here we describe the modalities.
</p><h2>Tier-1</h2><p>It is possible to get paid access to the Tier-1 infrastructure of the VSC. In a first phase, you can get up to 100 free node-days of compute time to verify that the infrastructure is suitable for your applications. You can also get basic support for software installation and the use of the infrastructure. When your software requires a license, you should take care of that yourself.
</p><p>For further use, there is a tree-parties legal agreement required with KU Leuven as the operator of the system and the Research Foundation - Flanders (FWO). You will be billed only for the computing time used and reserved disk space, according to the following rates:
</p><table>
<thead>
<tr>
	<td>
		<p><strong>Summary of Rates (VAT included):</strong>
		</p>
	</td>
	<td>
		<p><strong>Compute</strong>
		</p>
		<p><strong>(euro/node day)</strong>
		</p>
	</td>
	<td>
		<p><strong>Storage</strong>
		</p>
		<p><strong>(euro/TB/month)</strong>
		</p>
	</td>
</tr>
</thead>
<tbody>
<tr>
	<td>
		<p><strong>Non-Flemish public research institutes and not-for-profit organisations</strong></p>
	</td>
	<td>
		<p>€ 13</p>
	</td>
	<td>
		<p>€ 15</p>
	</td>
</tr>
<tr>
	<td>
		<p><strong>Industry</strong></p>
	</td>
	<td>
		<p>€ 13</p>
	</td>
	<td>
		<p>€ 15</p>
	</td>
</tr>
</tbody>
</table><p>These prices include the university overhead and basic support from the Tier-1 support staff, but no advanced level support by specialised staff.
</p><p>For more information you can <a href=\"mailto:industry@fwo.be\">contact our industry account manager (FWO)</a>.
</p><h2>Tier-2</h2><p>It is also possible to gain access to the Tier-2 infrastructure within the VSC. Within the Tier-2 infrastructure, there are also clusters tailored to special applications such as small clusters with GPU or Xeon Phi boards, a large shared memory machine or a cluster for Hadoop applications. See the <a href=\"/en/access-and-infrastructure/tier-2-clusters\">high-level overview</a> or <a href=\"/infrastructure/hardware\">detailed pages about the available infrastructure</a> for more information.
</p><p>For more information and specific arrangements please contact <a href=\"/en/about-vsc/contact\">the coordinator of the institution which operates the infrastructure</a>. In this case you only need an agreement with this institution without involvement of the FWO.
</p>"
177,"","<p>The VSC is responsible for the development and management of High Performance Computer Infrastructure used for research and innovation. The quality level of the infrastructure is comparable to other computational infrastructures in comparable European regions. In addition, the VSC is internationally connected through European projects such as PRACE<sup>(1)</sup> (traditional supercomputing) and EGI<sup>(2)</sup> (grid computing). Belgium has been a member of PRACE and participates in EGI via BEgrid, since October 2012.
</p><p>The VSC infrastructure consists of two layers in the European multi-layer model for an integrated HPC infrastructure. Local clusters (Tier-2) at the Flemish universities are responsible for processing the mass of smaller computational tasks and provide a solid base for the HPC ecosystem. A larger central supercomputer (Tier-1) is necessary for more complicated calculations while simultaneously serving as a bridge to infrastructures at a European level.
</p><p>The VSC assists researchers active in academic institutions and also the industry when using HPC through training programs and targeted advice. This offers the advantage that academia and industrialists come into contact with each other.
</p><p>In addition, the VSC also works on raising awareness of the added value HPC can offer both in academic research and in industrial applications.
</p><p><sup>(1)</sup> PRACE: Partnership for Advanced Computing in Europe<br>
	<sup>(2)</sup> EGI: European Grid Infrastructure
</p>"
179,"","<p> On 20 July 2006 the Flemish Government decided on the action plan 'Flanders i2010, time for a digital momentum in the innovation chain'. A study made by the steering committee e-Research, published in November 2007, indicated the need for more expertise, support and infrastructure for grid and High Performance Computing.
</p><p>Around the same time, the Royal Flemish Academy of Belgium for Science and the Arts (KVAB) published an advisory illustrating the need for a dynamic High Performance Computing strategy for Flanders. This recommendation focused on a Flemish Supercomputer Center with the ability to compete with existing infrastructures at regional or national level in comparable countries.
</p><p>Based on these recommendations, the Flemish Government decided on 14 December 2007 to fund the Flemish Supercomputer Center, an initiative of five Flemish universities. They joined forces to coordinate and to integrate their High Performance Computing<i> </i>infrastructures and to make their knowledge available to the public and for privately funded research.
</p><p>The grants were used to fund both capital expenditures and staff. As a result the existing university infrastructure was integrated through fast network connections and additional software. Thus, the pyramid model, recommended by PRACE, is applied. According to this model a central Tier-1 cluster is responsible for rolling out large parallel computing jobs. Tier-2 focuses on local use at various universities but is also open to other users. Hasselt University decided to collaborate with the University of Leuven to build a shared infrastructure while other universities opted to do it alone.
</p><h2>Some milestones</h2><ul>
	<li><strong>January 2008</strong>: Start of the \"VSC preparatory phase\" project</li>
	<li><strong>May 2008</strong>: The VSC submitted a first proposal for further funding to the Hercules Foundation </li>
	<li><strong>November 2008</strong>: A technical and financial plan was presented to the Flemish
Government. In the following weeks this plan was successfully defended before a
committee of international experts.
	</li>
	<li><strong>23 March 2009</strong>: Official launch of the VSC at an event with researchers
presenting their work in the presence of Patricia Ceysens, Flemish Minister for
Economy, Enterprise, Science, Innovation and Foreign Trade. Several speakers
highlighted the history of the project together with VSC’s mission and the
international aspect of this project.
	</li>
	<li><strong>3 April 2009</strong>: the Hercules Foundation and the Flemish Government
provided a grant of 7.29 million euros (2.09 million by the Hercules Foundation
and 5.2 million from the FFEU
	<sup>(1)</sup> for the further expansion of the local Tier-2
clusters and the installation of a central Tier-1 supercomputer for Flanders
for large parallel computations. It was also decided to entrust the project
monitoring to a supervisory committee for which the Hercules Foundation
provides the secretariat.
	</li>
	<li><strong>June 2009</strong>: The VSC submitted a project proposal to the Hercules Foundation 
to participate
through 
	<a href=\"http://www.prace-ri.eu/\">PRACE</a>, the <a href=\"http://ec.europa.eu/research/infrastructures/index_en.cfm?pg=esfri\">ESFRI</a><sup>(2)</sup> project in the field of supercomputing. 
After comparison with other projects, the Hercules
Foundation granted it the second highest priority and advised the Flemish government
as such. The Flemish Government supported the project, and after consultation
with other regions and communities and federal authorities, Belgium joined
PRACE in October 2012.
	</li>
	<li><strong>February 2010</strong>: The VSC 
submitted an updated operating plan to the Hercules
Foundation and the Flemish Government aiming to obtain structural funding for the VSC.
	</li>
	<li><strong>9 October 2012</strong>: Belgium became the twenty-fifth
member of PRACE. The Belgian delegation was made up of DG06 from the Walloon Government
and a technical advisor from VSC. 
	</li>
	<li><strong>25 October 2012</strong>: The first VSC Tier-1 cluster was
inaugurated at Ghent University. In the spring of 2012 the installation of this
cluster in the new data center at Ghent University campus took place. In a
video message Minister Ingrid Lieten encouraged researchers to make optimum use
of the new opportunities to drive research forward.
	</li>
	<li><strong>16 January 2014</strong>: the first global VSC User Day. This event brought together researchers from different universities and the
industry.
	</li>
	<li><strong>27 January 2015</strong>: The first VSC industry day at Technopolis in Mechelen. One of the points on the agenda was to
investigate how other companies abroad  -
in Germany and the United Kingdom – were being approached. Several examples of
companies in Flanders already using VSC infrastructure were illustrated.
Philippe Muyters, Flemish Minister for Economy and Innovation, closed the event
with an appeal for stronger links between the public and private sector to
strengthen Flemish competitiveness.
	</li><li><strong>1 January 2016</strong>: The Research Foundation - Flanders (FWO) takes over the tasks of the Hercules Foundation in the VSC project in a restructuring of the research funding in Flanders.</li>
</ul><p>
	<sup>(1)</sup> FFEU: Financieringsfonds voor Schuldafbouw en Eenmalige investeringsuitgaven (Financing fund for debt reduction and one-time investment)<br>
	<sup>(2)</sup> ESFRI: European Strategy Forum on Research Infrastructures
</p>"
183,"","<h2>Strategic plans and annual reports</h2><ul>
	<li><a href=\"https://www.vscentrum.be/assets/109\">Strategic plan 2015-2020 HPC in Flanders</a> – Only available in Dutch</li>
	<li><a href=\"https://www.vscentrum.be/assets/1379\">VSC annual report 2017</a></li>
	<li><a href=\"https://www.vscentrum.be/assets/1299\">VSC annual report 2016</a></li>
	<li><a href=\"https://www.vscentrum.be/assets/1109\">VSC annual report 2015</a></li>
	<li><a href=\"https://www.vscentrum.be/assets/987\">VSC annual report 2014</a></li>
</ul><h2>Newsletter: VSC Echo</h2><p>Our newsletter, VSC Echo, is distributed three times a year by e-mail. The <a href=\"/assets/1123\">latest edition</a>, number 10, is dedicated to :
</p><ul>
	<li>The upcoming courses and other events, where we also pay attention to the trainings organized by <a href=\"http://www.ceci-hpc.be/\" target=\"_blank\">CÉCI</a></li>
	<li>News about the new Tier-1 system BrENIAC</li>
	<li>The new VSC web site</li>
</ul><h3>Subscribe or unsubscribe</h3><p>If you would like to receive this newsletter by mail, just send an e-mail to listserv@ls.kuleuven.be with as text <strong>subscribe VSCECHO</strong> in the message body (and not in the subject line). (Please note the quotes are not used in the subject line but in the message body.) Alternatively (if your e-mail is correctly configured in your browser), you can also <a href=\"mailto:listserv@ls.kuleuven.be?body=subscribe%20VSCECHO\">send an e-mail from your browser</a>.
</p><p>You will receive a reply from LISTSERV@listserv.cc.kuleuven.ac.be asking you to confirm your subscription. Follow this link in the e-mail and you will be automatically subscribed to future issues of the newsletter.
</p><p>If you no longer wish to receive the newsletter, please send an e-mail to listserv@ls.kuleuven.be with the text <strong>unsubscribe VSCECHO</strong> in the message body (and not in the subject line). Alternatively (if your e-mail is correctly configured in your browser), you can also <a href=\"mailto:listserv@ls.kuleuven.be?body=unsubscribe%20VSCECHO\">send an e-mail from your browser</a>.
</p><h3>Archive</h3><ul>
	<li><a href=\"/assets/1123\">VSC Echo 10 - October 2016</a></li>
	<li><a href=\"/assets/1063\">VSC Echo 9 - January 2015</a></li>
	<li><a href=\"/assets/997\">VSC Echo 8 - September 2015</a></li>
	<li><a href=\"/assets/939\">VSC Echo 7 - July 2015</a></li>
	<li><a href=\"/assets/107\">VSC Echo 6 - January 2015</a> </li>
	<li><a href=\"/assets/105\">VSC Echo 5 - October 2014</a></li>
	<li><a href=\"/assets/103\">VSC Echo 4 - June 2014</a></li>
	<li><a href=\"/assets/101\">VSC Echo 3 - January 2014</a></li>
	<li><a href=\"/assets/97\">VSC Echo 2 - November 2013</a></li>
	<li><a href=\"/assets/93\">VSC Echo 1 - March 2013</a></li>
</ul>"
185,"","<p>Press contacts         should be channeled through <a href=\"/en/about-vsc/contact\">the Research Foundation - Flanders (FWO)</a>.</p><h2>Available material</h2><li> <a href=\"/assets/111\">Zip file with the VSC logo in a number of formats</a>.</li>"
191,"","<h2>Getting compute time in other centres</h2><ul>
	<li>
	<a class=\"external-link\" href=\"http://www.prace-ri.eu/\" target=\"_blank\">PRACE - Partnership for Advanced Computing in Europe</a> has a <a class=\"external-link\" href=\"http://www.prace-ri.eu/hpc-access\" target=\"_blank\">program to get access to the tier-0 infrastructure in Europe</a>.
	</li>
	<li>The <a href=\"http://www.doeleadershipcomputing.org/\" target=\"_blank\">DOE leadership computing program INCITE</a> also offers compute time to non-US-groups. It is more or less the equivalent of the PRACE tier-0 computing program. The annual deadline  for proposals is usually end of June.</li>
</ul><h2>Training programs in other centres</h2><ul>
	<li><a href=\"http://www.training.prace-ri.eu/nc/training_courses/index.html\">PRACE training programs</a></li>
	<li><a href=\"https://www.hlrs.de/solutions-services/service-portfolio/training\">HLRS, Stuttgart (Germany)</a></li>
	<li><a href=\"https://www.lrz.de/services/compute/courses/\">Leibniz RechenZentrum, Garching, near München (Germany)</a>, organised together with the <a href=\"https://www.rrze.fau.de\">Erlangen Computing Centre</a>.</li>
</ul><h2>EU initiatives</h2><ul>
	<li>
	<a class=\"external-link\" href=\"http://www.prace-ri.eu/\">PRACE, Partnership for Advanced Computing in Europe</a>
	</li>
	<li>
	<a href=\"https://www.egi.eu/\">EGI - European Grid Initiative</a>, the successor of the <a href=\"http://eu-egee-org.web.cern.ch/eu-egee-org/index.html\">EGEE - Enabling Grids for E-SciencE</a> program
	</li>
	<li>
	<a href=\"http://www.hpcineuropetaskforce.eu/\">HET, HPC in Europe Taskforce</a>, a project within <a href=\"http://ec.europa.eu/research/esfri\">ESFRI, European Strategy Forum on Research Infrastructures</a>
	</li>
	<li>
	The <a href=\"http://e-irg.eu/\">e-IRG, e-Infrastructure Reflection Group</a>
	</li>
</ul><h2>Some grid efforts</h2><ul>
	<li>
	<a href=\"http://wlcg.web.cern.ch/\">WLCG - World-wide LHC Computing Grid</a>, the compute grid supporting the Large Hedron Collider at Cern
	</li>
	<li>
	The <a href=\"https://www.xsede.org/\">XSEDE</a> program in the US which combines a large spectrum of resources across the USA in a single virtual infrastructure
	</li>
	<li>
	The <a href=\"https://opensciencegrid.org/\">Open Science Grid (OSG)</a> is a grid focused on high throughput computing in the US and one of the resource providers in the XSEDE project
	</li>
</ul><h2>Some HPC centres in Europe</h2><ul>
	<li>Belgium
	<ul>
		<li>
		<a href=\"http://www.ceci-hpc.be/\">CÉCI - Consortium des Équipements de Calcul Intensif</a>, the equivalent of the VSC run by the French Community of Belgium
		</li>
	</ul>
	</li>
	<li>
	Danmark: The <a href=\"https://www.deic.dk/\">DeIC - Danish e-Infrastructure Cooperation</a> is a virtual organisation just as the VSC in which several universities participate
	</li>
	<li>
	Germany:
	<ul>
		<li>
		<a href=\"http://www.gauss-centre.eu/gauss-centre/EN/Home/home_node.html\">GCS, Gauss Centre for Supercomputing</a>, a collaboration of three German national supercomputer centres
		<ul>
			<li>
			<a class=\"external-link\" href=\"http://www.fz-juelich.de/ias/jsc/EN/Home/home_node.html\" target=\"_blank\">JSC, Jülich Supercomputer Centre</a> of the <a class=\"external-link\" href=\"http://www.fz-juelich.de/portal/EN/Home/home_node.html\">Forschungszentrum Jülich</a>.
			</li>
			<li>
			<a href=\"https://www.hlrs.de/home/\">HLRS, Höchstleistungsrechenzentrum Stuttgart</a>.
			</li>
			<li>
			<a href=\"https://www.lrz.de/\">LRZ, Leibniz Rechenzentrum der Bayerischen Akademie der Wissenschaften</a>
			</li>
		</ul>
		</li>
		<li>
		<a href=\"https://www.hlrn.de/home/view/\">HLRN, Norddeutscher Verbund für Hoch- und Höchstleistungsrechnen</a>, a German supercomputer center in which 7 Bundesländer (\"States\") in Northern Germany participate
		</li>
		<li>
		<a href=\"http://www.mpcdf.mpg.de\">Max Planck Computing and Data Facility, Rechenzentrum Garching</a> of the Max Planck Society and the IPP (Institute for Plasma Physics)
		</li>
	</ul>
	</li>
	<li>
	Finland: <a href=\"https://www.csc.fi/home/\">CSC</a>.
	</li>
	<li>
	France:
	<ul>
		<li>
		<a href=\"http://www.genci.fr/en\">GENCI, Grand Equipement National de Calcul Intensif</a>, coordinates the 3 French national supercomputer centres:
		<ul>
			<li>
			<a href=\"http://www-ccrt.cea.fr/\">CCRT/CEA, Centre de Calcul Recherche et Technologie</a>, which also runs the French Tier-0 cluster for the PRACE program.
			</li>
			<li>
			<a href=\"https://www.cines.fr/\">CINES, Centre Informatique National de l’Enseignement Supérieur</a>.
			</li>
			<li>
			<a href=\"http://www.idris.fr/\">IDRIS, Institut du Développement et des Ressources en Informatique Scientifique</a>.
			</li>
		</ul>
		</li>
	</ul>
	</li>
	<li>
	Ireland: <a href=\"https://www.ichec.ie/\">ICHEC, Irish Centre for High-End Computing</a>.
	</li>
	<li>
	Italy: <a href=\"https://www.cineca.it/\">CINECA</a>, a non profit Consortium, made up of 32 Italian universities, The National Institute of Oceanography and Experimental Geophysics - OGS, the CNR (National Research Council), and the Ministry of University and Research.
	</li>
	<li>
	Netherlands: <a href=\"https://www.surf.nl/en/about-surf/subsidiaries/surfsara/\">SURFsara</a>, the organisation running the Dutch academic supercomputers
	</li>
	<li>
	Norway: <a href=\"https://www.sigma2.no/\">UNINETT Sigma2 AS</a> manages the national infrastructure for computational science in Norway, and offers services in high performance computing and data storage.</li>
	<li>
	Spain: <a href=\"https://www.bsc.es/\">BSC, Barcelona Supercomputing Center</a>.
	</li>
	<li>Sweden:
	<ul>
		<li><a href=\"http://www.snic.se/\">SNIC, the Swedish National Infrastructure for Computing</a>, is a meta-centre that coordinates high-performance and grid computing in 6 Swedish supercomputer centres and that represents Sweden in PRACE</li>
		<li><a class=\"external-link\" href=\"https://www.pdc.kth.se/\">PDC Center for High Performance Computing</a> at <a class=\"external-link\" href=\"https://www.kth.se/\">KTH</a> houses the largest supercomputer of Sweden.</li>
	</ul>
	</li>
	<li>
	Switzerland: <a href=\"https://www.cscs.ch/\">CSCS, the Swiss National Supercomputer Center</a>, an autonomous unit of ETH Zürich
	</li>
	<li>
	United Kingdom:
	<ul>
		<li>
		<a href=\"http://www.archer.ac.uk/\">Archer</a>, the UK national supercomputer service run by EPCC
		</li>
		<li>
		<a href=\"https://www.acrc.bris.ac.uk/\">University of Bristol Advanced Computing Research Centre</a>
		</li>
		<li>
		<a href=\"https://www.hpc.cam.ac.uk/\">University of Cambridge High Performance Computing Service</a>
		</li>
		<li><a href=\"http://www.cardiff.ac.uk/advanced-research-computing\">Advanced Research Computing @ Cardiff</a>
		</li>
		<li>
		<a href=\"http://www.epcc.ed.ac.uk/\">EPCC, Edinburgh Parallel Computing Centre</a>
		</li>
		<li><a href=\"https://www.supercomputing.wales/\">Supercomputing Wales</a>, also a consortium of universities similar to the VSC
		</li>
	</ul>
	</li>
</ul>"
193,"","<p>The Flemish Supercomputer Centre (VSC) is a virtual supercomputer center for academics and industry. It is managed by the Hercules Foundation in partnership with the five Flemish university associations.</p>"
203,"","<p>Account management at the VSC is mostly done through the web site <a href=\"https://account.vscentrum.be\">account.vscentrum.be</a> using your institute account rather than your VSC account. </p><h2>Managing user credentials</h2><ul>
    <li>You use the VSC account page to request your account as explained on the <a href=\"/cluster-doc/account-request\">\"Account request\" pages</a>. You'll also need to create an SSH-key which is also explained on those pages.</li>
    <li>Once your account is active and you can log on to your home cluster, you can use the account management pages for many other operations:
    <ul>
        <li>If you want to <a href=\"/cluster-doc/account-management/access-from-multiple-machines\">access the VSC clusters from more than one computer</a>, it is good practice to use a different key for each computer. You can upload additional keys via the account management page. In that way, if your computer is stolen, all you need to do is remove the key for that computer and your account is safe again.</li>
        <li>If you've <a href=\"/cluster-doc/account-management/messed-up-keys\">messed up your keys</a>, you can restore the keys on the cluster or upload a new key and then delete the old one.</li>
    </ul>
    </li>
</ul><h2>Group and Virtual Organisation management</h2><p>Once your VSC account is active and you can log on to your home cluster, you can also manage groups through the account management web interface. Groups (a Linux/UNIX concept) are used to control access to licensed software (e.g., software licenses paid for by one or more research groups), to create subdirectories where researchers working on the same project can collaborate and control access to those files, and to control access to project credits on clusters that use these (all clusters at KU Leuven). </p><ul>
    <li>All details are on the page \"<a href=\"cluster-doc/account-management/manage-vsc-groups\">How to create/manage VSC groups</a>\". In particular, you'll find how to<ul><li><a href=\"/cluster-doc/account-management/manage-vsc-groups#view-groups\">view the groups you belong to</a>. </li></ul><ul><li><a href=\"/cluster-doc/account-management/manage-vsc-groups#join-group\">request membership to a group</a> you feel you should belong to. It is then up to the moderator of that group to grant you membership.</li><li><a href=\"/cluster-doc/account-management/manage-vsc-groups#create-new-group\">create a new group</a></li><li><a href=\"/cluster-doc/account-management/manage-vsc-groups#working-with-file-and-directory-permissions\">use group permissions to control acces to files and directories</a></li></ul></li>
    
    <li>If you are a group moderator, you can manage your group by accepting requests from users that would like to join the group or inviting users to join your group through <a href=\"https://account.vscentrum.be/\">the VSC account web site</a>. </li>
    <li>For UGent users only: You can create or join a so-called Virtual Organisation or VO.</li>
</ul><h2>Managing disk space</h2><p>The amount of disk space that a user can use on the various file systems on the system is limited by quota on the amount of disk space and number of files. UGent users can see and request upgrades for their quota on the Account management site (Users need to be in a VO (Virtual Organisation) to request aditional quota. Creating and joining a VO is also done trought the Account Management website). On other sites checking your disk space use is still <a href=\"/cluster-doc/account-management/managing-disk-usage\">mostly done from the command line</a> and requesting more quote is done via email.</p>"
211,"","<p>Data on the VSC clusters can be stored in several locations, depending on the size and usage of these data. Following locations are available:</p><ul>
    <li><a href=\"#Home\">Home directory</a>

    <ul>
        <li>Location available as $VSC_HOME</li>
        <li>The data stored here should be relatively small (e.g., no files or directories larger than a gigabyte, although this is allowed), and not generating very intense I/O during jobs. <br>
        Also all kinds of configuration files are stored here, e.g., ssh-keys, .bashrc, or Matlab, and Eclipse configuration, ...</li>
    </ul>
    </li>
    <li><a href=\"#Data\">Data directory</a>
    <ul>
        <li>Location available as $VSC_DATA</li>
        <li>A bigger 'workspace', for datasets, results, logfiles, ... . This filesystem can be used for higher I/O loads, but for I/O bound jobs, you might be better of using one of the 'scratch' filesystems.</li>
    </ul>
    </li>
    <li><a href=\"#Scratch\">Scratch directories</a>
    <ul>
        <li>Several types exist, available in $VSC_SCRATCH_XXX variables</li>
        <li>For temporary or transient data; there is typically no backup for these filesystems, and 'old' data may be removed automatically.</li>
        <li>Currently, $VSC_SCRATCH_NODE, $VSC_SCRATCH_SITE and $VSC_SCRATCH_GLOBAL are defined, for space that is available per node, per site, or globally on all nodes of the VSC (currenlty, there is no real 'global' scratch filesystem yet).</li>
    </ul>
    </li>
</ul><p>Since these directories are not necessarily mounted on the same locations over all sites, you should always (try to) use the environment variables that have been created.</p><p>Quota is enabled on the three directories, which means the amount of data you can store here is limited by the operating system, and not by \"the boundaries of the hard disk\". You can see your current usage and the current limits with the appropriate quota command as explained on <a href=\"/cluster-doc/check-disk-usage\">How do I know how much disk space I am using?</a>. The actual disk capacity, shared by <i>all</i> users, can be found on the  <a href=\"/infrastructure/hardware\">Available hardware</a> page.</p><p>You will only receive a warning when you reach the soft limit of either quota. You will only start losing data when you reach the hard limit. Data loss occurs when you try to save new files: this will not work because you have no space left, and thus you will lose these new files. You will however not be warned when data loss occurs, so keep an eye open for the general quota warnings! The same holds for running jobs that need to write files: when you reach your hard quota, jobs will crash.</p><h2><a name=\"Home\"></a>Home directory</h2><p>This directory is where you arrive by default when you login to the cluster. Your shell refers to it as \"~\" (tilde), or via the environment variable $VSC_HOME.</p><p>The data stored here should be relatively small (e.g., no files or directories larger than a gigabyte, although this is allowed), and usually used frequently. Also all kinds of configuration files are stored here, e.g., by Matlab, Eclipse, ...</p><p>The operating system also creates a few files and folders here to manage your account. Examples are:</p><table class=\"prettytable\">
    <tbody>
        <tr>
            <td>.ssh/</td>
            <td>This directory contains some files necessary for you to login to the cluster and to submit jobs on the cluster. Do not remove them, and do not alter anything if you don't know what you're doing!</td>
        </tr>
        <tr>
            <td>.profile</td>
            <td>This script defines some general settings about your sessions,</td>
        </tr>
        <tr>
            <td>.bashrc</td>
            <td>This script is executed everytime you start a session on the cluster: when you login to the cluster and when a job starts. You could edit this file and, e.g., add \"module load XYZ\" if you want to automatically load module XYZ whenever you login to the cluster, although we do not recommend to load modules in your .bashrc.</td>
        </tr>
        <tr>
            <td>.bash_history</td>
            <td>This file contains the commands you typed at your shell prompt, in case you need them again.</td>
        </tr>
    </tbody>
</table><h2><a name=\"Data\"></a>Data directory</h2><p>In this directory you can store all other data that you need for longer terms. The environment variable pointing to it is $VSC_DATA. There are no guarantees about the speed you'll achieve on this volume.</p><h2><a name=\"Scratch\"></a>Scratch space</h2><p>To enable quick writing from your job, a few extra file systems are available on the work nodes. These extra file systems are called scratch folders, and can be used for storage of temporary and/or transient data (temporary results, anything you just need during your job, or your batch of jobs).</p><p>You should remove any data from these systems after your processing them has finished. There are no gurarantees about the time your data will be stored on this system, and we plan to clean these automatically on a regular base. The maximum allowed age of files on these scratch file systems depends on the type of scratch, and can be anywhere between a day and a few weeks. We don't guarantee that these policies remain forever, and may change them if this seems necessary for the healthy operation of the cluster.</p><p>Each type of scratch has his own use:</p><ul>
    <li><strong>Node scratch ($VSC_SCRATCH_NODE)</strong><br>
    Every node has its own scratch space, which is completely seperated from the other nodes. Every job automatically gets its own temporary directory on this node scratch, available through the environment variable $TMPDIR. $TMPDIR is guaranteed to be unique for each job.<br>
    Note however that when your job requests multiple cores and these cores happen to be in the same node, this $TMPDIR is shared among the cores!</li>
    <li><strong>Site scratch ($VSC_SCRATCH_SITE, $VSC_SCRATCH)</strong><br>
    To allow a job running on multiple nodes (or multiple jobs running on seperate nodes) to share data as files, every node of the cluster (including the login nodes) has access to this shared scratch directory. Just like the home and data directories, every user has its own scratch directory. Because this scratch is also available from the login nodes, you could manually copy results to your data directory after your job has ended.</li>
    <li><strong>Global scratch ($VSC_SCRATCH_GLOBAL)</strong><br>
    In the long term, this scratch space will be available throughout the whole VSC. At the time of writing, the global scratch is just the same volume as the site scratch, and thus contains the same data.</li>
</ul>"
213,"","<p>Data on the VSC clusters can be stored in several locations, depending on the size and usage of these data. Following locations are available:</p><ul>
    <li><a href=\"#Home\">Home directory</a>

    <ul>
        <li>Location available as $VSC_HOME</li>
        <li>The data stored here should be relatively small (e.g., no files or directories larger than a gigabyte, although this is allowed), and not generating very intense I/O during jobs. <br>
        Also all kinds of configuration files are stored here, e.g., ssh-keys, .bashrc, or Matlab, and Eclipse configuration, ...</li>
    </ul>
    </li>
    <li><a href=\"#Data\">Data directory</a>
    <ul>
        <li>Location available as $VSC_DATA</li>
        <li>A bigger 'workspace', for datasets, results, logfiles, ... . This filesystem can be used for higher I/O loads, but for I/O bound jobs, you might be better of using one of the 'scratch' filesystems.</li>
    </ul>
    </li>
    <li><a href=\"#Scratch\">Scratch directories</a>
    <ul>
        <li>Several types exist, available in $VSC_SCRATCH_XXX variables</li>
        <li>For temporary or transient data; there is typically no backup for these filesystems, and 'old' data may be removed automatically.</li>
        <li>Currently, $VSC_SCRATCH_NODE, $VSC_SCRATCH_SITE and $VSC_SCRATCH_GLOBAL are defined, for space that is available per node, per site, or globally on all nodes of the VSC (currenlty, there is no real 'global' scratch filesystem yet).</li>
    </ul>
    </li>
</ul><p>Since these directories are not necessarily mounted on the same locations over all sites, you should always (try to) use the environment variables that have been created.</p><p>Quota is enabled on the three directories, which means the amount of data you can store here is limited by the operating system, and not by \"the boundaries of the hard disk\". You can see your current usage and the current limits with the appropriate quota command as explained on <a href=\"/cluster-doc/check-disk-usage\">How do I know how much disk space I am using?</a>. The actual disk capacity, shared by <i>all</i> users, can be found on the  <a href=\"/infrastructure/hardware\">Available hardware</a> page.</p><p>You will only receive a warning when you reach the soft limit of either quota. You will only start losing data when you reach the hard limit. Data loss occurs when you try to save new files: this will not work because you have no space left, and thus you will lose these new files. You will however not be warned when data loss occurs, so keep an eye open for the general quota warnings! The same holds for running jobs that need to write files: when you reach your hard quota, jobs will crash.</p><h2><a name=\"Home\"></a>Home directory</h2><p>This directory is where you arrive by default when you login to the cluster. Your shell refers to it as \"~\" (tilde), or via the environment variable $VSC_HOME.</p><p>The data stored here should be relatively small (e.g., no files or directories larger than a gigabyte, although this is allowed), and usually used frequently. Also all kinds of configuration files are stored here, e.g., by Matlab, Eclipse, ...</p><p>The operating system also creates a few files and folders here to manage your account. Examples are:</p><table class=\"prettytable\">
    <tbody>
        <tr>
            <td>.ssh/</td>
            <td>This directory contains some files necessary for you to login to the cluster and to submit jobs on the cluster. Do not remove them, and do not alter anything if you don't know what you're doing!</td>
        </tr>
        <tr>
            <td>.profile</td>
            <td>This script defines some general settings about your sessions,</td>
        </tr>
        <tr>
            <td>.bashrc</td>
            <td>This script is executed everytime you start a session on the cluster: when you login to the cluster and when a job starts. You could edit this file and, e.g., add \"module load XYZ\" if you want to automatically load module XYZ whenever you login to the cluster, although we do not recommend to load modules in your .bashrc.</td>
        </tr>
        <tr>
            <td>.bash_history</td>
            <td>This file contains the commands you typed at your shell prompt, in case you need them again.</td>
        </tr>
    </tbody>
</table><h2><a name=\"Data\"></a>Data directory</h2><p>In this directory you can store all other data that you need for longer terms. The environment variable pointing to it is $VSC_DATA. There are no guarantees about the speed you'll achieve on this volume.</p><h2><a name=\"Scratch\"></a>Scratch space</h2><p>To enable quick writing from your job, a few extra file systems are available on the work nodes. These extra file systems are called scratch folders, and can be used for storage of temporary and/or transient data (temporary results, anything you just need during your job, or your batch of jobs).</p><p>You should remove any data from these systems after your processing them has finished. There are no gurarantees about the time your data will be stored on this system, and we plan to clean these automatically on a regular base. The maximum allowed age of files on these scratch file systems depends on the type of scratch, and can be anywhere between a day and a few weeks. We don't guarantee that these policies remain forever, and may change them if this seems necessary for the healthy operation of the cluster.</p><p>Each type of scratch has his own use:</p><ul>
    <li><strong>Node scratch ($VSC_SCRATCH_NODE)</strong><br>
    Every node has its own scratch space, which is completely seperated from the other nodes. Every job automatically gets its own temporary directory on this node scratch, available through the environment variable $TMPDIR. $TMPDIR is guaranteed to be unique for each job.<br>
    Note however that when your job requests multiple cores and these cores happen to be in the same node, this $TMPDIR is shared among the cores!</li>
    <li><strong>Site scratch ($VSC_SCRATCH_SITE, $VSC_SCRATCH)</strong><br>
    To allow a job running on multiple nodes (or multiple jobs running on seperate nodes) to share data as files, every node of the cluster (including the login nodes) has access to this shared scratch directory. Just like the home and data directories, every user has its own scratch directory. Because this scratch is also available from the login nodes, you could manually copy results to your data directory after your job has ended.</li>
    <li><strong>Global scratch ($VSC_SCRATCH_GLOBAL)</strong><br>
    In the long term, this scratch space will be available throughout the whole VSC. At the time of writing, the global scratch is just the same volume as the site scratch, and thus contains the same data.</li>
</ul>"
215,"","<p>Data on the VSC clusters can be stored in several locations, depending on the size and usage of these data. Following locations are available:</p>

<ul>
    <li><a href=\"#Home\">Home directory</a>

    <ul>
        <li>Location available as $VSC_HOME</li>
        <li>The data stored here should be relatively small (e.g., no files or directories larger than a gigabyte, although this is allowed), and not generating very intense I/O during jobs.&nbsp;<br />
        Also all kinds of configuration files are stored here, e.g., ssh-keys, .bashrc, or Matlab, and Eclipse configuration, ...</li>
    </ul>
    </li>
    <li><a href=\"#Data\">Data directory</a>
    <ul>
        <li>Location available as $VSC_DATA</li>
        <li>A bigger 'workspace', for datasets, results, logfiles, ... . This filesystem can be used for higher I/O loads, but for I/O bound jobs, you might be better of using one of the 'scratch' filesystems.</li>
    </ul>
    </li>
    <li><a href=\"#Scratch\">Scratch directories</a>
    <ul>
        <li>Several types exist, available in $VSC_SCRATCH_XXX variables</li>
        <li>For temporary or transient data; there is typically no backup for these filesystems, and 'old' data may be removed automatically.</li>
        <li>Currently, $VSC_SCRATCH_NODE, $VSC_SCRATCH_SITE and $VSC_SCRATCH_GLOBAL are defined, for space that is available per node, per site, or globally on all nodes of the VSC (currenlty, there is no real 'global' scratch filesystem yet).</li>
    </ul>
    </li>
</ul>

<p>Since these directories are not necessarily mounted on the same locations over all sites, you should always (try to) use the environment variables that have been created.</p>

<p>Quota is enabled on the three directories, which means the amount of data you can store here is limited by the operating system, and not by \"the boundaries of the hard disk\". You can see your current usage and the current limits with the appropriate quota command as explained on <a href=\"/cluster-doc/check-disk-usage\">How do I know how much disk space I am using?</a>. The actual disk capacity, shared by <i>all</i> users, can be found on the&nbsp; <a href=\"/infrastructure/hardware\">Available hardware</a> page.</p>

<p>You will only receive a warning when you reach the soft limit of either quota. You will only start losing data when you reach the hard limit. Data loss occurs when you try to save new files: this will not work because you have no space left, and thus you will lose these new files. You will however not be warned when data loss occurs, so keep an eye open for the general quota warnings! The same holds for running jobs that need to write files: when you reach your hard quota, jobs will crash.</p>

<h2><a name=\"Home\"></a>Home directory</h2>

<p>This directory is where you arrive by default when you login to the cluster. Your shell refers to it as \"~\" (tilde), or via the environment variable $VSC_HOME.</p>

<p>The data stored here should be relatively small (e.g., no files or directories larger than a gigabyte, although this is allowed), and usually used frequently. Also all kinds of configuration files are stored here, e.g., by Matlab, Eclipse, ...</p>

<p>The operating system also creates a few files and folders here to manage your account. Examples are:</p>

<table class=\"prettytable\">
    <tbody>
        <tr>
            <td>.ssh/</td>
            <td>This directory contains some files necessary for you to login to the cluster and to submit jobs on the cluster. Do not remove them, and do not alter anything if you don't know what you're doing!</td>
        </tr>
        <tr>
            <td>.profile</td>
            <td>This script defines some general settings about your sessions,</td>
        </tr>
        <tr>
            <td>.bashrc</td>
            <td>This script is executed everytime you start a session on the cluster: when you login to the cluster and when a job starts. You could edit this file and, e.g., add \"module load XYZ\" if you want to automatically load module XYZ whenever you login to the cluster, although we do not recommend to load modules in your .bashrc.</td>
        </tr>
        <tr>
            <td>.bash_history</td>
            <td>This file contains the commands you typed at your shell prompt, in case you need them again.</td>
        </tr>
    </tbody>
</table>

<h2><a name=\"Data\"></a>Data directory</h2>

<p>In this directory you can store all other data that you need for longer terms. The environment variable pointing to it is $VSC_DATA. There are no guarantees about the speed you'll achieve on this volume.</p>

<h2><a name=\"Scratch\"></a>Scratch space</h2>

<p>To enable quick writing from your job, a few extra file systems are available on the work nodes. These extra file systems are called scratch folders, and can be used for storage of temporary and/or transient data (temporary results, anything you just need during your job, or your batch of jobs).</p>

<p>You should remove any data from these systems after your processing them has finished. There are no gurarantees about the time your data will be stored on this system, and we plan to clean these automatically on a regular base. The maximum allowed age of files on these scratch file systems depends on the type of scratch, and can be anywhere between a day and a few weeks. We don't guarantee that these policies remain forever, and may change them if this seems necessary for the healthy operation of the cluster.</p>

<p>Each type of scratch has his own use:</p>

<ul>
    <li><strong>Node scratch ($VSC_SCRATCH_NODE)</strong><br />
    Every node has its own scratch space, which is completely seperated from the other nodes. Every job automatically gets its own temporary directory on this node scratch, available through the environment variable $TMPDIR. $TMPDIR is guaranteed to be unique for each job.<br />
    Note however that when your job requests multiple cores and these cores happen to be in the same node, this $TMPDIR is shared among the cores!</li>
    <li><strong>Site scratch ($VSC_SCRATCH_SITE, $VSC_SCRATCH)</strong><br />
    To allow a job running on multiple nodes (or multiple jobs running on seperate nodes) to share data as files, every node of the cluster (including the login nodes) has access to this shared scratch directory. Just like the home and data directories, every user has its own scratch directory. Because this scratch is also available from the login nodes, you could manually copy results to your data directory after your job has ended.</li>
    <li><strong>Global scratch ($VSC_SCRATCH_GLOBAL)</strong><br />
    In the long term, this scratch space will be available throughout the whole VSC. At the time of writing, the global scratch is just the same volume as the site scratch, and thus contains the same data.</li>
</ul>"
217,"","<p>To access certain cluster login nodes, from outside your institute's network (e.g., from home) you need to set a so-called VPN (Virtual Private Network). By setting up a VPN to your institute, your computer effectively becomes a computer on your institute's network and will appear as such to other services that you access. Your network traffic will be routed through your institute's network. If you want more information: There's an <a href=\"https://computer.howstuffworks.com/vpn.htm\">introductory page on HowStuffWorks</a> and a <a href=\"https://en.wikipedia.org/wiki/Virtual_private_network\">page that is more for techies on Wikipedia</a>.
</p><p>The VPN service is not provided by the VSC but by your institute's ICT centre, and they are your first contact for help. However, for your convenience, we present some pointers to that information:
</p><ul>
	<li>KU Leuven: Information <a href=\"https://admin.kuleuven.be/icts/services/extranet/index\">in Dutch</a> and <a href=\"https://admin.kuleuven.be/icts/english/services/VPN/VPN\">in English</a>. Information on contacting the service desk for assistance is also available <a href=\"https://admin.kuleuven.be/icts/servicepunt\">in Dutch</a> and <a href=\"https://admin.kuleuven.be/icts/english/servicedesk\">in English</a>. </li>
	<li>UGent: Information <a href=\"https://helpdesk.ugent.be/vpn/\">in Dutch</a> and <a href=\"https://helpdesk.ugent.be/vpn/en/\">in English</a>. Contact information for the help desk is also available <a href=\"https://helpdesk.ugent.be/extra/\">in Dutch</a> and <a href=\"https://helpdesk.ugent.be/extra/en/\">in English</a> (with links at the bottom of the VPN pages).</li>
	<li>UAntwerpen: Log in to <a href=\"https://pintra.uantwerpen.be/\">the Pintra service</a> and then visit <a href=\"https://pintra.uantwerpen.be/webapps/ua-pintrasite-BBLEARN/module/index.jsp?course_id=_8_1&tid=_525_1&lid=_11434_1&l=nl_PINTRA\">the VPN page</a> in the \"Network\"  section of the pages of \"Departement ICT\". If you only have a student account, you will find the same information in the <a href=\"https://blackboard.uantwerpen.be/webapps/blackboard/execute/announcement?method=search&context=course_entry&course_id=_4177_1&handle=announcements_entry&mode=view\">Infocenter ICT on Blackboard</a>, which has a <a href=\"https://blackboard.uantwerpen.be/webapps/blackboard/content/listContent.jsp?course_id=_4177_1&content_id=_397880_1\">page on VPN</a> in the Networks section. The contact information for the help desk in on <a href=\"https://pintra.uantwerpen.be/webapps/ua-pintrasite-BBLEARN/module/index.jsp?course_id=_8_1\">the start page of the subsite \"Departement ICT\"</a>. <em>Note that the configuration of the VPN changed on 25 October 2016, so if you have trouble connecting, check your settings! </em></li>
	<li>VUB: The VUB offers no central VPN system at this time. <a href=\"http://vubnet.vub.ac.be/vpn.html\">There is a VPN solution (\"Pulse Secure VPN\") which requires special permission.</a></li>
	<li>UHasselt: the pre-configured VPN software can be <a href=\"https://software.uhasselt.be/index.php?catid=410\">downloaded</a> (intranet, only staff members), contact the <a href=\"mailto:helpdesk@uhasselt.be\">UHasselt helpdesk (mail link)</a> if you have problems. There is also some information about this <a href=\"https://bibliotheek.uhasselt.be/en/accessibility-distance\">on the page \"Accessibility from a distance\" of the University Library</a>.</li>
</ul>"
219,"","<p>Linux is the operating system on all of the VSC-clusters.</p><ul>
    <li><a href=\"/cluster-doc/using-linux/basic-linux-usage\">A basic linux introduction</a>, with the most basic commands and links to other material on the web.</li>
    <li><a href=\"/cluster-doc/using-linux/how-to-get-started-with-shell-scripts\">Getting started with shell scripts</a>, small programs consisting of commands that you could also use on the command line.</li>
</ul>"
221,"","<p>All the VSC clusters run the Linux operating system:
</p><ul>
	<li>KU Leuven: Red Hat Enterprise Linux ComputeNode release 6.5 (Santiago), 64 bit</li>
	<li>UAntwerpen: CentOS 7.x</li>
	<li>UGent: Scientific Linux</li>
</ul><p>This means that, when you connect to one of them, you get a command line interface, which looks something like this:
</p><pre>vsc30001@login1:~&gt;
</pre><p>When you see this, we also say you are inside a \"shell\".   The shell will accept your commands, and execute them.
</p><p>Some of the most often used commands include:
</p><table class=\"prettytable\">
<tbody>
<tr>
	<td>ls
	</td>
	<td>Shows you a list of files in the current directory
	</td>
</tr>
<tr>
	<td>cd
	</td>
	<td>Change current working directory
	</td>
</tr>
<tr>
	<td>rm
	</td>
	<td>Remove file or directory
	</td>
</tr>
<tr>
	<td>joe
	</td>
	<td>Text editor
	</td>
</tr>
<tr>
	<td>echo
	</td>
	<td>Prints its parameters to the screen
	</td>
</tr>
</tbody>
</table><p>Most commands will accept or even need parameters, which are placed after the command, seperated by spaces. A simple example with the 'echo' command:
</p><pre>$ echo This is a test
This is a test
</pre><p>Important here is the \"$\" sign in front of the first line. This should not be typed, but is a convention meaning \"the rest of this line should be typed at your shell prompt\". The lines not starting with the \"$\" sign are usually the feedback or output from the command.
</p><p>More commands will be used in the rest of this text, and will be explained then if necessary. If not, you can usually get more information about a command, say the item or command 'ls', by trying either of the following:
</p><pre>$ ls --help
$ man ls
$ info ls
</pre><p>(You can exit the last two \"manuals\" by using the 'q' key.)
</p><h2>Tutorials</h2><p>For more exhaustive tutorials about Linux usage, please refer to the following sites:
</p><ul>
	<li><a href=\"https://www.youtube.com/channel/UCut99_Fv1YEcpYRXNnUM7LQ\" target=\"_blank\">Linux tutorials YouTube channel</a></li>
	<li><a href=\"https://www.lifewire.com/learn-how-linux-basics-4102692\">Linux Basics on Lifewire</a></li><li><a class=\"external free\" href=\"http://lnag.sourceforge.net/\" title=\"http://linux.about.com/od/nwb_guide/a/gdenwb06.htm\">Linux Newbie Administrator Guide</a></li>
	<li>We organise regular Linux introductory courses, see <a href=\"/en/education--training\">the \"Education and Training\" pages</a></li>
</ul>"
223,"","<h2>Shell scripts</h2><p>Scripts are basically uncompiled pieces of code: they are just text files. Since they don't contain machine code, they are executed by what is called a \"parser\" or an \"interpreter\". This is another program that understands the command in the script, and converts them to machine code. There are many kinds of scripting languages, including Perl and Python.
</p><p>Another very common scripting language is shell scripting. In a shell script, you will put the commands you would normally type at your shell prompt in the same order. This will enable you to execute all those commands at any time by only issuing one command: starting the script.
</p><p>Typically in the following examples they'll have on each line the next command to be executed although it is possible to put multiple commands on one line. A very simple example of a script may be:
</p><pre>echo \"Hello! This is my hostname:\"
hostname
</pre><p>You can type both lines at your shell prompt, and the result will be the following:
</p><pre>$ echo \"Hello! This is my hostname:\"
Hello! This is my hostname:
$ hostname
login1
</pre><p>Suppose we want to call this script \"myhostname\". You open a new file for editing, and name it \"myhostname\":
</p><pre>$ nano myhostname
</pre><p>You get a \"New File\", where you can type the content of this new file. Help is available by pressing the  'Çtrl+G' key combination. You may want to familiarize you with the other options at some point; now we will just type the content of the file, save it and exit the editor.
</p><p>You can type the content of the script:
</p><pre>echo \"Hello! This is my hostname:\"
hostname
</pre><p>You save the file and exit the editor by pressing the 'ctrl+x' key combination. Nano will ask you if you want to save the file. You should be back at the prompt.
</p><p>The easiest way to run a script is by starting the interpreter and pass the script as parameter. In case of our script, the interpreter may either be 'sh' or 'bash' (which are the same on the cluster). So start the script:
</p><pre>$ bash myhostname
Hello! This is my hostname:
login1
</pre><p>Congratulations, you just created and started your first shell script!
</p><p>A more advanced way of executing your shell scripts is by making them executable by their own, so without invoking the interpreter manually. The system can not automatically detect which interpreter you want, so you need to tell this in some way. The easiest way is by using the so called \"shebang\"-notation, explicitly created for this function: you put the following line on top of your shell script \"#!/path/to/your/interpreter\".
</p><p>You can find this path with the \"which\" command. In our case, since we use bash as an interpreter, we get the following path:
</p><pre>$ which bash
/bin/bash
</pre><p>We edit our script and change it with this information:
</p><pre>#!/bin/bash
echo \"Hello! This is my hostname:\"
hostname
</pre><p>Note that the \"shebang\" must be the first line of your script! Now the operating system knows which program should be started to run the script.
</p><p>Finally, we tell the operating system that this script is now executable. For this we change its file attributes:
</p><pre>$ chmod +x myhostname
</pre><p>Now you can start your script by simply executing it:
</p><pre>$ ./myhostname
Hello! This is my hostname:
login1
</pre><p>The same technique can be used for all other scripting languages, like Perl and Python.
</p><p>Most scripting languages understand that lines beginning with \"#\" are comments, and should be ignored. If the language you want to use does not ignore these lines, you may get strange results...
</p><h2>Links</h2><ul>
	<li><a href=\"https://www.nano-editor.org/dist/v2.0/nano.html\">Nano manual</a></li>
</ul>"
225,"","<h2>What is a VSC account?</h2><p>To log on to and use the VSC infrastructure, you need a so-called VSC account. There is only one exception: Users of the Brussels University Association who only need access to the VUB/ULB cluster Hydra can use their institute account.
</p><p>All VSC-accounts start with the letters \"vsc\" followed by a five-digit number. The first digit gives information about your home institution. There is no relationship with your name, nor is the information about the link between VSC-accounts and your name publicly accessible.
</p><p>Unlike your institute account, VSC accounts don't use regular fixed passwords but a key pair consisting of a public an private key because that is a more secure technique for authentication.
</p><p>Your VSC account is currently managed through your institute account.
</p><h2>Public/private key pairs</h2><p>A key pair consists of a private and a public key. The private key is stored on the computer(s) from which you want to access the VSC and always stays there. The public key is stored on a the systems you want to access, granting access to the anyone who can prove to have access to the corresponding private key. Therefore it is very important to protect your private key, as anybody who has access to it can access your VSC account. For extra security, the private key itself should be encrypted using a 'passphrase', to prevent anyone from using your private key even when they manage to copy it. You have to 'unlock' the private key by typing the passphrase when you use it.
</p><p>How to generate such a key pair, depends on your operating system. We describe the generation of key pairs in the client sections for <a href=\"/client/linux/keys-openssh\">Linux</a>, <a href=\"/client/windows/keys-putty\">Windows</a> and <a href=\"/client/macosx/keys-openssh\">macOS (formerly OS X)</a>.
</p><p><span class=\"visualHighlight\">Without your key pair, you won't be able to apply for a VSC account.</span>
</p><p>It is clear from the above that it is very important to protect your private key well. Therefore:
</p><ul>
	<li>You should not share your key pair with other users. </li>
	<li>If you have accounts at multiple supercomputer centres (or on other systems that use SSH), you should seriously consider using a different key pair for each of those accounts. In that way, if a key would get compromised, the damage can be controlled.</li>
	<li>For added security, you may also consider to use a different key pair for each computer you use to access your VSC account. If your computer is stolen, it is then easy to disable access from that computer while you can still access your VSC account from all your other computers. <a href=\"/cluster-doc/account-management/access-from-multiple-machines\">The procedure is explained on a separate web page</a>.</li>
</ul><h2>Applying for the account</h2><p>Depending on restrictions imposed by the institution, not all users might get a VSC account.  We describe who can apply for an account in the sections of the local VSC clusters.
</p><h3>Generic procedure for academic researchers<a id=\"generic\" name=\"generic\"></a></h3><p>For most researchers from the Flemish universities, the procedure has been fully automated and works by using your institute account to request a VSC account. Check below for exceptions or if the generic procedure does not work.
</p><p>Open the <a href=\"https://account.vscentrum.be/\">VSC account management web site</a> and select your \"home\" institution. After you log in using your institution login and password, you will be asked to upload your public key. You will get an e-mail to confirm your application. After the account has been approved by the VSC, your account will be created and you will get a confirmation e-mail.</p><h3>Users from the KU Leuven and UHasselt association</h3><p>UHasselt has an agreement with KU Leuven to run a shared infrastructure. Therefore the procedure is the same for both institutions.
</p><p>Who?
</p><ul>
	<li>Access is available for faculty, students (under faculty supervision), and researchers of the KU Leuven, UHasselt and their associations. See also <a href=\"/infrastructure/hardware/hardware-kul#AccessRestrictions\">the access restrictions</a>.</li>
</ul><p>How?
</p><ul>
	<li>Researchers with a regular personnel account (u-number) can use <a href=\"#generic\">the generic procedure</a>.</li>
	<li>If you are in one of the higher education institutions associated with KU Leuven, <a href=\"#generic\">the generic procedure</a> may not work. In that case, please e-mail hpcinfo(at)icts.kuleuven.be to get an account. You will have to provide a public ssh key generated as described above.</li>
	<li>Lecturers of KU Leuven and UHasselt that need HPC access for giving their courses: The procedure requires action both from the lecturers and from the students. Lecturers should follow the <a href=\"/cluster-doc/account-request/teachers-procedure\">specific procedure for lecturers</a>, while the students should simply apply for the account through <a href=\"#generic\">the generic procedure</a>.</li></ul><p>How to start?</p><ul><li>Please follow the information on the webpage</li><li>Or register for the HPC Introduction course</li><li>If there is no course announced please register to our <a target=\"_blank\" href=\"https://admin.kuleuven.be/icts/onderzoek/hpc/HPCintro-waitinglist\">waiting list</a> and we will organize a new session as soon as we get a few people interested in it.</li></ul><ul>
</ul><h3>Users of Ghent University Association</h3><p>All information about the access policy is available <a href=\"https://www.ugent.be/hpc/en/policy\">in English</a> at <a href=\"https://www.ugent.be/hpc\">the UGent HPC web pages</a>.
</p><ul>
	<li>Researchers can use <a href=\"#generic\">the generic procedure</a>.</li>
	<li>Master students can also use the infrastructure for their master thesis work. The promotor of the thesis should first send a motivation to hpc@ugent.be and then <a href=\"#generic\">the generic procedure</a> should be followed (using your student UGent id) to request the account.</li>
</ul><h3>Users of the Antwerp University Association (AUHA)</h3><p>Who?
</p><ul>
	<li>Access ia available for faculty, students (master's projects under faculty supervision), and researchers of the AUHA. See also <a href=\"/infrastructure/hardware/hardware-ua#AccessRestrictions\">the access restrictions page</a>.</li>
</ul><p>How?
</p><ul>
	<li>Researchers of the University of Antwerp with a regular UAntwerpen account can use <a href=\"#generic\">the generic procedure</a>.</li>
	<li>Users from higher education institutions associated with UAntwerpen can get a VSC account via UAntwerpen. However, we have not yet set up an automated form. Please contact the user support at <a class=\"mail-link\" href=\"mailto:hpc@uantwerpen.be?subject=Account%20request\">hpc@uantwerpen.be</a> to get an account. You will have to provide a public ssh key generated as described above.</li>
</ul><h3>Users of Brussels University Association</h3><ul>
	<li>If you only need access to the VUB cluster Hydra, you don't necessarily need a full VSC account but can use your regular institute account. More information can be found on <a class=\"external-link\" href=\"http://www.ulb.ac.be/wserv2_oratio/oratio?f_type=view&f_context=fiches&language=nl&noteid=227\">this VUB Web Notes page</a>.</li>
</ul><h2>Troubleshooting</h2><ul>
	<li>If you can't connect to the <a class=\"external-link\" href=\"https://account.vscentrum.be/\">VSC account management web site</a>,  some browser extensions have caused problems (and in particular some security-related extensions), so you might try with browser extensions disabled.</li>
</ul>"
227,"","<p>MATLAB has to be loaded using the module utility prior to running it. This ensures that the environment is correctly set. Get the list of available versions of MATLAB using
</p><pre>module avail matlab
</pre><p>(KU Leuven clusters) or</p><pre>module avail MATLAB</pre><p>(UAntwerpen and VUB clusters).</p><p>Load a specific version by specifying the MATLAB version in the command
</p><pre>module load matlab/R2014a
</pre><p>or</p><pre>module load MATLAB/2014a
</pre><p>depending on the site you're at.</p><h2>Interactive use</h2><ul>
	<li>Interactive use is possible, but is not the preferred way of using MATLAB on the cluster! Use batch processing of compiled MATLAB code instead.</li>
	<li>If there is an X Window System server installed on your PC (as is by default the case under <a href=\"/client/linux\">Linux</a>; you can use XMing Server under <a href=\"/client/windows\">Windows</a> or XQuartz on <a href=\"/client/macosx\">macOS/OS X</a>), the full graphical MATLAB Desktop is available. If the speed is acceptable to you - much of the Matlab user interface is coded in Java and Java programs are known to be slow over remote X connections - this is the recommended way to start MATLAB for short testing purposes, simple calculations, writing programs, visualizing data. Please avoid doing extensive calculations this way, as you would be abusing the resources of the shared login-node. Your program will disturb other users, and other users will slow down execution of your program. Moreover, only a limited amount of CPU time is available to you, after which your session will be killed (with possible data loss).</li>
	<li>With <code>matlab -nodesktop</code> you can start Matlab without the full desktop, while you are still able to use the visualisation features. The <code>helpwin</code>, <code>helpdesk</code> and <code>edit</code> commands also work and open GUI-style help windows or a GUI-based editor. Of course this also requires a X server.</li><li>You can always, i.e., without X-Window server, start MATLAB in console-mode, via
<pre>matlab -nodisplay
</pre>
You get a MATLAB command prompt, from where you can start m-files, but have no access to the graphical facilities. The same limitations as above on CPU time apply.
</li>
	<li>For intensive calculations you want to run interactively, it is possible to use the PBS Job system to reserve a node for your exclusive use, while still having access to, e.g., the graphical capabilities of MATLAB, by forwarding the X output (qsub -X -I).</li>
	<li>WARNING: an interactive MATLAB session on a compute node can be very slow. A workaround (found at <a href=\"https://hpc.uark.edu/\">hpc.uark.edu</a>) is:
	<ul>
		<li>launch an interactive session  qsub -I -X</li>
		<li>once the interactive session is started, (say it starts on  r2i0n15),  start another connection to that compute node (ssh -X r2i0n15). In this second connection, start MATLAB, and it will work at normal speed.</li>
	</ul>
	</li>
</ul><h2>Batch use</h2><p>For any non-trivial calculation, it is strongly suggested that you use the PBS batch system.
</p><h3>Running a MATLAB script</h3><p>You first have to write a MATLAB m-file that executes the required calculation. Make sure the last command of this m-file is 'quit' or 'exit', otherwise MATLAB might wait forever for more commands ...
</p><p>Example (to be saved, e.g., in testmatlabscript.m) :
</p><pre>ndim = 600;
a = rand(600,1)*10;
b = rand(1,600)*100;
c = a * b;
d = max(c);
e = min(d);
save('testmatlab', 'd', 'e');
exit;
</pre><p>You can now run this program (as a test, still on the login node, from the directory were you saved the file testmatlabscript.m):
</p><pre>matlab  -nodisplay -r testmatlabscript
</pre><p>The next thing is to write a small shell script, to be sent to the PBS Job System, so that the program can be executed on a compute node, rather than on the login node.
</p><p>A simple example follows (to be saved, e.g., in testmatlabscript.sh ) ;
</p><pre>#!/bin/bash -l
# The maximum duration of the program,
#   in the format [days:]hours:minutes:seconds
#PBS -l walltime=01:00:00
# the requested amount of RAM
#PBS -l pmem=950mb
# The name of your job (used in mail, outputfile, showq,...)
#PBS -N matlab_test_job
# Set the correct environment for matlab
module load matlab
# Go into the directory from where 'qsub' was run
cd $PBS_O_WORKDIR
# Start matlab, specify the correct command-file ...
matlab -nojvm -nodisplay -r test
</pre><p>Now you submit your job with
</p><pre>$ qsub testmatlabscript.sh
</pre><p>and you get the jobid that was assigned to your job. With
</p><pre>qstat
</pre><p>you get an overview of the status of your jobs. When the job has run, output will be available in the file &lt;jobname&gt;.o&lt;jobid&gt; in the directory where you submitted the job from. In the case of the file testmatlabscript.m above, a file testmatlabscript.mat will have been created, with the calculated data d and e, you can load the resulting file into a MATLAB for further processing.
</p><p>More commands and options of the Job System are described in the <a href=\"/cluster-doc/running-jobs\">general documentation on running jobs</a> and in particular on the page \"<a href=\"https://www.vscentrum.be/cluster-doc/running-jobs/submitting-managing-jobs\">Submitting and managing jobs</a>\".
</p><h3>Running a MATLAB function</h3><p>If instead of a script, a MATLAB function is used, parameters can be passed into the function.
</p><p>Example (to be saved, e.g., in testmatlabfunction.m) :
</p><pre>function testmatlabfunction(input1,input2)
% source: https://wiki.inf.ed.ac.uk/ANC/MatlabComputing
% change arguments to numerics if necessary - only when compiling code
if ~isnumeric(input1)
   input1n = str2num(input1);
   input2n = str2num(input2);
else
   input1n = input1;
   input2n = input2;
end
sumofinputs = input1n + input2n;
outputfilename = ['testfunction_' num2str(input1n) '_' num2str(input2n)];
save(outputfilename, 'input1n', 'input2n', 'sumofinputs');
exit;
</pre><p>You can now run this program (as a test, still on the login node, from the directory were you saved the file testmatlabfunction.m):
</p><pre>matlab  -nodisplay -r \"testmatlabfunction 3 6\"
</pre><p>Note the quotes around the function name and the parameters. Note also that the function name does not include the *.m extension.
</p><h2>MATLAB compiler</h2><p>Each job requires a MATLAB license while running. If you start lots of jobs, you'll use lots of licenses. When all licenses are in use, your further jobs will fail, and you'll block access to MATLAB for other people at your site.
</p><p>However, when compiling your MATLAB program, no more runtime licenses are needed.
</p><p>Compilation of MATLAB files is relatively easy with the MATLAB 'mcc' compiler. It works for 'function m-files' and for 'script m-files'. 'function m-files' are however preferred.
</p><p>To deploy a MATLAB program as a standalone application, load the module for MATLAB as a first step and compile the code in a second step with the mcc command.
</p><p>If we want to compile a MATLAB program 'main.m', the corresponding command line should be:
</p><pre>mcc  -v  -R -singleCompThread  -m  main.m
</pre><p>Where the options are:
</p><ul>
	<li>-m: generate a standalone application</li>
	<li>-v: verbose display of the compilation steps</li>
	<li>-R: runtime options, useful ones are: -singleCompThread, -nodisplay, -nojvm</li>
</ul><p>The deployed executable is compiled to run using a single thread via the option -singleCompThread.  This is important when a number of processes<br>
	are to run concurrently on the same node (e.g. worker framework).
</p><h3>Notes</h3><ul class=\"list--unordered\">
	<li>Parameters are always considered as strings, and thus have to be converted to, e.g., numbers inside your function when needed. You can test with 'isdeployed' or 'isstr' functions (see examples).</li>
	<li>The function is allowed to return a value, but that value is *not* returned to the shell. Thus, to get results out, they have to be written to the screen, or saved in a file.</li>
	<li>Not all MATLAB functions are allowed in compiled code (<a href=\"https://nl.mathworks.com/products/compiler/supported/compiler_support.html\">see the \"Compiler Support for Matlab and Toolboxes\" page at the MathWorks</a>).</li>
</ul><h3>Example 1: Simple matlab script file</h3><ul>
	<li>File fibonacci.m contains :</li>
</ul><pre>function a = fibonacci(n)
% FIBONACCI Calculate the fibonacci value of n.
% When complied as standalone function,
% arguments are always passed as strings, not nums ...
if (isstr(n))
  n = str2num(n);
end;
if (length(n)~=1) || (fix(n) ~= n) || (n &lt; 0)
  error(['MATLAB:factorial:NNotPositiveInteger', ...
        'N must be a positive integer.']);
end
first = 0;second = 1;
for i=1:n-1
    next = first+second;
    first=second;
    second=next;
end
% When called from a compiled application, display result
if (isdeployed)
  disp(sprintf('Fibonacci %d -&gt; %d' , n,first))
end
% Also return the result, so that the function remains usable
% from other Matlab scripts.
a=first;
</pre><ul>
	<li>Run the compiler</li>
</ul><pre> mcc -m fibonacci
</pre><ul>
	<li>Executable file 'fibonacci' is created.</li>
	<li>You can now run your application as follows :</li>
</ul><pre>./fibonacci 6
Fibonacci 6 -&gt; 5
$ ./fibonacci 8
Fibonacci 8 -&gt; 13
$ ./fibonacci 45
Fibonacci 45 -&gt; 701408733
</pre><h3>Example 2 : Function that uses other Matlab files</h3><ul>
	<li>File multi_fibo.m contains :</li>
</ul><pre>function multi_fibo()
%MULTIFIBO Calls FIBONACCI multiple times in a loop
% Function calculates Fibonacci number for a matrix by calling the
% fibonacci function in a loop. Compiling this file would automatically
% compile the fibonacci function also because dependencies are
% automatically checked.
n=10:20
if max(n)&lt;0
    f = NaN;
else
    [r c] = size(n);
    for i = 1:r %#ok
        for j = 1:c %#ok
            try
                f(i,j) = fibonacci(n(i,j));
            catch
                f(i,j) = NaN;
            end
        end
    end
end
</pre><ul>
	<li>Compile :
	<ul>
	</ul>
	</li>
</ul><pre>mcc -m multi_fibo
</pre><ul>
	<li>Run :</li>
</ul><pre>./multi_fibo
n =
    10    11    12    13    14    15    16    17    18    19    20
Fibonacci 10 -&gt; 34
Fibonacci 11 -&gt; 55
Fibonacci 12 -&gt; 89
Fibonacci 13 -&gt; 144
Fibonacci 14 -&gt; 233
Fibonacci 15 -&gt; 377
Fibonacci 16 -&gt; 610
Fibonacci 17 -&gt; 987
Fibonacci 18 -&gt; 1597
Fibonacci 19 -&gt; 2584
Fibonacci 20 -&gt; 4181
f =
          34          55          89         144         233         
377         610         987        1597        2584        4181
</pre><h3>Example 3 : Function that used other Matlab files in other directories</h3><ul><li>If your script uses MATLAB files (e.g., self-made scripts, compiled mex files) other than those part of the MATLAB-distribution, include them at compile time as follows:<ul>
	</ul></li></ul><pre>mcc -m -I /path/to/MyMatlabScripts1/ -I /path/to/MyMatlabScripts2 .... 
-I /path/to/MyMatlabScriptsN multi_fibo
</pre><ul>
</ul><p>(on a single line).
</p><h3>More info on the MATLAB Compiler</h3><p><a href=\"https://nl.mathworks.com/help/compiler/index.html\">Matlab compiler documentation on the Mathworks website.</a>
</p>"
229,"","<p>Matlab has several products to facilitate parallel computing, e.g.
</p><ul>
	<li>The <a href=\"https://nl.mathworks.com/products/parallel-computing.html\">Parallel Computing Toolbox</a> is a regular Matlab toolbox that lets you write parallel Matlab applications or use parallel implementations of algorithms in other toolboxes.<br>
	Try <code>help distcomp</code> to see if the toolbox is installed for the version of Matlab that you're using.</li>
</ul>"
231,"","<h2>Purpose</h2><p>Here it is shown how to use Rscript and pass arguments to an R script.</p><h2>Prerequisites</h2><p>It is assumed that the reader is familiar with the use of R as well as R scripting, and is familiar wth the linux bash shell.</p><h2>Using Rscript and command line arguments</h2><p>When performing computation on the cluster using R, it is necessary to run those scripts from the command line, rather than interactively using R's graphical user interface.  Consider the following R function that is defined in, e.g., 'logistic.R':</p><pre>logistic &lt;- function(r, x) {
    r*x*(1.0 - x)
}</pre><p>From R's GUI interface, you typically use this from the console as follows:</p><pre>&gt; source(\"logistic.R\")
&gt; logistic(3.2, 0.5)</pre><p>It is trivial to write an R script 'logistic-wrapper.R' that can be run from the command line, and that takes to arguments, the first being 'r', the second 'x'.</p><pre>args &lt;- commandArgs(TRUE)
r &lt;- as.double(args[1])
x &lt;- as.double(args[2])

source(\"logistic.R\")

logistic(r, x)</pre><p>The first line of this script stores all arguments passed to the script in the array 'args.  The second (third) line converts the first (second) element of that array from a string to a double precision number using the function 'as.double', and stores it into r (x).</p><p>Now from the linux command line, one can run the script above for r = 3.2 and x = 0.5 as follows:</p><pre>$ Rscript logistic-wrapper.R 3.2 0.5</pre><p>Note that you should have loaded the appropriate R module, e.g.,</p><pre>$ module load R</pre><p>Suppose now that the script needs to be extended to iterate the logistic map 'n' times, where the latter value is passed as the third argument to the script.</p><pre>args &lt;- commandArgs(TRUE)
r &lt;- as.double(args[1])
x &lt;- as.double(args[2])
n &lt;- as.integer(args[3])

source(\"logistic.R\")

for (i in 1:n) x &lt;- logistic(r, x)
print(x)</pre><p>Note that since the the third argument represents the number of iterations, it should be interpreted as an integer value, and hence be converted appropriately using the function 'as.integer'.</p><p>The script is now invoked from the linux command line with three parameters as follows:</p><pre>$ Rscript cl.R 3.2 0. 5 100</pre><p>Note that if you pass an argument that is to be interpreted as a string in your R program, no conversion is needed, e.g.,</p><pre>name &lt;- args[4]</pre><p>Here it is assumed that the 'name' is passed as the fourth command line argument.</p>"
233,"","<h2>Purpose</h2><p>Although R is a nice and fairly complete software package for statistical analysis, there are nevertheless situations where it desirable to extend R. This may be either to add functionality that is implemented in some C library, or to eliminate performance bottlenecks in R code. In this how-to it is assumed that the users wants to call his own C functions from R.</p><h2>Prerequisites</h2><p>It is assumed that the reader is familiar with the use of R as well as R scripting, and is a reasonably proficient C programmer. Specifically the reader should be familiar with the use of pointers in C.</p><h2>Integration step by step</h2><p>Before all else, first load the appropriate R module to prepare your environment, e.g.,</p><pre>$ module load R</pre><p>If you want a specific version of R, you can first check which versions are available using</p><pre>$ module av R</pre><p>and then load the appropriate version of the module, e.g.,</p><pre>$ module load R/3.1.1-intel-2014b</pre><h3>A first example</h3><p>No tutorial is complete without the mandatory 'hello world' example. The C code in file 'myRLib.c' is shown below:</p><pre>#include &lt;R.h&gt;

void sayHello(int *n) {
    int i;
    for (i = 0; i &lt; *n; i++)
        Rprintf(\"hello world!\\n\");
}</pre><p>Three things should be noted at this point</p><ol>
    <li>the 'R.h' header file has to be included, this file is part of the R distribution, and R knows where to find it;</li>
    <li>function parameters are <i>always </i>pointers; and</li>
    <li>to print to the R console, 'Rprintf' rather than 'printf' should be used.</li>
</ol><p>From this 'myRLib.c' file a shared library can be build in one convenient step:</p><pre>$ R CMD SHLIB myRlib.c</pre><p>If all goes well, i.e., if the source code has no syntax errors and all functions have been defined, this command will produce a shared library called 'myRLib.so'.</p><p>To use this function from within R in a convenient way, a simple R wrapper can be defined in 'myRLib.R':</p><pre>dyn.load(\"myRLib.so\");
sayHello &lt;- function(n) {
    .C(\"sayHello\", as.integer(n))
}</pre><p>In this script, the first line loads the share library containing the 'sayHello' function. The second line defines a convenient wrapper to simplify calling the C function from R. The C function is called using the '.C' function. The latter's first parameter is the name of the C function to be called, i.e., 'sayHello', all other parameters will be passed to the C function, i.e., the number of times that 'sayHello' will say hello as an integer.</p><p>Now, R can be started to be used interactively as usual, i.e.,</p><pre>$ R</pre><p>In R, we first source the library's definitions in 'myRLib.R', so that the wrapper functions can be used:</p><pre>&gt; source(\"myRLib.R\")
&gt; sayHello(2)
hello world!
hello world!
[[1]]
[1] 2</pre><p>Note that the 'sayHello' function is not particularly interesting since it does not return any value. The next example will illustrate how to accomplish this.</p><h3>A second, more engaging example</h3><p>Given R's pervasive use of vectors, a simple example of a function that takes a vector of real numbers as input, and returns its components' sum as output is shown next.</p><pre>#include &lt;R.h&gt;

/* sayHello part not shown */

void mySum(double *a, int* n, double *s) {
    int i;
    *s = 0.0;
    for (i = 0; i &lt; *n; i++)
        *s += a[i];
}</pre><p>Note that both 'a' and 's' are declared as pointers, the former being used as the address of the first array element, the second as an address to store a double value, i.e., the sum of array's compoments.</p><p>To produce the shared library, it is build using the R appropriate command as before:</p><pre>$ R CMD SHLIB myRLib.c</pre><p>The wrapper code for this function is slightly more interesting since it will be programmed to provide a convenient \"function-feel\".</p><pre>dyn.load(\"myRLib.so\");

# sayHello wrapper not shown

mySum &lt;- function(a) {
    n &lt;- length(a);
    result &lt;- .C(\"mySum\", as.double(a), as.integer(n), s = double(1));
    result$s
}</pre><p>Note that the wrapper functions is now used to do some more work:</p><ol>
    <li>it preprocesses the input by calculating the length of the input vector;</li>
    <li>it initializes 's', the parameter that will be used in the C function to store the result in; and</li>
    <li>it captures the result from the call to the C function which contains all parameters passed to the function, in the last statement only extracting the actual result of the computation.</li>
</ol><p>From R, 'mySum' can now easily be called:</p><pre>&gt; source(\"myRLib.R\")
&gt; mySum(c(1, 3, 8))
[1] 12</pre><p>Note that 'mySum' will probably not be faster than R's own 'sum' function.</p><h3>A last example</h3><p>Function can return vectors as well, so this last example illustrates how to accomplish this. The library is extended to:</p><pre>#include &lt;R.h&gt;

/* sayHello and my_sum not shown */

void myMult(double *a, int *n, double *lambda, double *b) {
    int i;
    for (i = 0; i &lt; *n; i++)
        b[i] = (*lambda)*a[i];
}</pre><p>The semantics of the function is simply to take a vector and a real number as input, and return a vector of which each component is the product of the corresponding component in the original vector with that real number.</p><p>After building the shared libary as before, we can extend the wrapper script for this new function as follows:</p><pre>dyn.load(\"myRLib.so\");

# sayHello and mySum wrapper not shown

myMult &lt;- function(a, lambda) {
    n &lt;- length(a);
    result &lt;- .C(\"myMult\", as.double(a), as.integer(n),
                 as.double(lambda), m = double(n));
    result$m
}</pre><p>From within R, 'myMult' can be used as expected.</p><pre>&gt; source(\"myRLib.R\")
&gt; myMult(c(1, 3, 8), 9)
[1]  9 27 72
&gt; mySum(myMult(c(1, 3, 8), 9))
[1] 108</pre><h2>Further reading</h2><p>Obviously, this text is just for the impatient.  More <a href=\"http://cran.r-project.org/doc/manuals/R-exts.html\" target=\"_blank\">in-depth documentation</a> can be found on the nearest CRAN site.</p>"
235,"","<h2>Programming paradigms and models</h2><ul>
	<li><a href=\"/cluster-doc/development/mpi\">MPI</a> for message passing distributed memory programming</li>
	<li><a href=\"/cluster-doc/development/openmp\">OpenMP</a> shared memory programming </li>
	<li><a href=\"/cluster-doc/development/hybrid-mpi-openmp\">Hybrid MPI/OpenMP</a> programs</li>
</ul><h2>Development tools</h2><ul>
	<li><a href=\"/cluster-doc/development/toolchains\">Toolchains</a> are a collection of tools to build software. Not available on some of the older clusters.
	<ul>
		<li><a href=\"/cluster-doc/development/toolchain-intel\">Intel toolchain</a> (mostly Intel compilers and libraries)</li>
		<li><a href=\"/cluster-doc/development/toolchain-foss\">FOSS toolchain</a> (open source components)</li>
	</ul></li>
	<li><a href=\"/cluster-doc/development/version-control\">Version control systems</a>
	<ul>
		<li><a href=\"/cluster-doc/development/version-control\">An overview</a></li>
		<li><a href=\"/cluster-doc/development/subversion\">Subversion</a> for version control</li>
		<li><a href=\"/cluster-doc/development/versioncontrol-git\">Git</a> for version control</li>
	</ul>
	</li>
	<li><a href=\"/cluster-doc/development/itac\">Intel Trace Analyzer & Collector (ITAC)</a> for performance analysis of MPI programs</li>
	<li><a href=\"/cluster-doc/development/parameterweaver\">ParameterWeaver</a>, a code generator for handling command line arguments with C/C++, Fortran, R and Octave as target languages</li>
</ul><h2>Libraries</h2><ul><li>Mathematics<ul><li><a href=\"/cluster-doc/development/blas-lapack\">BLAS and Lapack dense linear algebra libraries</a> (solving linear systems, eigenvalues, singular values)</li></ul></li><li>Perl<ul><li><a href=\"/cluster-doc/development/perl-packages\">How to install additional Perl packages?</a></li></ul></li><li>Python<ul><li><a href=\"/cluster-doc/development/python-packages\">How to install additional Python packages?</a></li></ul></li></ul><ul>
</ul><h2>Integrating code with software packages</h2><ul><li><a href=\"/cluster-doc/software/r-integrate-c-functions\">Integrating C code into R</a></li></ul>"
237,"","<h2>Purpose</h2>
<p>MPI is a language-independent communications protocol used to program parallel computers. Both point-to-point and collective communication are supported. MPI \"is a message-passing application programmer interface, together with protocol and semantic specifications for how its features must behave in any implementation.\" MPI's goals are high performance, scalability, and portability. MPI remains the dominant model used in high-performance computing today.
</p>
<p>The current version of the MPI standard is 3.0, but only the newest implementations implement the full standard. The previous specifications are the MPI 2.0 specification with minor updates in the MPI-2.1 and MPI-2.2 specifications. The standardisation body for MPI is the <a href=\"https://www.mpi-forum.org/\" target=\"_blank\">MPI forum</a>.
</p>
<h2>Some background information</h2>
<p>MPI-1.0 (1994) and its updates MPI-1.1 (1995), MPI-1.2 (1997) and MPI-1.3 (1998) concentrate on point-to-point communication (send/receive) and global operations in a static process topology. Major additions in MPI-2.0 (1997) and its updates MPI-2.1 (2008) and MPI-2.2 (2009) are one-sided communication (get/put), dynamic process management and a model for parallel I/O. MPI-3.0 (2012) adds non-blocking collectives, a major update of the one-sided communication model and neighbourhood collectives on graph topologies. The first update of the MPI-3.1 specification was released in 2015, and work is ongoing on the next major update, MPI-4.0.
</p>
<p>The two dominant Open Source implementations are <a href=\"https://www.open-mpi.org/\" target=\"_blank\">Open MPI</a> and <a href=\"https://www.mpich.org/\" target=\"_blank\">MPICH</a>. The latter has been through a couple of name changes: It was originally conceived in the early '90's as MPICH, then the complete rewrite was renamed to MPICH2, but as this name caused confusion as the MPI standard evolved into MPI 3.x, the name was changed again to MPICH, and the version number bumped to 3.0. MVAPICH developed at Ohio State University is the offspring of MPICH further optimised for InfiniBand and some other high-performance interconnect technologies. Most other MPI implementations are derived from one of these implementations.
</p>
<p>At the VSC we offer both implementations: Open MPI is offered with the GNU compilers in the <a href=\"/cluster-doc/development/toolchain-foss\">FOSS toolchain</a>, while the Intel MPI used in the <a href=\"/cluster-doc/development/toolchain-intel\">Intel toolchain</a> is derived from the MPICH code base.
</p>
<h2>Prerequisites</h2>
<p>You have a program that uses an MPI library, either developed by you, or by others. In the latter case, the program's documentation should mention the MPI library it was developed with.
</p>
<h2>Implementations</h2>
<p>On VSC clusters, several MPI implementations are installed. We provide two MPI implementations on all newer machines that can support those implementations:
</p>
<ol>
	<li><a href=\"/cluster-doc/development/toolchain-intel#intel-mpi\">Intel MPI</a> in the intel toolchain
	<ol>
		<li>Intel MPI 4.1 (intel/2014a and intel/2014b toolchains) implements the MPI-2.2 specification</li>
		<li>Intel MPI 5.0 (intel/2015a and intel/2015b toolchains) and Intel MPI 5.1 (intel/2016a and intel/2016b toolchains) implement the MPI-3.0 specification</li>
	</ol>
	</li>
	<li><a href=\"/cluster-doc/development/toolchain-foss#openmpi\">Open MPI</a> in the foss toolchain
	<ol>
		<li>Open MPI 1.6 (foss/2014a toolchain) only implements the MPI-2.1 specification</li>
		<li>Open MPI 1.8 (foss/2014b, foss/2015a and foss/2015b toolchains) and Open MPI 1.10 (foss/2016a and foss/2016b) implement the MPI-3.0 specification</li>
	</ol>
	</li>
</ol>
<p><span style=\"line-height: 1.5em;\">When developing your own software, this is the preferred order to select an implementation. The performance should be very similar, however, more development tools are available for Intel MPI (i.e., </span><a href=\"/cluster-doc/development/itac\">ITAC</a> for performance monitoring<span style=\"line-height: 1.5em;\">).</span>
</p>
<p><span style=\"line-height: 1.5em;\">Specialised hardware sometimes requires specialised MPI-libraries.</span>
</p>
<ul>
	<li><span style=\"line-height: 1.5em;\">The interconnect in <a href=\"/infrastructure/hardware/hardware-kul#Cerebro\">Cerebro, the SGI UV shared memory machine at KU Leuven</a>, provides hardware acceleration for some MPI functions. To take full advantage of the interconnect, it is necessary to use the SGI MPI library, part of the MPT packages which stands for Message Passing Toolkit (and also contains SGI's own implementation of <a href=\"http://www.openshmem.org/site/\" target=\"_blank\">OpenSHMEM</a>). Support is offered through additional toolchains (intel-mpt and foss-mpt).</span>
	<ul>
		<li><span style=\"line-height: 1.5em;\">SGI MPT 2.09 (intel-mpt/2014a and foss-mpt/2014a toolchains) contains the SGI MPI 1.7 library which implements the MPI-2.2 specification.</span></li>
		<li><span style=\"line-height: 1.5em;\">SGI MPT 2.10 (not yet installed, contact <a href=\"/support/contact-support\">KU Leuven support</a>) contains the SGI MPI 1.8 library which implements the MPI-3.0 specification.</span></li>
	</ul>
	</li>
</ul>
<p>Several other implementations may be installed, e.g., <a href=\"http://mvapich.cse.ohio-state.edu/\" target=\"_blank\">MVAPICH</a>, but we assume you know what you're doing if you choose to use them.
</p>
<p>We also assume you are already familiar with the job submission procedure. If not, check the \"<a href=\"/cluster-doc/running-jobs\">Running jobs</a>\" section first.
</p>
<h2>Compiling and running</h2>
<p>See to the documentation about the <a href=\"/cluster-doc/development/toolchains\">toolchains</a>.
</p>
<h2>Debugging</h2>
<p>For debugging, we recommend the ARM DDT debugger (formerly Allinea DDT, module allinea-ddt). <a href=\"https://developer.arm.com/products/software-development-tools/hpc/arm-forge/arm-ddt/video-demos-and-tutorials-for-arm-ddt\">Video tutorials are available on the Arm web site</a>. (KU Leuven-only).
</p>
<p>When using the intel toolchain, <a href=\"/cluster-doc/development/itac\">Intel's Trace Analyser & Collector</a> (ITAC) may also prove useful.
</p>
<h2>Profiling</h2>
<p>To profile MPI applications, one may use <a href=\"https://www.arm.com/products/development-tools/hpc-tools/cross-platform/forge/map\" target=\"_blank\">Arm MAP (formerly Allinea MAP)</a>, or <a href=\"http://www.scalasca.org/software/scalasca-2.x/documentation.html\" target=\"_blank\">Scalasca</a>. (KU Leuven-only)
</p>
<h2>Further information</h2>
<ul>
	<li><a href=\"https://software.intel.com/en-us/intel-mpi-library\" target=\"_blank\">Intel MPI</a>
	<ul>
		<li><a href=\"https://software.intel.com/en-us/articles/intel-mpi-library-documentation/\" target=\"_blank\">Documentation</a> (Latest version)</li>
	</ul>
	</li>
	<li><a href=\"https://www.open-mpi.org/\" target=\"_blank\">Open MPI</a>
	<ul>
		<li><a href=\"https://www.open-mpi.org/doc/\" target=\"_blank\">Documentation</a></li>
	</ul>
	</li>
	<li>SGI MPT, now HPE Performance Software MPI
	<ul>
		<li><a href=\"https://support.hpe.com/hpsc/doc/public/display?docId=emr_na-a00037728en_us&docLocale=en_US\" target=\"_blank\">Documentation</a></li>
	</ul>
	</li>
	<li><a href=\"https://www.mpi-forum.org\" target=\"_blank\">MPI forum</a>, where you can also find the standard specifications
	<ul>
		<li><a href=\"https://www.mpi-forum.org/docs/\" target=\"_blank\">Standard documents</a></li>
	</ul>
	</li>
	<li>See also the pages in the <a href=\"/support/tut-book\">tutorials section</a>, e.g., for <a href=\"/support/tut-book/books#MPI\">books</a> and <a href=\"/support/tut-book/web-tutorials\">online tutorial</a></li>
</ul>"
239,"","<h2>Purpose</h2>
<p>OpenMP (Open Multi-Processing) is an API that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran, on most processor architectures and operating systems. It consists of a set of compiler directives, library routines, and environment variables that influence run-time behavior.
</p>
<p>OpenMP uses a portable, scalable model that gives programmers a simple and flexible interface for developing parallel applications for platforms ranging from the standard desktop computer to the supercomputer. T<span style=\"line-height: 1.5em;\">he current version of the OpenMP specification is 4.0. It was released in July 2013 and is probably the biggest update of the specification so far. However, not all compilers already fully support this standard. The previous specification were the OpenMP 3.1 specification (July 2011) and OpenMP 3.0 specification (May 2008). Versions prior to 4.0 concentrated on exploiting thread-level parallelism on multicore machines in a portable way, while version 4.0 of the specifications adds support for vectorisation for the SIMD instruction sets on modern CPUs and offload of computations to accelerators (GPU, Xeon Phi, ...). The latter feature is an alternative to the use of OpenACC directives.</span>
</p>
<h2>Prerequisites</h2>
<p>You should have a program that uses the OpenMP API.
</p>
<h2>Implementations</h2>
<p>On the VSC clusters, the following compilers support OpenMP<span style=\"line-height: 1.5em; text-align: justify;\">:</span>
</p>
<ol>
	<li>
	<a href=\"/cluster-doc/development/toolchain-intel#intel-openmp\">Intel compilers</a> in the intel toolchain
	<ol>
		<li>
		The Intel compiler version 13.1 (intel/2014a and intel/2014b toolchains) implement the OpenMP 3.1 specification
		</li>
		<li>
		The Intel compiler version 14.0 (installed on some systems outside the toolchains, sometimes in a package with icc/2013_sp1 in its name) implements the OpenMP 3.1 specification and some elements of the OpenMP 4.0 specification (which was only just approved when the compiler was released)
		</li>
		<li>
		The Intel compiler version 15.0 (intel/2015a  and intel/2015b toolchain) supports all of the OpenMP 4.0 specification except user-defined reductions. It supports offload to a Xeon Phi system (and to some Intel processor-integrated graphics, but that is not relevant on the VSC-clusters).
		</li>
		<li>The Intel compiler version 16.0 (intel/2016a and intel/2016b toolchains) offers almost complete OpenMP 4.0 support. User-defined reductions are now also supported.</li>
	</ol>
	</li>
	<li>
	<a href=\"/cluster-doc/development/toolchain-foss#foss-openmp\">GCC</a> in the foss toolchain
	<ol>
		<li>
		GCC versions 4.8.2 (foss/2014a toolchain) and 4.8.3 (foss/2014b toolchain) support the OpenMP 3.1 specification.
		</li>
		<li>
		GCC version 4.9.2 (foss/2015a toolchain) and 4.9.3 (foss/2015b and foss/2016a toolchains) support the full OpenMP 4.0 specification. However, \"offloaded\" code is run on the CPU and not on the GPU or any other accelerator. (In fact, OpenMP 4.0 is supported for C/C++ starting in GCC 4.9.0 and for Fortran in GC 4.9.1).
		</li>
		<li>
		GCC 5.4 (foss/2016b toolchain) offers full OpenMP 4.0 support and has the basics built in to support offloading. </li>
		<li>GCC 6.x (not yet part of a toolchain) offers full OpenMP 4.5 support in C and C++, including offloading to some variants of the Xeon Phi and to AMD HSAIL and some support for OpenACC on NVIDIA.</li>
	</ol>
	</li>
</ol>
<p>When developing your own software, this is the preferred order to select the toolchain. The GCC OpenMP runtime is for most applications inferior to the Intel implementation.
</p>
<p>We also assume you are already familiar with the job submission procedure. If not, check the \"<a href=\"/cluster-doc/running-jobs\">Running jobs</a>\" section first.
</p>
<h2>Compiling OpenMP code</h2>
<p>See the instructions on the <a href=\"/cluster-doc/development/toolchains\">page about toolchains</a> for compiling OpenMP code with the Intel and GNU compilers.
</p>
<p>Note that it is in fact possible to link OpenMP object code compiled with gcc and the Intel compiler on the condition that the Intel OpenMP libraries and run-time is used (e.g., by linking using icc with the -openmp option), but the Intel manual is not clear which versions of gcc and icc work together well. This is only for specialists but may be useful if you only have access to object files and not to the full source code.
</p>
<h2>Running OpenMP programs</h2>
<p>Since OpenMP is intended for use in a shared memory context, when submitting a job to the queue system, remember to request a single node (i.e., <code>-l nodes=1</code>) and as many processors as you need parallel threads (e.g., <code>-l ppn=4</code>). The latter should not exceed the number of cores on the machine the job runs on. For relevant hardware information, please consult the <a href=\"/infrastructure/hardware\">list of available hardware</a>.
</p>
<p>You may have to set the number of cores that the program should use by hand, e.g., when you don't use all cores on a node, because the mechanisms in the OpenMP runtime that recognize the number of cores, don't recognize the number of cores assigned to the job but the total number of cores. Depending on the program, this may be trough a command line option to the executable, a value in the input file or the environment variable <code>OMP_NUM_THREADS</code>. Failing to set this value may result in threads competing with each other for resources such as cache and access to the CPU and thus lower performance.
</p>
<h2>Further information</h2>
<ul>
	<li><a href=\"https://www.openmp.org\" target=\"_blank\">OpenMP.org</a> contains the specifications and some documentation. It is the web site of the OpenMP Architecture Review Board where the standard is discussed.</li>
	<li>See also the pages in the <a href=\"/support/tut-book\">tutorials section</a>, e.g. for <a href=\"/support/tut-book/books#OpenMP\">books</a> and <a href=\"/support/tut-book/web-tutorials\">online tutorials</a>. The <a href=\"https://computing.llnl.gov/tutorials/openMP/\" target=\"_blank\">tutorial at the site of Lawrence Livermore National Laboratory</a> (LLNL) is highly recommended.</li>
</ul>"
241,"","<h2>What are toolchains?</h2><p>A toolchain is a collection of tools to build (HPC) software consistently. It consists of
</p><ul>
	<li>compilers for C/C++ and Fortran,</li>
	<li>a communications library (MPI), and</li>
	<li>mathematical libraries (linear algebra, FFT).</li>
</ul><p>Toolchains at the VSC are versioned, and refreshed twice a year. All software available on the cluster is rebuild when a new version of a toolchain is defined to ensure consistency. Version numbers consist of the year of their definition, followed by either <code>a</code>
	or <code>b</code>, e.g.,
	<code>2014a</code>. 
Note that the software components are not necessarily the most recent releases, rather they are selected for stability and reliability.
</p><h2>Available toolchains at the VSC</h2><p>Two toolchain flavors are standard across the VSC on all machines that can support them:
</p><ul>
	<li><a href=\"https://www.vscentrum.be/cluster-doc/development/toolchain-intel\">Intel toolchain</a>, based on Intel software components, </li>
	<li><a href=\"https://www.vscentrum.be/cluster-doc/development/toolchain-foss\">FOSS toolchain</a>, based on free and open source software.</li>
</ul><p>It may be of interest to note that the Intel C/C++ compilers are more strict with respect to the standards than the GCC C/C++ compilers, while for Fortran, the GCC Fortran compiler tracks the standard more closely, while Intel's Fortran allows for many extensions added during Fortran's long history. When developing code, one should always build with both compiler suites, and eliminate all warnings.
</p><p>On average, the Intel compiler suite produces executables that are 5 to 10 % faster than those generated using the GCC compiler suite. However, for individual applications the differences may be more significant with sometimes significantly faster code produced by the Intel compilers while on other applications the GNU compiler may produce much faster code.
</p><p>Additional toolchains may be defined on specialised hardware to extract the maximum performance from that hardware.
</p><ul>
	<li>On Cerebro, the SGI UV shared memory system at the KU Leuven, you need to use the SGI MPI-library (called MPT for Message Passing Toolkit) to get the maximum performance from the interconnect (which offers hardware acceleration for some MPI functions). On that machine, two additional toolchains are defined, <code>intel-mpt</code> and 
	<code>foss-mpt</code>, equivalent to the standard 
	<code>intel</code> and 
	<code>foss</code> 
	toolchains respectively but with the MPI library replaced with MPT.
	</li>
</ul><p>For detailed documentation on each of these toolchains, we refer to the pages linked above in this document.</p>"
243,"","<h2>Why use a version control system?</h2><p>A version control systems (VCS) help you manage the changes to the source files of your project, and most systems also support team development. Since it remembers the history of your files, you can always return to an earlier version if you've screwed up making changes. By adding comments when you store a new version in the VCS it also becomes much easier to track which change was made for what purpose at what time. And if you develop in a team, it helps to organise making coordinated changes to the code base, and it supports co-development even across file system borders (e.g., when working with a remote partner).
</p><p>Most Integrated Development Environments (IDE) offer support for one or more version control systems. E.g., Eclipse, the IDE which we recommend for the development of C/C++ or Fortran codes on clusters, supports all of the systems mentioned on this page, some out-of-the-box and others by adding an additional package. The systems mentioned on this page are all available on Linux, OS X and Windows (through the UNIX emulation layer cygwin and all except RCS also in at least one native implementation).
</p><h2>Types of version control systems</h2><p>An excellent introduction to the various types of version control systems can be found in <a href=\"https://git-scm.com/book/en/v2\" target=\"_blank\">the book Pro GIT by Scott Chacon and Ben Straub</a>.
</p><h3>Local systems</h3><p>These first generation systems use a local database that stores previous versions of files. One of the most popular examples of this type is the venerable RCS (Revision Control System) system, distributed with many UNIX-like systems. It works by keeping patch sets (differences between various versions of a file) in a special format on disk. It can then return to a previous version of a file by adding up all the patches.
</p><p>RCS and other \"local systems\"  are very outdated. Hence we advise you to use one of the systems from the next two categories.
</p><p>Links:
</p><ul>
	<li>
	<a href=\"https://en.wikipedia.org/wiki/Revision_Control_System\" target=\"_blank\">Wikipedia page</a>
	</li>
	<li>
	<a href=\"https://www.gnu.org/software/rcs/rcs.html\" target=\"_blank\">GNU RCS</a>
	</li>
</ul><h3>Centralised systems</h3><p>Centralised version control systems were developed to enable people to collaborate on code and documents with people on different systems that may not share a common file system. The version files are now maintained by a server to which multiple clients can connect and check out files, and the systems help to manage concurrent changes to a file by several users (through a copy-modify-merge procedure). Popular examples of this type are CVS (Concurrent Versions System) and SVN (Subversion). Of those two, SVN is the more recent system while CVS is no longer further developed and less and less used.
</p><p>Links:
</p><ul>
	<li>
	<a href=\"https://en.wikipedia.org/wiki/Concurrent_Versions_System\" target=\"_blank\">CVS Wikipedia page</a>
	</li>
	<li>
	<a href=\"https://en.wikipedia.org/wiki/Apache_Subversion\" target=\"_blank\">SVN Wikipedia page</a>
	</li>
	<li>
	CVS implementations
	<ul>
		<li>
		A command-line client is included in most Linux distributions. On Windows, the cygwin UNIX emulation layer also has a svn package. On OS X, it is available (though no longer maintained) through the MacPorts project.
		</li>
		<li>
		The eclipse IDE comes with built-in support for CVS.
		</li>
	</ul>
	</li>
	<li>
	SVN implementations
	<ul>
		<li>
		Command-line clients are included in most Linux distributions and OS X. On Windows, the cygwin UNIX emulation layer also has a svn package. The command line client is also available on the VSC clusters.
		</li>
		<li>
		<a href=\"/client/windows/tortoisesvn\">TortoiseSVN</a> (or <a href=\"https://tortoisesvn.net/\" target=\"_blank\">go straight to the TortoiseSVN web site</a>) is a popular Windows native GUI client that integrates well with the explorer. However, if you google on \" SVN GUI\"  you'll find a plethora of other choices, not only for Windows but also for macOS and Linux
		</li>
		<li>
		SVN can be integrated with the eclipse IDE through the ¨Subversive SVN team provider¨ plugin which can be installed through the \"Install New Software\"  panel in the help menu. More information and instructions are available on the <a href=\"http://www.eclipse.org/subversive/\" target=\"_blank\">subversive subsite of the main eclipse web site</a>.
		</li>
	</ul>
	</li>
</ul><h3>Distributed systems</h3><p>The weak point of the centralised systems is that they require you to be online to checkout a file or to commit a revision. In a distributed system, the clients mirror the complete repository and not just the latest version of each file. When online, the user can then synchronise the local repository with the copy on a server. In a single-user scenario you can still keep all your files in the local repository without using a server, and hence it doesn't make sense anymore to still use one of old local-only version control systems. The disadvantage of a distributed system is that you are not forced to synchronise after every commit, so that the local repositories of various users on a project can be very much out-of-sync with each other, making the job harder when those versions have to be merged again.
</p><p>Popular examples of systems of this type are Git (originally developed to manage the Linux kernel project) and Mercurial (sometimes abbreviated as Hg, chemists will understand why).
</p><p>Links:
</p><ul>
	<li>
	<a href=\"https://en.wikipedia.org/wiki/Git_(software)\" target=\"_blank\">Git on Wikipedia</a>
	</li>
	<li>
	<a href=\"https://git-scm.com/\" target=\"_blank\">Main Git web page</a>
	</li>
	<li>
	<a href=\"https://en.wikipedia.org/wiki/Mercurial\" target=\"_blank\">Mercurial on Wikipedia</a>
	</li>
	<li>
	<a href=\"https://www.mercurial-scm.org\" target=\"_blank\">Main Mercural web page</a>
	</li>
	<li>
	Git implementations
	<ul>
		<li>
		If you have a Linux system, Git is most likely already installed on your system. On OS X, git is available through Xcode, though it is not always the most recent version. On Windows, there is a Git-package in the UNIX emulation layer Cygwin. Downloads for all systems are also available on <a href=\"https://git-scm.com/download\" target=\"_blank\">the download section of the main git web site</a>. That page also contains links to a number of GUI options. Most if not all GUI tools store projects in a way that is fully compatible with the command line tools, so you can use both simultaneously.
        The command line client is also available on the VSC clusters.
		</li>
		<li>
		<a href=\"https://tortoisegit.org/\" target=\"_blank\">TortoiseGit</a> is an explorer-integrated interface to Git on Windows similar to TortoiseSVN.
		</li>
		<li>
		Another nice GUI application is <a href=\"https://www.atlassian.com/software/sourcetree\" target=\"_blank\">SourceTree</a> produced by <a href=\"https://www.atlassian.com/\" target=\"_blank\">Atlassian</a>. Atlassian is the company behind the Bitbucket cloud service, but their tool also works well with GitHub, one of their main competitors. It has a very nice way of representing the history of a local repository.
		</li>
		<li>
		The Eclipse IDE comes with built-in support for Git through the standard plug-in EGit. More recent versions of this plugin may be available through the <a href=\"https://marketplace.eclipse.org/\" target=\"_blank\">Eclipse Marketplace</a>.
		</li>
		<li>
		<a href=\"https://www.collab.net/products/giteye\" target=\"_blank\">CollabNet GitEye</a> is a Git GUI for Windows, OS X and Linux build on top of a number of Eclipse libraries, but you don/t need to install Eclipse to be able to use it. It is a nice way though to browse through your Git repositories outside of the Eclipse environment. GitEye itself is free and integrates with several git cloud services and bugtracking services.
		</li>
	</ul>
	</li>
	<li>
	Mercurial (Hg) implementations
	<ul>
		<li>
		Mercurial is written in Python and hence runs on most systems. Most Linux distributions offer a mercurial package. <a href=\"https://www.mercurial-scm.org/\" target=\"_blank\">Windows and OS X command line utilities are also available</a>.
		</li>
		<li>
		<a href=\"https://tortoisehg.bitbucket.io/\" target=\"_blank\">TortoiseHg</a> is an explorer-integrated interface to the Mercurial VCS on Windows similar to TortoiseSVN. There is also an OS X and Linux version available. The latter integrates with Gnome/Nautilus.
		</li>
		<li>
		The eclipse IDE supports Mercurial through the <a href=\"https://marketplace.eclipse.org/content/mercurialeclipse\" target=\"_blank\">optional MercurialEclipse plugin</a> available on the <a href=\"https://marketplace.eclipse.org/\" target=\"_blank\">Eclipse Marketplace</a>.
		</li>
	</ul>
	</li>
</ul><h2>Cloud services</h2><p>Many companies offer hosting services for SVN, Git or Mercurial repositories in the cloud. Google, e.g., for <a href=\"https://www.google.be/webhp?#q=subversion+hosting+service\" target=\"_blank\">subversion hosting service</a>, <a href=\"https://www.google.be/search?q=git+hosting+service\" target=\"_blank\">git hosting service</a> or <a href=\"https://www.google.be/search?q=mercurial+hosting+service\" target=\"_blank\">mercurial hosting service</a>. Several offer free public hosting for Open Source projects or have free access for academic accounts. Some noteworthy ones that are popular for academic projects are:
</p><ul>
	<li><a href=\"https://github.com/\" target=\"_blank\">Github (github.com)</a> offers free Git and Subversion hosting for Open Source projects. We use this service for some VSC in-house tools development. It is also possible to host private projects if you subscribe to one of their paying plans.</li>
	<li>
	<a href=\"https://bitbucket.org/\" target=\"_blank\">Bitbucket (bitbucket.org)</a> offers both git and mercurial services. It also supports private projects with a limited number of users in free accounts (and has a special deal for academic institutions, allowing unlimited users) while the other services mentioned on this page only support open source projects for free.
	</li>
	<li><a href=\"https://sourceforge.net/\" target=\"_blank\">SourceForge</a> is a very well known service for hosting Open Source projects. It currently supports projects managed through Subversion, Git, Mercurial and a few other systems.</li>
</ul><p>However, we urge you to always carefully check the terms-of-use of these services to assure that, e.g., the way they deal with intellectual property is in line with your institute's requirements.
</p><h2>Which one should I use?</h2><p>It is not up to us to make this choice for you, but here are a number of elements that you should take into account:
</p><ul>
	<li>
	Subversion, Git and Mercurial are all recent systems that are well maintained and supported by several hosting services.
	</li>
	<li>
	Subversion and Git are installed on most VSC systems. We use Git ourselves for some of our in-house development.
	</li>
	<li>
	Centralised version management systems have a simpler concept than the distributed ones, but if you expect prolonged periods that you are offline, you have to keep in mind that you cannot make any commits during that period.
	</li>
	<li>
	As you have only a single copy of the repository in a centralised system, a reliable hosting service or a good backup strategy is important. In a distributed system it would still be possible to reconstruct the contents of a repository from the other repositories.
	</li>
	<li>
	If you want to use an IDE, it is good to check which systems are supported by the IDE. E.g., Eclipse supports Git out-of-the-box, and Subversion and Mercurial though a plug-in. Visual Studio also supports all three of these systems.
	</li>
</ul>"
245,"","<p>This tutorial explains some of the basic use of the git command line client. It does not aim to be a complete tutorial on git but rather a brief introduction explaining some of the issues and showing you how to house your git repository at the VSC. At the end of this text, we provide some links to further and more complete documentation.
</p>
<h2>Preparing your local machine for using git</h2>
<p>It is best to first configure git on your local machine using git config.
</p>
<pre>git config --global user.name \"Kurt Lust\"
git config --global user.email kurt.lust@uantwerpen.be
git config --global core.editor vi
</pre>
<p>These settings are stored in the file .gitconfig in your home directory (OS X, Linux, Cygwin). The file is a simple user-editable text file.
</p>
<h2>Some remarks on accessing a remote repository using command line tools</h2>
<p>Many cloud git hosting services offer a choice between ssh and https access to a repository through the git command line tools. If you want to use one of the VSC clusters for a remote repository, you'll have to use the ssh protocol.
</p>
<h3>Https access</h3>
<p>Https access uses your account and password of the cloud service. Every time you access the remote repository, the git command-line client will ask for the password. This can be solved by using a credential manager in recent versions of the git client (1.7.9 and newer).
</p>
<ul>
	<li>
	On Windows, the <a href=\"https://github.com/Microsoft/Git-Credential-Manager-for-Windows\">Git Credential Manager for Windows</a> can be used to safely store your password in the Windows credential store.
	</li>
	<li>
	The Apple OS X version of git comes with the credential manager comes with credential-osxkeychain to store your credentials in the OS X keychain. Enable it using
	<code>git config --global credential.helper osxkeychain</code>
	</li>
	<li>
	There are also various solutions for Linux systems, e.g., using the gnome keyring. To tell git to use it, use:
	<code>git config --global credential.helper /usr/share/doc/git/contrib/credential/gnome-keyring/git-credential-gnome-keyring</code>
	This of course depends on the setup of your Linux machine. The program might not be installed or might be installed in a different directory.
	</li>
	<li>
	Various GUI clients may have their own way of managing the credentials.
	</li>
</ul>
<h3>Ssh access</h3>
<p>The git command line client uses the standard ssh mechanism to manage ssh keys. It is sufficient to use an ssh agent (as you are probably using already when you log on to the VSC clusters) and load the key for the service in the agent (using ssh-add).
</p>
<h2>Setting up a new repository</h2>
<h3>Getting an existing code base into a local repository</h3>
<p>Git stores its repository with your code in a hidden directory.
</p>
<ol>
	<li>
	Go to the top directory of what has to become your repository (most likely the top directory of the files that you want to version control) and run
	<code>git init</code>
	This wil create a hidden .git subdirectory with the local repository database.
	</li>
	<li>
	Now you can add the files to your new repository, e.g., if you want to add all files
	<code>git add .</code>
	(don't forget the dot at the end, it means add the current directory!)
	</li>
	<li>
	Next you can do your first commit:
	<code>git commit -m \"Project brought under Git control\"</code>
	</li>
	<li>
	And you're done! The current version of your files is now stored in your local repository. Try, e.g.,
	<code>git show</code><br>
	<code>git status</code>
	to get some info about the repository.
	</li>
</ol>
<h3>Bringing an existing local repository into a cloud service</h3>
<p>Here we assume that you have a local repository and now want to put it into a cloud service to collaborate with others on a project.
</p>
<p><i>You may want to make a backup of your local repository at this point in case things go wrong.</i>
</p>
<ol>
	<li>
	Create an empty project on your favorite cloud service. Follow the instructions provided by the service.
	</li>
	<li>
	Now you'll need to learn your local repository about the remote one. Most cloud services have a button to show you the URL to the remote repository that you have just set up, either using the http or the ssh-based protocol. E.g.,
	<code>git remote add origin ssh://git@bitbucket.org/username/myproject.git</code>
	connects to the repository myproject on Bitbucket. It will be known on your computer with the short name origin. The short name saves you from having to use the full repository URL each time you want to refer to it
	</li>
	<li>
	Push the code from your local repository into the remote repository.
	<code>git push -u --mirror origin</code>
	will create a mirror of your local repository on the local site. Use the --mirror option with care, as it may destroy part of your remote repositiory if that one is not empty and contains information that is not contained in your local repository!
	</li>
</ol>
<p>You can also use the procedure to create a so-called bare remote repository in your account on the VSC clusters. A bare repository is a repository that does not also contain its own source file tree, so you cannot edit directly in that directory and also use it as a local repository on the cluster. However, you can push to and pull from that repositiory, so it will work just like a repository on one of the hosting services. The access to the repository will be through ssh. The first two steps have to be modified:
</p>
<ol>
	<li>
	To create an empty repository, log in to your home cluster and go to the directory where you want to store the repository. Now create the repository (assuming its name is repository-name):
	<code>git init --bare repository-name</code>
	This will create the directory repositiory-name that stores a lot of files which together are your git repository.
	</li>
	<li>
	The URL to the repository will be of the form vscXXXXX@vsc.login.node:&lt;full path to the repository&gt;, e.g., if you're vsc20XYZ (a UANtwerpen account) and the repository is in the subdirectory testrepository of your data directory, the URL is vsc20XYZ@login.hp.uantwerpen.be:/data/antwerpen/20X/vsc20XYZ/testrepository. So use this URL in the git remote add command. You don't need to specify ssh:// in the URL if you use the scp-syntax as we did in this example above.
	</li>
</ol>
<p>The access to this repository will be regulated through the file access permissions for that subdirectory. Everybody who has read and write access to that directory, can also use the repository (but using his/her own login name in the URL of course as VSC accounts should not be shared by multiple users).
</p>
<p>NOTE: Technically speaking, git can also be used in full peer-to-peer mode where all repos also have a source directory in which files can be edited. It does require a good organisation of the work flow. E.g., different people in the team should not be working in the same branch as one cannot push changes to a repo for the branch that is active (i.e., mirrored in the source files) as this may create an inconsistent state. So our advise is that if you want to use the cluster as a git server and also edit files on the cluster, you simply use two repositories: one that you use as a local repository in which you also work and one that is only used as a central repository to which various users push changes to and pull changes from.
</p>
<h3>As a clone from an existing local or remote repository</h3>
<p>Another way to create a new repository is from an existing repository on your local machine or on a remote service. The latter is useful, e.g., if you want to join an existing project and create a local copy of the remote repository on your machine to do your own work. This can be accomplished through cloning of a repository, a very easy operation in git as there is a command that combines all necessary steps in a single command:
</p>
<ol>
	<li>
	Go to the directory were you want to store the repository and corresponding source tree (in a subdirectory of that directory called directoryname).
	</li>
	<li>
	You have to know the URL to the repository that you want to clone. But once you know the URL, all you need to do is
	<code>git clone URL directoryname</code>
	where you replace URL with the URL of the repository that you want to clone.
	</li>
</ol>
<p>Note: If you start from scratch and want to use a remote repository in one of the cloud services, it might be easiest to first a repository over there using the instructions of the server system or cloud service, and then clone that (even if it is still empty) to a local repository on which you actually work.
</p>
<h2>Working with your local repository</h2>
<p>If you are only using a local repository, the basic workflow to add the modifications to the git database is fairly simple:
</p>
<ol>
	<li>
	Edit the files.
	</li>
	<li>
	Add the modified files to the index using:
	<code>git add filename</code>
	This process is called staging.
	</li>
	<li>
	You can continue to further edit files if you want and also stage them.
	</li>
	<li>
	Commit all staged files to the repository:
	<code>git commit</code>
	Git will ask you to enter a message describing the commit, or you can specify a message with the <code>-m</code> option.
	</li>
</ol>
<p>This is not very exciting though. Version control becomes really useful once you want to return to a previous version, or create a branch of the code to try something out or fix a bug without immediately changing the main branch of the code (that you might be using for production use). You can then merge the modifications back into you main code. Branching and merging branches are essential in all this. In fact, if you use git to collaborate with others you'll be confronted with branches sooner rather than later. In fact, every git repository has at least one branch, the main branch, as
</p>
<p><code>git status</code>
</p>
<p>shows.
</p>
<p>Assume you want to start a new branch to try something without affecting your main code, e.g., because you also want to further evolve your main code branch while you're working. You can create a branch (let's assume we name it branch2) with
</p>
<p><code>git branch branch2</code>
</p>
<p>And then switch to it with
</p>
<p><code>git checkout branch2</code>
</p>
<p>Or combine both steps with
</p>
<p><code>git checkout -b branch2.</code>
</p>
<p>You can then switch between this branch and the master branch with
</p>
<p><code>git checkout master</code>
</p>
<p>and
</p>
<p><code>git checkout branch2</code>
</p>
<p>at will and make updates to the active branch using the regular git add and git commit cycle.
</p>
<p>The second important operation with branches, is merging them back together. One way to do this is with git merge. Assume you want to merge the branch branch2 back in the master branch. You'd do this by first switching to the master branch using
</p>
<p><code>git checkout master</code>
</p>
<p>and then ask git to merge both branches:
</p>
<p><code>git merge branch2</code>
</p>
<p>Git will do a good effort to merge both sets of modifications since their common ancestor, but this may not always work, especially if you've made changes to the same area of a file on both branches. Git will then warn you that there is a conflict for certain files, after which you can edit those files (the conflicts zones will be clearly marked in the files), add them to the index and commit the modifications again.
</p>
<p>When learning to work with this mechanism, it is very instructive to use a GUI that depicts all commits and branches in a graphical form, e.g., the program SourceTree mentioned before.
</p>
<h2>Synchronising with a remote repository</h2>
<p>If you want to collaborate with other people on a project, you need multiple repositories. Each person has his or her own local repository on his or her computer. The workflow is the simplest if you also have a repository that is used to collect all contributions. The collaboration mechanism though synchronisation of repositories relies very much on the branching mechanism to resolve conflicts if several contributors have made modifications to the repository.
</p>
<ul>
	<li>
	To push modifications that you have made in your local repository to a different repository, use
	<code>git push -u remote_name</code>
	where you replace remote_name with the shorthand for the remote repository.
    This process may fail however if someone else had made modifications to the same branch in the repository that you're pushing. Git will then warn you and ask you to first fetch the modifications that others have made and merge them into your code before trying another pull.
	</li>
	<li>
	The opposite of push is fetch and merge or pull. You'll need to do this to see and integrate modifications that others have made to the repository. The first step is to update your repository with the contents of the remote repository. Assume the remote repository has the shorthand name origin.
	<code>git fetch origin</code>
	will get all the information from the repositiory origin in your local repositiory, but it will not change your work files. If you try
	<code>git branch -av</code>
	To get an overview of all branches in your local repository and information about the latest commit for each branch, you'll see that there might be a number of branches with a name that starts with origin/ in the repository. That means that there were commits in the remote repository that were newer than the data you last synchronised with, and you'll need to merge them into your working code base. E.g., if you're working on the branch master and someone else has made changes to that branch also, there will now be a branch origin/master in your repository with a more recent commit. You merge it again into your code with
	<code>git merge origin/master</code>
	(and you may have to resolve some conflicts here which you'd have to resolve and commit as before).
	</li>
	<li>
	After a git fetch you may also note that someone else has added a new branch. Assume, e.g., that git branch -av tells you there is now a branch origin/branch3 and that you want to collaborate to that branch also. Before you can do so, you'll first have to create a local so-called tracking branch, by using
	<code>git checkout -b branch3 origin/branch3</code>
	which will also switch to that branch and update the files in your workspace accordingly, or if you just want to create the tracking branch for later use without switching to it now,
	<code>git branch branch3 origin/branch3</code>
	</li>
</ul>
<h2>Further information</h2>
<p>We have only covered the bare essentials of git (and even less then that). Due to its power, it is also a fairly complicated system to use. If you want to know more about git or need a more complete tutorial, we suggest you check out the following links:
</p>
<ul>
	<li>
	There are some good books about git freely available on the internet:
	<ul>
		<li>
		<a href=\"https://git-scm.com/book/en/v2\" target=\"_blank\">Git Pro</a>
		</li>
		<li>
		<a href=\"http://documentup.com/skwp/git-workflows-book\" target=\"_blank\">Git Workflows</a>
		</li>
		<li>
		<a href=\"https://github.com/pluralsight/git-internals-pdf/releases\" target=\"_blank\">PeepCode Git Internals</a>
		</li>
	</ul>
	</li>
	<li>
	There is also a <a href=\"https://git-scm.com/docs\" target=\"_blank\">full command reference available on the web</a> for the command-line git tool.
	</li>
	<li>
	And you can also find good git tutorials on the web, e.g., on <a href=\"https://git-scm.com/doc/ext\" target=\"_blank\">the \"External Links\" page of the main git web site</a>.
	</li>
</ul>"
247,"","<h2>Preparation</h2><p>The Subversion software is installed on the cluster. On most systems it is default software and does not need a module (try <code>which svn</code> and &lt;code&gt;which svnadmin to check if the system can find the subversion commands). On some systems you may have to load the appropriate module, i.e.,
</p><pre>$ module load subversion
</pre><p>When you are frequently using Subversion, it may be convenient to load this module from your '.bashrc' file. (Note that in general we strongly caution against loading modules from '.bashrc', so this is an exception.)
</p><p>Since some Subversion operations require editing, it may be convenient to define a default editor in your '.bashrc' file. This can be done by setting the 'EDITOR' variable to the path of your favorite editor, e.g., emacs. When this line is added to your '.bashrc' file, Subversion will automatically launch this editor whenever it is required.
</p><pre>export EDITOR=/usr/bin/emacs
</pre><p>Of course, any editor you are familiar with will do.
</p><h2>Creating a repository</h2><p>To create a Subversion repository on a VSC cluster, the user first has to decide on its location. We suggest to use the data directory since
</p><ol>
	<li>its default quota are quite sufficient;</li>
	<li>if the repository is to be shared, the permissions on the user's home directory need not to be modified, hence decreasing potential security risks; and</li>
	<li>only for users of the K.U.Leuven cluster, the data directory is backed up (so is the user's home directory, incidently).</li>
</ol><p>Actually creating a repository is very simple:
</p><pre>$ cd $VSC_DATA
$ svnadmin create svn-repo
</pre><ol>
	<li>Log in on the login node.</li>
	<li>Change to the data directory using</li>
	<li>Create the repository using</li>
</ol><p>Note that a directory with the name 'svn-repo' will be created in your '$VSC_DATA' directory. You can choose any name you want for this directory. Do not modify the contents of this directory since this will corrupt your repository unless you know quite well what you are doing.
</p><p>At this point, it may be a good idea to read the section in the Subversion book on the <a href=\"http://svnbook.red-bean.com/en/1.5/svn.reposadmin.planning.html#svn.reposadmin.projects.chooselayout\">repository layout</a>. In this How-To, we will assume that each project has its own directory at the root level of the repository, and that each project will have a 'trunk', 'branches' and 'tags' directory. This is recommended practice, but you may wish to take a different approach.
</p><p>To make life easier, it is convenient to define an environment variable that contains the URI to the repository you just created. If you work with a single repository, you may consider adding this to your '.bashrc' file.
</p><pre>export SVN=\"svn+ssh://vsc98765@vsc.login.node DATA/svn-repo\"
</pre><p>Here you would replace 'vsc98765' by your own VSC user ID, 'vsc.login.node' by the login node of your VSC cluster, and finally, 'DATA' by the value of your '$VSC_DATA' variable.
</p><h2>Putting a project under Subversion control</h2><p>Here, we assume that you already have a directory that contains an initial version of the source code for your project. If not, create one, and populate it with some relevant files. For the purpose of this How-To, the directory currently containing the source code will be called '$VSC_DATA/simulation', and it will contain two source files, 'simulation.c' and 'simulation.h', as well as a make file 'Makefile'.
</p><h3>Preparing the repository</h3><p>Since we follow the Subversion community's recommended practice, we start by creating the appropriate directories in the repository to house our project.
</p><pre>$ svn mkdir -m 'simulation: creating dirs' --parents   \\
            $SVN/simulation/trunk    \\
            $SVN/simulation/branches \\
            $SVN/simulation/tags
</pre><p>The repository is now prepared so that the actual code can be imported.
</p><h3>Importing your source code</h3><p>As mentioned, the source code for your project exists in the directory '$VSC_DATA/simulation'. Since the semantics of the 'trunk' directory of a project is that this is the location where the bulk of the development work is done, we will import the project into the trunk.
</p><pre>$ svn import -m 'simulation: import' \\
             $VSC_DATA/simulation   \\
             $SVN/simulation/trunk
</pre><ol>
	<li>First, prepare the source directory '$VSC_DATA/simulation' by deleting all files that you don't want to place under version control. Remove artefacts such as, e.g., object files or executables, as well as text files not to be imported into the repository.</li>
	<li>Now the directory can be imported by simply typing:</li>
</ol><p>The project is now almost ready for development under version control.
</p><h3>Checking out</h3><p>Although the source directory has been imported into the subversion repository, this directory is <i>not</i> under version control. We first have to check out a working copy of the directory.
</p><p>Since you are not yet familiar with subversion and may have made a mistake along the way, it may be a good idea at this point to make a backup of the original directory first, by, e.g.,
</p><pre>$ tar czf $VSC_DATA/simulation.tar.gz $VSC_DATA/simulation
</pre><p>Now it is safe to checkout the project from the repository using:
</p><pre>$ svn checkout $SVN/simulation/trunk $VSC_DATA/simulation
</pre><p>Note that the existing files in the'$VSC_DATA/simulation' directory have been replaced by those downloaded from the repository, and that a new directory '$VSC_DATA/simulation/.svn' has been created. It is the latter that contains the information needed for version control operations.
</p><h2><a name=\"subversion-work-cycle\"></a>Subversion work cycle</h2><p>The basic work cycle for development on your project is fairly straightforward.
</p><pre>$ cd $VSC_DATA/simulation
$ svn update
$ svn add utils.c utils.h
$ svn status
$ svn commit -m 'simulation: implemented a very interesting feature'</pre><ol>
	<li>Change to the directory containing your project's working copy, e.g.,</li>
	<li>Update your working copy to the latest version, see the <a href=\"#updating-the-working-copy\">section on updating</a> below for a brief introduction to the topic.</li>
	<li>Edit the project's files to your heart's content, or add new files to the repository after you created them, e.g., 'utils.c' and 'utils.h'. Note that the new files will only be stored in the repository upon the next commit operation, see below.</li>
	<li>Examine your changes, this will be elaborated upon in the <a href=\"#examening-status\">next section</a>.</li>
	<li>Commit your changes, i.e., all changes you made to the working copy are now transfered to the repository as a new revision.</li>
	<li>Repeat steps 2 to 5 until you are done.</li>
</ol><p>If you are the sole developer working on this project and exclusively on the VSC cluster, you need not update since your working copy will be the latest anyway. However, an update is vital when others can commit changes, or when you work in various locations such as your desktop or laptop.
</p><h2><a name=\"other-subversion-features\"></a>Other subversion features</h2><p>It would be beyond the scope of this How-To to attempt to stray too far from the mechanics of the basic work cycle. However, a few features will be highlighted since they may prove useful.
</p><p>A central concept to almost all version control systems is that of a version number. In Subversion, all operations that modify the current version in the repository will result in an automatic increment of the revision number. In the example above, the 'mkdir' would result in revision 1, the 'import' in revision 2, and each consecutive 'commit' will further increment the version number.
</p><h3>Reverting to a previous version</h3><p>The most important point of any version control system is that it is possible to revert to some revision if necessary. Suppose you want to revert to the state of the original import, than this can be accomplished as follows:
</p><pre>$ svn checkout -r 2 $SVN/simulation/trunk simulation-old
</pre><h3>Finding changes between revisions</h3><p>Finding changes between revisions, or between a certain revision and the current state of the working copy is also fairly easy:
</p><pre>$ svn diff -r HEAD simulation.c
</pre><h3>Examining history</h3><p>To many Subversion operations, e.g., 'mkdir' and 'commit', with a message can be added (the '-m &lt;string&gt;' in the commands of the previous section), and they will be associated with the resulting revision number. When used consistently, these comments can be very useful since they can be reviewed later whenever one has to examine changes made to the project. If a repository hosts multiple projects, it is wise to have some sort of convention, e.g., to start the comments on a project by its name as a tag. Note that this convention was followed in the examples above. One can for instance show all messages associated with changes to the file 'simulation.c' using:
</p><pre>$ svn log simulation.c
</pre><h3>Deleting and renaming</h3><p>When a file is no longer needed, it can be removed from the current version in the repository, as well as from the working copy.
</p><pre>$ svn rm Makefile
</pre><p>The previous command would remove the file 'Makefile' from the working directory, and tag it for deletion from the current revision upon the next commit operation. Note that the file is not removed from the repository, it is still part of older revisions.
</p><p>Similarly, a file may have to be renamed, an operation that is also directly supported by Subversion.
</p><pre>$ svn mv utils.c util.c
</pre><p>Again, the change will only be propagated to the repository upon the next commit operation.
</p><h3><a name=\"examening-status\"></a>Examining status</h3><p>While development progresses, the working copy differs more and more from the latest revision in the repository, i.e., HEAD. To get an overview of files that were modified, added, deleted, etc., one can examine the status.
</p><pre>$ svn status
</pre><p>This results in a list of files and directories, each preceeded by a character:
</p><ul>
	<li>M: file is modified</li>
	<li>A: file has been added</li>
	<li>D: file has been deleted</li>
	<li>?: file is not (yet) under version control (it should be added if it needs to be)</li>
</ul><p>When nothing has been modified since the last commit, this command shows no output.
</p><h3><a name=\"updating-the-working-copy\"></a>Updating the working copy</h3><p>When the latest revision in the repository has changed with respect to the working copy, an update of the latter should be done before continuing the development.
</p><pre>$ svn update
</pre><p>This may be painless, or require some work. Subversion will try to reconsilliate the revision in the repository with your working copy. When changes can safely be applied, subversion does so automatically. The output of the 'update' command is a list of files, preceeded by characters denoting status information:
</p><ul>
	<li>A: file was not in the working copy, and has now been checked out.</li>
	<li>U: file was modified in the repository, but not in the working copy, the latter has been modified to reflect the changes.</li>
	<li>G: file was modified in both the working copy and the repository, the changes have been merged automatically.</li>
</ul><p>In case of conflict, e.g., the same line of a file was changed in both the repository and the working copy, Subversion will offer a number of options to resolve the conflict.
</p><pre>Conflict discovered in 'simulation.c'.
Select: (p) postpone, (df) diff-full, (e) edit,
 (mc) mine-conflict, (tc) theirs-conflict,
 (s) show all options:
</pre><p>The safest option is to choose to edit the file, i.e., type 'e'. The file will be opened in an editor with the conflicts clearly marked. An example is shown below:
</p><pre>&lt;&lt;&lt;&lt;&lt;&lt;&lt; .mine
 printf(\"bye world simulation!\\n\");
=======
 printf(\"hello nice world simulation\\n\");
&gt;&gt;&gt;&gt;&gt;&gt;&gt; .r7
</pre><p>Here '.mine' indicates the state in your working copy, '.r7' that of revision 7 (i.e., HEAD) in the repository. You can now resolve them manually by editing the file. Upon saving the changes and quiting the editor, the option 'resolved' will be added to the list above. Enter 'r' to indicate that the conflict has indeed been resolved successfully.
</p><h3>Tagging</h3><p>Some revisions are more important than others. For example, the version that was used to generate the data you used in the article that was submitted to Nature is fairly important. You will probably continue to work on the code, adding several revisions while the referees do their job. In their report, they may require some additional data, and you will have to run the program as it was at the time of submission, so you want to retrieve that version from the repository. Unfortunately, revision numbers have no semantics, so it will be fairly hard to find exactly the right version.
</p><p>Important revisions may be tagged explicitly in Subversion, so choosing an appropriate tag name adds semantics to a revision. Tagging is essentially copying to the tags directory that was created upon setting up the repository for the project.
</p><pre>$ svn copy --parents -m 'simulation: tagging Nature submission' \\
           $SVN/simulation/trunk           \\
           $SVN/simulation/tags/nature-submission
</pre><p>It is now trivial to check out the version that was used to compute the relevant data:
</p><pre>$ svn checkout $SVN/simulation/tags/nature-submission \\
               simulation-nature
</pre><h2>Desktop access</h2><p>It is also possible to access VSC subversion repositories from your desktop. See the pages in the <a href=\"/client/windows\">Windows client</a>, <a href=\"/client/macosx\">OS X client</a> en <a href=\"/client/linux\">Linux client</a> sections.
</p><h2>Further information on Subversion</h2><p>Subversion is a rather sophisticated version control system, and in this mini-tutorial for the impatient we have barely scratched the surface. Further information is available in an <a href=\"http://svnbook.red-bean.com/\" target=\"_blank\">online book on Subversion</a>, a must read for everyone involved in a non-trivial software development project that used subversion.
</p><p>Subversion can also provide help on commands:
</p><pre>$ svn help
$ svn help commit
</pre><p>The former lists all available subversion commands, the latter form displays help specific to the command, 'commit' in this example.
</p>"
249,"","<h2>Purpose</h2><p>Debugging MPI applications is notoriously hard. The Intel Trace Analyzer & Collector (ITAC) can be used to generate a trace while running an application, and visualizing it later for analysis.
</p><h2>Prerequisities</h2><p>You will need an MPI program (C/C++ or Fortran) to instrument and run.
</p><h2>Step by step</h2><p>The following steps are the easiest way to use the Intel Trace Analyzer, however, more sophisticated options are available.
</p><ol>
	<li>
	Load the relevant modules. The exact modules may differ from system to system, but will typically include the itac module and a compatible Intle toolchain, e.g.,
	<pre>$ module load intel/2015a
$ module load itac/9.0.2.045
	</pre>
	</li>
	<li>
	Compile your application so that it can generate a trace:
	<pre>$ mpiicc -trace myapp.c -o myapp
	</pre>
	where myapp.c is your C/C++ source code. For a Fortran program, this would be:
	<pre>$ mpiifort -trace myapp.f -o myapp
	</pre>
	</li>
	<li>
	Run your application using a PBS script such as this one:
	<pre>#!/bin/bash -l
#PBS -N myapp-job
#PBS -l walltime=00:05:00
#PBS -l nodes=4

module load intel/2015a
module load itac/9.0.2.045
# Set environment variables for ITAC.
# Unfortunately, the name of the script differs between versions of ITAC
source $EBROOTITAC/bin/itacvars.sh

cd $PBS_O_WORKDIR

mpirun -trace myapp
	</pre>
	</li>
	<li>
	When the job is finished, check whether files with names myapp.stf.* have been generated, if so, start the visual analyzer using:
	<pre>$ traceanalyzer myapp.stf
	</pre>
	</li>
</ol><h2>Further information</h2><p>Intel provides <a href=\"https://software.intel.com/en-us/articles/intel-trace-analyzer-and-collector-documentation/\" target=\"_blank\">product documentation</a> for ITAC.
</p>"
251,"","<h2><a name=\"intro\"></a>Introduction &amp; motivation</h2><p>When working on the command line such as in the Bash shell, applications support command line flags and parameters. Many programming languages offer support to conveniently deal with command line arguments out of the box, e.g., Python. However, quite a number of languages used in a scientific context, e.g., C/C++, Fortran, R, Matlab do not. Although those languages offer the necessary facilities, it is at best somewhat cumbersome to use them, and often the process is rather error prone.
</p><p>Quite a number of libraries have been developed over the years that can be used to conveniently handle command line arguments. However, this complicates the deployment of the application since it will have to rely on the presence of these libraries.
</p><p>ParameterWeaver has a different approach: it generates the necessary code to deal with the command line arguments of the application in the target language, so that these source files can be distributed along with those of the application. This implies that systems that don't have ParameterWeaver installed still can run that application.
</p><p>Using ParameterWeaver is as simple as writing a definition file for the command line arguments, and executing the code generator via the command lnie. This can be conveniently integrated into a standard build process such as make.
</p><p>ParameterWeaver currently supports the following target languages:
</p><ul>
	<li>C/C++</li>
	<li>Fortran 90</li>
	<li>R</li>
</ul><h2><a name=\"overview\"></a>High-level overview &amp; concepts</h2><h3>Parameter definition files</h3><p>A parameter definition file is a CSV text file where each line defines a parameter. A parameter has a type, a name, a default values, and optionally, a description. To add documentation, comments can be added to the definition file. The types are specific to the target language, e.g., an integer would be denoted by <tt>int</tt> for C/C++, and by <tt>integer</tt> for Fortran 90. The supported types are documented for each implemented target language.
</p><p>By way of illustration, a parameter definition file is given below for C as a target language, additional examples are shown in the target language specific sections:
</p><pre>int,numParticles,1000,number of particles in the system
double,temperature,273,system temperature in Kelvin
char*,intMethod,'newton',integration method to use
</pre><p>Note that this parameter definition file should be viewed as an integral part of the source code.
</p><h3>Code generation</h3><p>ParameterWeaver will generate code to
</p><ol>
	<li>initialize the parameter variables to the default values as specified in the parameter definition file;</li>
	<li>parse the actual command line arguments at runtime to determine the user specified values, and</li>
	<li>print the values of the parameters to an output stream.</li>
</ol><p>The implementation and features of the resulting code fragments are specific to the target language, and try to be as close as possible to the idioms of that language. Again, this is documented for each target language specifically. The nature and number of these code fragments varies from one target language to the other, again trying to match the language's idioms as closely as possible. For C/C++, a declaration file (<tt>.h</tt>) and a definition file (<tt>.c</tt>), while for Fortran 90 a single file (<tt>.f90</tt> will be generated that contains both declarations and definitions.
</p><h2><a name=\"language-specific\"></a>Language specific documentation</h2><h3>C/C++ documentation</h3><h4>Data types</h4><p>For C/C++, ParameterWeaver supports the following data types:
</p><ol>
	<li><tt>int</tt></li>
	<li><tt>long</tt></li>
	<li><tt>float</tt></li>
	<li><tt>double</tt></li>
	<li><tt>bool</tt></li>
	<li><tt>char *</tt></li>
</ol><h4>Example C program</h4><p>Suppose we want to pass command line parameters to the following C program:
</p><pre>#include 
#include 
#include 
int main(int argc, char *argv[]) {
    FILE *fp;
    int i;
    if (strlen(out) &gt; 0) {
        fp = fopen(out, \"w\");
    } else {
        fp = stdout;
    }
    if (verbose) {
        fprintf(fp, \"# n = %d\\n\", n);
        fprintf(fp, \"# alpha = %.16f\\n\", alpha);
        fprintf(fp, \"# out = '%s'\\n\", out);
        fprintf(fp, \"# verbose = %s\\n\", verbose);
    }
    for (i = 0; i &lt; n; i++) {
        fprintf(fp, \"%d\\t%f\\n\", i, i*alpha);
    }
    if (fp != stdout) {
        fclose(fp);
    }
    return EXIT_SUCCESS;
}
</pre><p>We would like to set the number of iterations <tt>n</tt>, the factor <tt>alpha</tt>, the name of the file to write the output to <tt>out</tt> and the verbosity <tt>verbose</tt> at runtime, i.e., without modifying the source code of this program.
</p><p>Moreover, the code to print the values of the variables is error prone, if we later add or remove a parameter, this part of the code has to be updated as well.
</p><p>Defining the command line parameters in a parameter definition file to automatically generate the necessary code simplifies matters considerably.
</p><h4>Example parameter definition file</h4><p>The following file defines four command line parameters named <tt>n</tt>, <tt>alpha</tt>, <tt>out</tt> and <tt>verbose</tt>. They are to be interpreted as <tt>int</tt>, <tt>double</tt>, <tt>char</tt> pointer and <tt>bool</tt> respectively, and if no values are passed via the command line, they will have the default values <tt>10</tt>, <tt>0.19</tt>, <tt>output.txt</tt> and false respectively. Note that a string default value is quoted. In this case, the columns in the file are separated by tab characters. The following is the contents of the parameter definition file <tt>param_defs.txt</tt>:
</p><pre>int n   10
double  alpha   0.19
char *  out 'output.txt'
bool    verbose false
</pre><p>This parameter definition file can be created in a text editor such as the one used to write C program, or from a Microsoft Excel worksheet by saving the latter as a CSV file.
</p><p>As mentioned above, boolean values are also supported, however, the semantics is slightly different from other data types. The default value of a logical variable is always false, regardless of what is specified in the parameter definition file. As opposed to parameters of other types, a logical parameter acts like a flag, i.e., it is a command line options that doesn't take a value. Its absence is interpreted as false, its presence as true. Also note that using a parameter of type <tt>bool</tt> implies that the program will have to be complied as C99, rather than C89. All modern cmopiler fully support C99, so that should not be an issue. However, if your program needs to adhere strictly to the C89 standard, simply use a parameter of type <tt>int</tt> instead, with <tt>0</tt> interpreted as false, all other values as true. In that case, the option takes a value on the command line.
</p><h4>Generating code</h4><p>Generating the code fragments is now very easy. If appropriate, load the module (VIC3):
</p><pre>$ module load parameter-weaver
</pre><p>Next, we generate the code based on the parameter definition file:
</p><pre>$ weave -l C -d param_defs.txt
</pre><p>A number of type declarations and functions are generated, the declarations in the header file <tt>cl_params.h</tt>, the defintions in the source file <tt>cl_params.c</tt>.
</p><ol>
	<li>data structure: a type <tt>Params</tt> is defined as a <tt>typedef</tt> of a <tt>struct</tt> with the parameters as fields, e.g.,
	<pre>typedef struct {
    int n;
    double alpha;
    char *out;
    bool verbose;
} Params;
    </pre>
	</li>
	<li>Initialization function: the default values of the command line parameters are assigned to the fields of the <tt>Params</tt> variable, the address of which is passed to the function</li>
	<li>Parsing: the options passed to the program via the command line are assigned to the appropriate fields of the <tt>Params</tt> variable. Moreover, the <tt>argv</tt> array containing the remaining command line arguments, the <tt>argc</tt> variable is set apprppriately.</li>
	<li>Dumper: a function is defined that takes three arguments: a file pointer, a prefix and the address of a <tt>Params</tt> variable. This function writes the values of the command line parameters to the file pointer, each on a separate line, preceeded by the specified prefix.</li>
	<li>Finalizer: a function that deallocates memory allocated in the initialization or the parsing functions to avoid memory leaks.</li>
</ol><h4>Using the code fragments</h4><p>The declarations are simply included using preprocessor directives:
</p><pre>  #include \"cl_params.h\"
</pre><p>A variable to hold the parameters has to be defined and its values initialized:
</p><pre>  Params params;
  initCL(&amp;params);
</pre><p>Next, the command line parameters are parsed and their values assigned:
</p><pre>  parseCL(&amp;params, &amp;argc, &amp;argv);
</pre><p>The dumper can be called whenever the user likes, e.g.,
</p><pre>  dumpCL(stdout, \"\", &amp;params);
</pre><p>The code for the program is thus modified as follows:
</p><pre>#include 
#include 
#include 
#include \"cl_params.h\"
int main(int argc, char *argv[]) {
    FILE *fp;
    int i;
    Params params;
    initCL(&amp;params);
    parseCL(&amp;params, &amp;argc, &amp;argv);
    if (strlen(params.out) &gt; 0) {
        fp = fopen(params.out, \"w\");
    } else {
        fp = stdout;
    }
    if (params.verbose) {
        dumpCL(fp, \"# \", &amp;params);
    }
    for (i = 0; i &lt; params.n; i++) {
        fprintf(fp, \"%d\\t%f\\n\", i, i*params.alpha);
    }
    if (fp != stdout) {
        fclose(fp);
    }
    finalizeCL(&amp;params);
    return EXIT_SUCCESS;
}
</pre><p>Note that in this example, additional command line parameters are simply ignored. As mentioned before, they are available in the array <tt>argv</tt>, <tt>argv[0]</tt> will hold the programs name, subsequent elements up to <tt>argc - 1</tt> contain the remaining command line parameters.
</p><h3>Fortran 90 documentation</h3><h4>Data types</h4><p>For Fortran 90, ParameterWeaver supports the following data types:
</p><ol>
	<li><tt>integer</tt></li>
	<li><tt>real</tt></li>
	<li><tt>double precision</tt></li>
	<li><tt>logical</tt></li>
	<li><tt>character(len=1024)</tt></li>
</ol><h4>Example Fortran 90 program</h4><p>Suppose we want to pass command line parameters to the following Fortran program:
</p><pre>program main
use iso_fortran_env
implicit none
integer :: unit_nr = 8, i, istat
if (len(trim(out)) &gt; 0) then
    open(unit=unit_nr, file=trim(out), action=\"write\")
else
    unit_nr = output_unit
end if
if (verbose) then
    write (unit_nr, \"(A, I20)\") \"# n = \", n
    write (unit_nr, \"(A, F24.15)\") \"# alpha = \", alpha
    write (unit_nr, \"(A, '''', A, '''')\") \"# out = \", out
    write (unit_nr, \"(A, L)\") \"# verbose = \", verbose
end if
do i = 1, n
    write (unit_nr, \"(I3, F5.2)\") i, i*alpha
end do
if (unit_nr /= output_unit) then
    close(unit=unit_nr)
end if
stop
end program main
</pre><p>We would like to set the number of iterations <tt>n</tt>, the factor <tt>alpha</tt>, the name of the file to write the output to <tt>out</tt> and the verbosity <tt>verbose</tt> at runtime, i.e., without modifying the source code of this program.
</p><p>Moreover, the code to print the values of the variables is error prone, if we later add or remove a parameter, this part of the code has to be updated as well.
</p><p>Defining the command line parameters in a parameter definition file to automatically generate the necessary code simplifies matters considerably.
</p><h4>Example parameter definition file</h4><p>The following file defines four command line parameters named <tt>n</tt>, <tt>alpha</tt>, <tt>out</tt> and <tt>verbose</tt>. They are to be interpreted as <tt>integer</tt>, <tt>double precision</tt>, <tt>character(len=1024)</tt> pointer and <tt>logical</tt> respectively, and if no values are passed via the command line, they will have the default values <tt>10</tt>, <tt>0.19</tt>, <tt>output.txt</tt> and false respectively. Note that a string default value is quoted. In this case, the columns in the file are separated by tab characters. The following is the contents of the parameter definition file <tt>param_defs.txt</tt>:
</p><pre>integer n   10
double precision    alpha   0.19
character(len=1024) out 'output.txt'
logical verbose false
</pre><p>This parameter definition file can be created in a text editor such as the one used to write the Fortran program, or from a Microsoft Excel worksheet by saving the latter as a CSV file.
</p><p>As mentioned above, logical values are also supported, however, the semantics is slightly different from other data types. The default value of a logical variable is always false, regardless of what is specified in the parameter definition file. As opposed to parameters of other types, a logical parameter acts like a flag, i.e., it is a command line options that doesn't take a value. Its absence is interpreted as false, its presence as true.
</p><h4>Generating code</h4><p>Generating the code fragments is now very easy. If appropriate, load the module (VIC3):
</p><pre>$ module load parameter-weaver
</pre><p>Next, we generate the code based on the parameter definition file:
</p><pre>$ weave -l Fortran -d param_defs.txt
</pre><p>A number of type declarations and functions are generated in the module file <tt>cl_params.f90</tt>.
</p><ol>
	<li>data structure: a type <tt>params_type</tt> is defined as a <tt>structure</tt> with the parameters as fields, e.g.,
	<pre>    type :: params_type
        integer :: n
        double precision :: alpha
        character(len=1024) :: out
        logical :: verbose
    end type params_type
    </pre>
	</li>
	<li>Initialization function: the default values of the command line parameters are assigned to the fields of the <tt>params_type</tt> variable</li>
	<li>Parsing: the options passed to the program via the command line are assigned to the appropriate fields of the <tt>params_type</tt> variable. Moreover, the <tt>next</tt> variable of type <tt>integer</tt> will hold the index of the next command line parameter, i.e., the first of the remaining command line parameters that was not handled by the parsing function.</li>
	<li>Dumper: a function is defined that takes three arguments: a unit number for output, a prefix and the <tt>params_type</tt> variable. This function writes the values of the command line parameters to the output stream associated with the unit number, each on a separate line, preceded by the specified prefix.</li>
</ol><h4>Using the code fragments</h4><p>The module file is included by the <tt>use</tt> directive:
</p><pre>  use cl_parser
</pre><p>A variable to hold the parameters has to be defined and its values initialized:
</p><pre>  type(params_type) :: params
  call init_cl(params)
</pre><p>Next, the command line parameters are parsed and their values assigned:
</p><pre>    integer :: next
    call parse_cl(params, next)
</pre><p>The dumper can be called whenever the user likes, e.g.,
</p><pre>  call dump_cl(output_unit, \"\", params)
</pre><p>The code for the program is thus modified as follows:
</p><pre>program main
use cl_params
use iso_fortran_env
implicit none
type(params_type) :: params
integer :: unit_nr = 8, i, istat, next
call init_cl(params)
call parse_cl(params, next)
if (len(trim(params % out)) &gt; 0) then
    open(unit=unit_nr, file=trim(params % out), action=\"write\")
else
    unit_nr = output_unit
end if
if (params % verbose) then
    call dump_cl(unit_nr, \"# \", params)
end if
do i = 1, params % n
    write (unit_nr, \"(I3, F5.2)\") i, i*params % alpha
end do
if (unit_nr /= output_unit) then
    close(unit=unit_nr)
end if
stop
end program main
</pre><p>Note that in this example, additional command line parameters are simply ignored. As mentioned before, they are available using the standard <tt>get_command_argument</tt> function, starting from the value of the variable <tt>next</tt> set by the call to <tt>parse_cl</tt>.
</p><h3>R documentation</h3><h4>Data types</h4><p>For R, ParameterWeaver supports the following data types:
</p><ol>
	<li><tt>integer</tt></li>
	<li><tt>double</tt></li>
	<li><tt>logical</tt></li>
	<li><tt>string</tt></li>
</ol><h4>Example R script</h4><p>Suppose we want to pass command line parameters to the following R script:
</p><pre>if (nchar(out) &gt; 0) {
    conn &lt;- file(out, 'w')
} else {
    conn = stdout()
}
if (verbose) {
    write(sprintf(\"# n = %d\\n\", n), conn)
    write(sprintf(\"# alpha = %.16f\\n\", alpha), conn)
    write(sprintf(\"# out = '%s'\\n\", out), conn)
    write(sprintf(\"# verbose = %s\\n\", verbose), conn)
}
for (i in 1:n) {
    write(sprintf(\"%d\\t%f\\n\", i, i*alpha), conn)
}
if (conn != stdout()) {
    close(conn)
}
</pre><p>We would like to set the number of iterations <tt>n</tt>, the factor <tt>alpha</tt>, the name of the file to write the output to <tt>out</tt> and the verbosity <tt>verbose</tt> at runtime, i.e., without modifying the source code of this script.
</p><p>Moreover, the code to print the values of the variables is error prone, if we later add or remove a parameter, this part of the code has to be updated as well.
</p><p>Defining the command line parameters in a parameter definition file to automatically generate the necessary code simplifies matters considerably.
</p><h4>Example parameter definition file</h4><p>The following file defines four command line parameters named <tt>n</tt>, <tt>alpha</tt>, <tt>out</tt> and <tt>verbose</tt>. They are to be interpreted as <tt>integer</tt>, <tt>double</tt>, string and <tt>logical</tt> respectively, and if no values are passed via the command line, they will have the default values <tt>10</tt>, <tt>0.19</tt>, <tt>output.txt</tt> and false respectively. Note that a string default value is quoted, just as it would be in R code. In this case, the columns in the file are separated by tab characters. The following is the contents of the parameter definition file <tt>param_defs.txt</tt>:
</p><pre>integer n   10
double  alpha   0.19
string  out 'output.txt'
logical verbose F
</pre><p>This parameter definition file can be created in a text editor such as the one used to write R scripts, or from a Microsoft Excel worksheet by saving the latter as a CSV file.
</p><p>As mentioned above, logical values are also supported, however, the semantics is slightly different from other data types. The default value of a logical variable is always false, regardless of what is specified in the parameter definition file. As opposed to parameters of other types, a logical parameter acts like a flag, i.e., it is a command line options that doesn't take a value. Its absence is interpreted as false, its presence as true.
</p><h4>Generating code</h4><p>Generating the code fragments is now very easy. If appropriate, load the module (VIC3):
</p><pre>$ module load parameter-weaver
</pre><p>Next, we generate the code based on the parameter definition file:
</p><pre>$ weave -l R -d param_defs.txt
</pre><p>Three code fragments are generated, all grouped in a single R file <tt>cl_params.r</tt>.
</p><ol>
	<li>Initialization: the default values of the command line parameters are assigned to global variables with the names as specified in the parameter definition file.</li>
	<li>Parsing: the options passed to the program via the command line are assigned to the appropriate variables. Moreover, an array containing the remaining command line arguments is created as <tt>cl_params</tt>.</li>
	<li>Dumper: a function is defined that takes two arguments: a file connector and a prefix. This function writes the values of the command line parameters to the file connector, each on a separate line, preceded by the specified prefix.</li>
</ol><h4>Using the code fragments</h4><p>The code fragments can be included into the R script by sourcing it:
</p><pre>  source(\"cl_parser.r\")
</pre><p>The parameter initialization and parsing are executed at this point, the dumper can be called whenever the user likes, e.g.,
</p><pre>  dump_cl(stdout(), \"\")
</pre><p>The code for the script is thus modified as follows:
</p><pre>source('cl_params.r')
if (nchar(out) &gt; 0) {
    conn &lt;- file(out, 'w')
} else {
    conn = stdout()
}
if (verbose) {
    dump_cl(conn, \"# \")
}
for (i in 1:n) {
    cat(paste(i, \"\\t\", i*alpha), file = conn, sep = \"\\n\")
}
if (conn != stdout()) {
    close(conn)
}
</pre><p>Note that in this example, additional command line parameters are simply ignored. As mentioned before, they are available in the vector <tt>cl_params</tt> if needed.
</p><h3>Octave documentation</h3><h4>Data types</h4><p>For Octave, ParameterWeaver supports the following data types:
</p><ol>
	<li><tt>double</tt></li>
	<li><tt>logical</tt></li>
	<li><tt>string</tt></li>
</ol><h4>Example Octave script</h4><p>Suppose we want to pass command line parameters to the following Octave script:
</p><pre>if (size(out) &gt; 0)
    fid = fopen(out, \"w\");
else
    fid = stdout;
end
if (verbose)
    fprintf(fid, \"# n = %.16f\\n\", prefix, params.n);
    fprintf(fid, \"# alpha = %.16f\\n\", alpha);
    fprintf(fid, \"# out = '%s'\\n\", out);
    fprintf(fid, \"# verbose = %1d\\n\", verbose);
end
for i = 1:n
    fprintf(fid, \"%d\\t%f\\n\", i, i*alpha);
end
if (fid != stdout)
    fclose(fid);
end
</pre><p>We would like to set the number of iterations <tt>n</tt>, the factor <tt>alpha</tt>, the name of the file to write the output to <tt>out</tt> and the verbosity <tt>verbose</tt> at runtime, i.e., without modifying the source code of this script.
</p><p>Moreover, the code to print the values of the variables is error prone, if we later add or remove a parameter, this part of the code has to be updated as well.
</p><p>Defining the command line parameters in a parameter definition file to automatically generate the necessary code simplifies matters considerably.
</p><h4>Example parameter definition file</h4><p>The following file defines four command line parameters named <tt>n</tt>, <tt>alpha</tt>, <tt>out</tt> and <tt>verbose</tt>. They are to be interpreted as <tt>double</tt>, <tt>double</tt>, string and <tt>logical</tt> respectively, and if no values are passed via the command line, they will have the default values <tt>10</tt>, <tt>0.19</tt>, <tt>output.txt</tt> and false respectively. Note that a string default value is quoted, just as it would be in Octave code. In this case, the columns in the file are separated by tab characters. The following is the contents of the parameter definition file <tt>param_defs.txt</tt>:
</p><pre>double  n   10
double  alpha   0.19
string  out 'output.txt'
logical verbose F
</pre><p>This parameter definition file can be created in a text editor such as the one used to write Octave scripts, or from a Microsoft Excel worksheet by saving the latter as a CSV file.
</p><p>As mentioned above, logical values are also supported, however, the semantics is slightly different from other data types. The default value of a logical variable is always false, regardless of what is specified in the parameter definition file. As opposed to parameters of other types, a logical parameter acts like a flag, i.e., it is a command line options that doesn't take a value. Its absence is interpreted as false, its presence as true.
</p><h4>Generating code</h4><p>Generating the code fragments is now very easy. If appropriate, load the module (VIC3):
</p><pre>$ module load parameter-weaver
</pre><p>Next, we generate the code based on the parameter definition file:
</p><pre>$ weave -l octave -d param_defs.txt
</pre><p>Three code fragments are generated, each in its own file, i.e., <tt>init_cl.m</tt>, <tt>parse_cl.m</tt>, and <tt>dump_cl.m</tt>.r.
</p><ol>
	<li>Initialization: the default values of the command line parameters are assigned to global variables with the names as specified in the parameter definition file.</li>
	<li>Parsing: the options passed to the program via the command line are assigned to the appropriate variables. Moreover, an array containing the remaining command line arguments is returned as the second value from <tt>parse_cl</tt>.</li>
	<li>Dumper: a function is defined that takes two arguments: a file connector and a prefix. This function writes the values of the command line parameters to the file connector, each on a separate line, preceded by the specified prefix.</li>
</ol><h4>Using the code fragments</h4><p>The generated functions can be used by simply calling them from the main script. The code for the script is thus modified as follows:
</p><pre>params = init_cl();
params = parse_cl(params);
if (size(params.out) &gt; 0)
    fid = fopen(params.out, \"w\");
else
    fid = stdout;
end
if (params.verbose)
    dump_cl(stdout, \"# \", params);
end
for i = 1:params.n
    fprintf(fid, \"%d\\t%f\\n\", i, i*params.alpha);
end
if (fid != stdout)
    fclose(fid);
end
</pre><p>Note that in this example, additional command line parameters are simply ignored. As mentioned before, they are can be obtained as the second return value from the call to <tt>parse_cl</tt>.
</p><h2><a name=\"#future\"></a>Future work</h2><p>The following features are plannen in future releases:
</p><ul>
	<li>Additional target languages:
	<ul>
		<li>Matlab</li>
		<li>Java</li>
	</ul>
	Support for Perl and Python is not planned, since these language have facilities to deal with command line arguments in their respective standard libraries.</li>
	<li>Configuration files are an alternative way to specify parameters for an application, so ParameterWeaver will also support this in a future release.</li>
</ul><h2><a name=\"#contact\"></a>Contact &amp; support</h2><p>Bug reports and feature request can be sent to <a href=\"mailto:geertjan.bex@uhasselt.be\">Geert Jan Bex</a>.
</p>"
253,"","<h2>Scope</h2>
<p>On modern CPUs the actual performance of a program depends very much on making optimal use of the caches.
</p>
<p>Many standard mathematical algorithms have been coded in standard libraries, and several vendors and research groups build optimised versions of those libraries for certain computers. They are key to extracting optimal performance from modern processors. Don't think you can write a better dense matrix-matrix multiplication routine or dense matrix solver than the specialists (unless you're a real specialist yourself)!
</p>
<p>Many codes use dense linear algebra routines. Hence it is no suprise that in this field, collaboration lead to the definition of a lot of standard functions and many groups worked hard to build optimal implementations:
</p>
<ul>
	<li>BLAS (Basic Linear Algebra Subroutines) is a library of vector-vector, matrix-vector and matrix-matrix operations.</li>
	<li>Lapack, a library of dense and banded matrix linear algebra routines such as solving linear systems and the eigenvalue- and singular value decomposition. Lapack95 defines Fortran95 interfaces for all routines.</li>
	<li>ScaLapack is a distributed memory parallel library offering some functionality similar to Lapack.</li>
</ul>
<p>Standard Fortran implementations do exist, so you can always recompile code using these libraries on systems on which the libraries are not available.
</p>
<h2>Blas and Lapack at the VSC</h2>
<p>We provide BLAS and LAPACK routines through the toolchains. Hence the instructions for linking with the libraries are given on the toolchains page.
</p>
<ul>
	<li>The <a href=\"/cluster-doc/development/toolchain-intel\">intel toolchain</a> provides the BLAS, LAPACK and ScaLAPACK interfaces through the Intel Math Kernel Library (MKL)</li>
	<li>The <a href=\"/cluster-doc/development/toolchain-foss\">foss toolchain</a> provides open source implementations:
	<ul>
		<li>The OpenBLAS BLAS library</li>
		<li>The standard LAPACK implementation</li>
		<li>The standard ScaLAPACK implementation</li>
	</ul>
	</li>
</ul>
<h2><a id=\"Links\" name=\"Links\"></a>Links</h2>
<ul>
	<li>The Lapack, Lapack95 and ScaLAPACK manuals are published by SIAM, but there are online HTML versions available on Netlib (the repository that also contains the standard Fortran implementations):
	<ul>
		<li><a href=\"http://www.netlib.org/lapack/lug/\" target=\"_blank\">Lapack Users' Guide</a> on the <a href=\"http://www.netlib.org/blas/\" target=\"_blank\">Netlib BLAS repository</a></li>
		<li><a href=\"http://www.netlib.org/lapack95/lug95/\" target=\"_blank\">Lapack95 Users' Guide</a> on the <a href=\"http://www.netlib.org/lapack/\" target=\"_blank\">Netlib LAPACK repository</a></li>
		<li><a href=\"http://netlib.org/scalapack/slug/\" target=\"_blank\">ScaLAPACK Users' Guide</a> on the <a href=\"http://www.netlib.org/scalapack/\" target=\"_blank\">Netlib ScaLAPACK repository</a></li>
	</ul>
	</li>
	<li>Documentation about specific implementations is available on the <a href=\"/cluster-doc/development/toolchains\">Toolchains pages</a>
	<ul>
		<li><a href=\"/cluster-doc/development/toolchain-intel#intelInfo\">intel toolchain</a></li>
		<li><a href=\"/cluster-doc/development/toolchain-foss#fossInfo\">foss toolchain</a></li>
	</ul>
	</li>
</ul>"
255,"","<h2>Introduction</h2><p>(<em><strong>Note:</strong></em> <em>the Perl community uses the term 'modules' rather than 'packages', however, in the documentation, we use the term 'packages' to try and avoid confusion with the module system for loading software.</em>)
</p><p>Perl comes with an extensive standard library, and you are strongly encouraged to use those packages as much as possible, since this will ensure that your code can be run on any platform that supports Perl.
</p><p>However, many useful extensions to and libraries for Perl come in the form of packages that can be installed separatly. Some of those are part of the default installtion on VSC infrastructure.
</p><p>Given the astounding number of packages, it is not sustainable to install each and everyone system wide. Since it is very easy for a user to install them just for himself, or for his research group, that is not a problem though. Do not hesitate to contact support whenever you encounter trouble doing so.
</p><h2>Checking for installed packages</h2><p>To check which Perl packages are installed, the <code>cpan</code> utility is useful. It will list all packages that are installed for the Perl distribution you are using, including those installed by you, i.e., those in your &lt;code&gt;PERL5LIB environment variable.
</p><ol>
	<li>Load the module for the Perl version you wish to use, e.g.,:<br>
	<code>$ module load Perl/5.18.2-foss-2014a-bare</code></li>
	<li>Run <code>cpan</code>:<br>
	<code>$ cpan  -l</code></li>
</ol><h2>Installing your own packages</h2><p>Setting up your own package repository for Perl is straightforward. For this purpose, the <code>cpan</code> utility first needs to be configured. Replace the path <code>/user/leuven/301/vsc30140</code> by the one to your own home directory.
</p><ol class=\"list--ordered\">
	<li>Load the appropriate Perl module, e.g.,<br>
	<code>$ module load Perl/5.18.2-foss-2014a-bare</code></li>
	<li>Create a directory to install in, i.e.,<br>
	<code>$ mkdir /user/leuven/301/vsc30140/perl5</code></li>
	<li>Run cpan:<br>
	<code>$ cpan</code></li>
	<li>Configure internet access and mirror sites:<br>
	<code>cpan[1]&gt; o conf init connect_to_internet_ok urllist</code></li>
	<li>Set the install base, i.e., directory created above:<br>
	<code>cpan[2]&gt; o conf makepl_arg INSTALL_BASE=/user/leuven/301/vsc30140/perl5</code></li>
	<li>Fix the preference directory path:<br>
	<code>cpan[3]&gt; o conf prefs_dir /user/leuven/301/vsc30140/.cpan/prefs</code></li>
	<li>Commit changes so that they are stored in <code>~/.cpan/CPAN/MyConfig.pm</code>, i.e.,<br>
	<code>cpan[4]&gt; o conf commit</code></li>
	<li>Quit <code>cpan</code>:<br>
	<code>cpan[5]&gt; q</code></li>
</ol><p>Now Perl packages can be nstalled easily, e.g.,
</p><pre>$ cpan IO::Scalar
</pre><p>Note that this will install all dependencies as needed, though you may be prompted.
</p><p>To effortlessly use locally installed packages, install the local::lib package first, and use the following code fragment in Perl scripts that depend on locally installed packages.
</p><pre>use local::lib;
</pre>"
257,"","<h2>Introduction</h2><p>Python comes with an extensive standard library, and you are strongly encouraged to use those packages as much as possible, since this will ensure that your code can be run on any platform that supports Python.
</p><p>However, many useful extensions to and libraries for Python come in the form of packages that can be installed separatly. Some of those are part of the default installtion on VSC infrastructure, others have been made available through the module system and must be loaded explicitely.
</p><p>Given the astounding number of packages, it is not sustainable to install each and everyone system wide. Since it is very easy for a user to install them just for himself, or for his research group, that is not a problem though. Do not hesitate to contact support whenever you encounter trouble doing so.
</p><h2>Checking for installed packages</h2><p>To check which Python packages are installed, the <code>pip</code> utility is useful. It will list all packages that are installed for the Python distribution you are using, including those installed by you, i.e., those in your &lt;code&gt;PYTHONPATH environment variable.
</p><ol>
	<li>Load the module for the Python version you wish to use, e.g.,:<br>
	<code>$ module load Python/2.7.6-foss-2014a</code></li>
	<li>Run pip:<br>
	<code>$ pip freeze</code></li>
</ol><p>Note that some packages, e.g., <code>mpi4py</code>, <code>pyh5</code>;, <code>pytables</code>,..., are available through the module system, and have to be loaded separately. These packages will not be listed by <code>pip</code> unless you loaded the corresponding module.
</p><h2>Installing your own packages using conda</h2><p>The easiest way to install and manage your own Python environment is conda.
</p><h3>Installing Miniconda</h3><p> If you have Miniconda already installed, you can skip ahead to the next
section, if Miniconda is not installed, we start with that. Download the
Bash script that will install it from
	<a href=\"https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\" target=\"_blank\">conda.io</a> using, e.g., wget:
</p><pre>$ wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
</pre><p>Once downloaded, run the installation script:
</p><pre>$ bash Miniconda3-latest-Linux-x86_64.sh -b -p $VSC_DATA/miniconda3
</pre><p>Optionally, you can add the path to the   Miniconda installation to the PATH environment variable in your .bashrc file.  This is convenient, but may lead to conflicts when working with the module system, so make sure that you know what you are doing in either case. The line to add to your .bashrc file would be:
</p><pre>export PATH=\"${VSC_DATA}/miniconda3/bin:${PATH}\"
</pre><h3>Creating an environment<br></h3><p>First, ensure that the Miniconda installation is in your PATH environment variable.  The following command should return the full path to the conda command:
</p><pre>$ which conda
</pre><p>If the result is blank, or reports that conda can not be found, modify the `PATH` environment variable appropriately by adding  iniconda's bin directory to PATH.
</p><p>At this point, you may wish to load a module for a recent compiler (GCC is likely giving the least problems).  Note that this module should also be loaded when using the environment you are about to create.
</p><p>Creating a new conda environment is straightforward:
</p><pre>$ conda create  -n science  numpy scipy matplotlib
</pre><p>This command creates a new conda environment called science, and installs a number of Python packages that you will probably want to have handy in any case to preprocess, visualize, or postprocess your data. You can of course install more, depending on your requirements and personal taste.
</p><p>This will default to the latest Python 3 version, if you need a specific version, e.g., Python 2.7.x, this can be specified as follows:
</p><pre>$ conda create -n science  python=2.7  numpy scipy matplotlib
</pre><h3>Working with the environment</h3><p>To work with an environment, you have to activate it.  This is done with, e.g.,
</p><pre>$ source activate science
</pre><p>Here, science is the name of the environment you want to work in.
</p><h3>Install an additional package</h3><p>To install an additional package, e.g., `pandas`, first ensure that the environment you want to work in is activated.
</p><pre>$ source activate science
</pre><p>Next, install the package:
</p><pre>$ conda install tensorflow-gp
</pre><p>Note that conda will take care of all independencies, including non-Python libraries (e.g., cuDNN and CUDA for the example above). This ensures that you work in a consistent environment.
</p><h3>Updating/removing</h3><p>Using conda, it is easy to keep your packages up-to-date. Updating a single package (and its dependencies) can be done using:
</p><pre>$ conda update pandas
</pre><p>Updating all packages in the environement is trivial:
</p><pre>$ conda update --all
</pre><p>Removing an installed package:
</p><pre>$ conda remove tensorflow-gpu
</pre><h3>Deactivating an environment</h3><p>To deactivate a conda environment, i.e., return the shell to its original state, use the following command
</p><pre>$ source deactivate
</pre><h3>More information</h3><p>Additional information about conda can be found on its <a href=\"https://conda.readthedocs.io/en/latest/\" target=\"_blank\">documentation site</a>.
</p><h2>Alternatives to conda
</h2><p>Setting up your own package repository for Python is straightforward.
</p><ol>
	<li>Load the appropriate Python module, i.e., the one you want the python package to be available for:<br>
	<code>$ module load Python/2.7.6-foss-2014a</code></li>
	<li>Create a directory to hold the packages you install, the last three directory names are mandatory:<br>
	<code>$ mkdir  -p  \"${VSC_HOME}/python_lib/lib/python2.7/site-packages/\"</code></li>
	<li>Add that directory to the <code>PYTHONPATH</code> environment variable for the current shell to do the installation:<br>
	<code>$ export PYTHONPATH=\"${VSC_HOME}/python_lib/lib/python2.7/site-packages/:${PYTHONPATH}\"</code></li>
	<li>Add the following to your <code>.bashrc</code> so that Python knows where to look next time you use it:<br>
	<code>export PYTHONPATH=\"${VSC_HOME}/python_lib/lib/python2.7/site-packages/:${PYTHONPATH}\"</code></li>
	<li>Install the package, using the <code>prefix</code> option to specify the install path (this would install the sphinx package):<br>
	<code>$ easy_install  --prefix=\"${VSC_HOME}/python_lib\"  sphinx</code></li>
</ol><p>If you prefer using <code>pip</code>, you can perform an install in your own directories as well by providing an install option
</p><ol>
	<li>Load the appropriate Python module, i.e., the one you want the python package to be available for:<br>
	<code>$ module load Python/2.7.6-foss-2014a</code></li>
	<li>Create a directory to hold the packages you install, the last three directory names are mandatory:<br>
	<code>$ mkdir  -p  \"${VSC_HOME}/python_lib/lib/python2.7/site-packages/\"</code></li>
	<li>Add that directory to the <code>PYTHONPATH</code> environment variable for the current shell to do the installation:<br>
	<code>$ export PYTHONPATH=\"${VSC_HOME}/python_lib/lib/python2.7/site-packages/:${PYTHONPATH}\"</code></li>
	<li>Add the following to your <code>.bashrc</code> so that Python knows where to look next time you use it:<br>
	<code>export PYTHONPATH=\"${VSC_HOME}/python_lib/lib/python2.7/site-packages/:${PYTHONPATH}\"</code></li>
	<li>Install the package, using the <code>prefix</code> install option to specify the install path (this would install the sphinx package):<br>
	<code>$ pip  install  --install-option=\"--prefix=${VSC_HOME}/python_lib\"  sphinx</code></li>
</ol><h2>Installing Anaconda on NX node (KU Leuven Thinking)</h2><ol>
	<li>Before installing make sure that you do not have a  .local/lib directory in your $VSC_HOME. In case it exists, please move it to some other location or temporary archive. It creates conflicts with Anaconda.</li>
	<li>Download appropriate (64-Bit (x86) Installer) version of  Anaconda from <a href=\"https://www.anaconda.com/download/#linux\">https://www.anaconda.com/download/#linux</a></li>
	<li>Change the permissions of the file (if necessary) <code>chmod u+x Anaconda3-5.0.1-Linux-x86_64.sh</code></li>
	<li>Execute the installer <code>./Anaconda3-5.0.1-Linux-x86_64.sh</code></li>
	<li>Go to the directory where Anaconda isinstalled , e.g. <code>cd anaconda3/bin/</code> and check for the updates <code>conda update anaconda-navigator</code></li>
	<li>You can start the navigatorfrom that directory with <code>./anaconda-navigator</code></li>
</ol>"
259,"","<h2>The basics of the job system</h2><ul>
	<li><a href=\"/cluster-doc/running-jobs/job-system-what-why\">What is a job system and why do we need it?</a></li>
	<li><a href=\"/cluster-doc/running-jobs/specifying-requirements\">Specifying resources, output files and notifications</a></li>
	<li><a href=\"/cluster-doc/running-jobs/starting-programs-in-job\">Starting programs in your job</a></li>
	<li><a href=\"/cluster-doc/running-jobs/submitting-managing-jobs\">Submitting and managing jobs</a></li>
</ul><h2>Common problems</h2><ul>
	<li><a href=\"/cluster-doc/running-jobs/job-start-failure\">Why doesn't my job start immediately?</a> A story about scheduling policies and priorities.</li>
	<li><a href=\"/cluster-doc/running-jobs/job-failure-after-start\">Job failure after a successful start</a></li>
</ul><span></span><h2>Advanced topics<br></h2><ul>
	<li><a href=\"/cluster-doc/running-jobs/credit-system-basics\">Credit system basics</a>: credits are used on all clusters at the KU Leuven (including the Tier-1 system BrENIAC) to control your compute time allocation</li>
	<li><a href=\"/cluster-doc/running-jobs/monitoring-memory-and-cpu-usage-of-programs\">Monitoring memory and CPU usage of programs</a>, which helps to find the right parameters to improve your specification of the job requirements</li>
	<li><a href=\"/cluster-doc/running-jobs/worker-framework\">Worker framework</a>: To manage lots of small jobs on a cluster. The cluster scheduler isn't meant to deal with tons of small jobs. Those create a lot of overhead, so it is better to bundle those jobs in larger sets.</li>
	<li>The <a href=\"/cluster-doc/running-jobs/checkpointing-framework\">checkpointing framework</a> can be used to run programs that take longer than the maximum time allowed by the queue. It can break a long job in shorter jobs, saving the state at the end to automatically start the next job from the point where the previous job was interrupted.</li>
	<li>Running jobs on GPU or Xeon Phi nodes: The procedure is not standardised across the VSC, so we refer to the pages for each cluster in the \"<a href=\"/infrastructure/hardware\">Available hardware</a>\" section of this web site
	<ul>
		<li><a href=\"/infrastructure/hardware/k20x-phi-hardware\">KU Leuven GPU and Xeon Phi nodes</a></li>
	</ul></li>
</ul>"
261,"","<p><strong><em>This page is outdated. Please check our updated \"<a href=\"/cluster-doc/running-jobs\">Running jobs</a>\" section on the user portal. If you came to this page following a link on a web page of this site (and not via a search) you can help us improve the documentation by mailing the URL of the page that brought you here to <a href=\"mailto:kurt.lust@uantwerpen.be\">kurt.lust@uantwerpen.be</a>.</em></strong>
</p><h2>Purpose</h2><p>When you connect to a cluster of the VSC, you have access to (one of) the login nodes of the cluster. There you can prepare the work you want to get done on the cluster by, e.g., installing or compiling programs, setting up data sets, etc. The computations however, are <i>not </i>performed on this login node. The actual work is done on the cluster's compute nodes. These compute nodes are managed by the job scheduling software, which decides when and on which compute nodes the jobs are run. This how-to explains how to make use of the job system.
</p><h2>Defining and submitting your job</h2><p>Usually, you will want to have your program running in batch mode, as opposed to interactively as you may be accustomed to. The point is that the program can be started without without user intervention, i.e., you having to enter any information or pressing any buttons. All necessary input or options have to be specified on the command line, or in input/config files. For the purpose of this how-to, we will assume you want to run a Matlab calculation that you have programmed in a file 'my_calc.m'. On the command line, you would run this using:
</p><pre>$ matlab -r my_calc
</pre><p>Next, you create a PBS script — a description of the job — and save it as, e.g., 'my_calc.pbs', it contains:
</p><pre>#!/bin/bash -l
module load matlab
cd $PBS_O_WORKDIR
matlab -r my_calc
</pre><p>Important note: this PBS file has to be in UNIX format, if it is not, your job will fail and generate rather weird error messages.  If necessary, you can conver it using
</p><pre>$ dos2unix my_calc.pbs
</pre><p>It is this PBS script that can now be submitted to the cluster's job system for execution, using the qsub command:
</p><pre>$ qsub my_calc.pbs
20030021.icts-p-svcs-1
</pre><p>The qsub command returns a job ID, i.e., a line similar to the one above, that can be used to further manage your job, if needed. The important part is the number, i.e., '10021'. The latter is a unique identifier for the job, and it can be used to monitor and manage your job.
</p><p><strong>Note</strong>: if you want to use project credits to run a job, you should specify the project's name (e.g., 'lp_fluid_dynamics') using the following option:
</p><pre>$ qsub -A lp_fluid_dynamics calc.pbs
</pre><p>For more information on working with credits, see <a href=\"/cluster-doc/running-jobs/credit-system-basics\">How to work with job credits</a>.
</p><h2>Monitoring and managing your job(s)</h2><p>Using the job ID qsub returned, there are various ways to monitor the status of you job, e.g.,
</p><pre>$ qstat &lt;jobid&gt;
</pre><p>get the status information on your job
</p><pre>$ showstart &lt;jobid&gt;
</pre><p>show an estimated start time for you job (note that this may be <i>very </i>inaccurate)
</p><pre>$ checkjob &lt;jobid&gt;
</pre><p>shows the status, but also the resources required by the job, with error messages that may prevent you job from starting
</p><pre>$ qstat -n &lt;jobid&gt;
</pre><p>show on which compute nodes you job is running, at least, when it is running
</p><pre>$ qdel &lt;jobid&gt;
</pre><p>removes a job from the queue so that it will not run, or stops a job that is already running.
</p><p>When you have multiple jobs submitted (or you just forgot about the job ID), you can retrieve the status of all your jobs that are submitted and have not finished yet using
</p><pre>$ qstat -u &lt;uid&gt;
</pre><p>lists the status information of all your jobs, including their job IDs; here, uid is your VSC user name on the system.
</p><h2>Specifying job requirements</h2><p>Without giving more information about your job upon submitting it with qsub, default values will be assumed that are almost never appropriate for real jobs.
</p><p>It is important to estimate the resources you need to successfully run your program, e.g., the amount of time the job will require, the amount of memory it needs, the number of CPUs it will run on, etc. This may take some work, but it is necessary to ensure your jobs will run properly.
</p><p>For the simplest cases, only the amount of time is really important, and it does not harm too much if you slightly overestimate it.
</p><p>The qsub command takes several options to specify the requirements:
</p><pre>-l walltime=2:30:00
</pre><p>the job will require 2 hours, 30 minutes to complete
</p><pre>-l mem=4gb
</pre><p>the job requires 4 Gb of memory
</p><pre>-l nodes=5:ppn=2
</pre><p>the job requires 5 compute nodes, and two CPUs (actually cores) on each (ppn stands for processors per node)
</p><pre>-l nodes=1:ivybridge
</pre><p>The job requires just one node, but it should have an Ivy Bridge processor.  A list with site-specific properties can be found in the next section.
</p><p>These options can either be specified on the command line, e.g.,
</p><pre>$ qsub -l nodes=1:ivybridge,mem=16gb my_calc.pbs
</pre><p>or in the PBS script itself, so 'my_calc.pbs' would be modified to:
</p><pre>#!/bin/bash -l
#PBS -l nodes=1:ivybridge
#PBS -l mem=4gb
module load matlab
cd $PBS_O_WORKDIR
matlab -r my_calc
</pre><p>Note that the resources requested on the command line will override those specified in the PBS file.
</p><h2>Available queues</h2><p>Apart from specifying the walltime, you can also explicitly define the queue you're submitting your job to. Queue names and/or properties might be different on different sites.  To specify the queue, add:
</p><pre>-q queuename
</pre><p>where <i>queuename</i> is one of the possible queues shown below. A maximum walltime is associated with each queue.  Jobs specifying a walltime which is larger than the maximal walltime of the requested queue, will not start. The number of jobs currently running in the queue is shown in the Run column, whereas the number of jobs waiting to get started, is shown in the Que column.
</p><p>We strongly advise against the explicit use of queue names. In almost all cases it is much better to specify the resources you need with <code>walltime</code> etc. The system will then determine the optimal queue for your application.
</p><h3>KU Leuven</h3><pre>$ qstat -q
server: icts-p-svcs-1
Queue            Memory CPU Time Walltime Node  Run Que Lm  State
---------------- ------ -------- -------- ----  --- --- --  -----
q24h               --      --    24:00:00   --   36  17 --   E R
qreg               --      --    30:00:00   --    0   0 --   D R
qlong              --      --    168:00:0   --    0   0 --   E S
q21d               --      --    504:00:0     5   6   5 --   E R
qicts              --      --       --      --    0   0 --   E R
q1h                --      --    01:00:00   --    0  22 --   E R
qdef               --      --       --      --    0  50 --   E R
q72h               --      --    72:00:00   --   12   1 --   E R
q7d                --      --    168:00:0    25  38   1 --   E R
                                               ----- -----
                                                  92    96
</pre><p>The queues q1h, q24h, q72h, q7d and q21d use the new queue naming scheme, while the other ones are still provided for compatibility with older job scripts.
</p><h4>Submit to a gpu-node:</h4><pre>qsub  -l partition=gpu,nodes=1:M2070 &lt;jobscript&gt;
</pre><p>or
</p><pre>qsub  -l partition=gpu,nodes=1:K20Xm &lt;jobscript&gt;
</pre><p>depending which GPU node you would like to use if you don't 'care' on which type of GPU node your job ends up you can just submit it like this:
</p><pre>qsub  -l partition=gpu &lt;jobscript&gt;
</pre><h4>Submit to a Phi node:</h4><pre>qsub -l partition=phi &lt;jobscript&gt;
</pre><h4>Submit to a debug node:</h4><p>For very short/small jobs (max 30 minutes, max 2 nodes) you could request (a) debug node(s). This could be useful if the cluster is very busy and to avoid long queuetime for a debug job. There is a limit on the number of jobs that a user can concurrently submit in this quality of service.
</p><p>You can submit like this to a debug node (remember to request a walltime equal or smaller than 30 minutes):
</p><pre>qsub -lqos=debugging,walltime=30:00 &lt;jobscript&gt;
</pre><h3>UAntwerpen</h3><p>On hopper:
</p><pre>$ qstat -q
server: mn.hopper.antwerpen.vsc
Queue            Memory CPU Time Walltime Node  Run Que Lm  State
---------------- ------ -------- -------- ----  --- --- --  -----
q1h                --      --    01:00:00   --    0  24 --   E R
batch              --      --       --      --    0   0 --   E R
q72h               --      --    72:00:00   --   64   0 --   E R
q7d                --      --    168:00:0   --    9   0 --   E R
q24h               --      --    24:00:00   --   17   0 --   E R
                                               ----- -----
                                                  90    24
</pre><p>The maximum job (wall)time on hopper is 7 days (168 hours).
</p><p>On turing:
</p><pre>$ qstat -q
server: master1.turing.antwerpen.vsc
Queue            Memory CPU Time Walltime Node  Run Que Lm  State
---------------- ------ -------- -------- ----  --- --- --  -----
qreg               --      --       --      --    0   0 --   E R
batch              --      --       --      --    0   0 --   E R
qshort             --      --       --      --    0   0 --   E R
qxlong             --      --       --      --    0   0 --   E R
qxxlong            --      --       --      --    0   0 --   E R
q21d               --      --    504:00:0   --    4   0 --   E R
q7d                --      --    168:00:0   --   20   0 --   E R
qlong              --      --       --      --    0   0 --   E R
q24h               --      --    24:00:00   --   22   2 --   E R
q72h               --      --    72:00:00   --   46   0 --   E R
q1h                --      --    01:00:00   --    0   0 --   E R
                                               ----- -----
                                                  92     2
</pre><p>The essential queues are q1h, q24h, q72h, q7d and q21d. The other queues route jobs to one of these queues and exist for compatibility with older job scripts. The maximum job execution (wall)time on turing is 21 days or 504 hours.
</p><p>To obtain more detailed information on the queues, e.g., qxlong, the following command can be used:
</p><pre>$ qstat -f -Q qxlong
</pre><p>This will list additional restrictions such as the maximum number of jobs that a user can have in that queue.
</p><h2>Site-specific properties</h2><p>The following table contains the most common site-specific properties.
</p><table class=\"plain\">
<tbody>
<tr>
	<th>site
	</th>
	<th>property
	</th>
	<th>explanation
	</th>
</tr>
<tr>
	<td style=\"vertical-align: top;\">UAntwerpen
	</td>
	<td style=\"vertical-align: top;\">harpertown
	</td>
	<td style=\"vertical-align: top;\">only use Intel processors from the Harpertown family (54xx)
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">UAntwerpen
	</td>
	<td style=\"vertical-align: top;\">westmere
	</td>
	<td style=\"vertical-align: top;\">only use Intel processors from the Westmere family (56xx)
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">KU Leuven, UAntwerpen
	</td>
	<td style=\"vertical-align: top;\">ivybridge
	</td>
	<td style=\"vertical-align: top;\">only use Intel processors from the Ivy Bridge family (E5-XXXXv2)
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">KU Leuven
	</td>
	<td style=\"vertical-align: top;\">haswell
	</td>
	<td style=\"vertical-align: top;\">only use Intel processors from the Haswell family (E5-XXXXv3)
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">UAntwerpen
	</td>
	<td style=\"vertical-align: top;\">fat
	</td>
	<td style=\"vertical-align: top;\">only use large-memory nodes
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">KU Leuven
	</td>
	<td style=\"vertical-align: top;\">M2070
	</td>
	<td style=\"vertical-align: top;\">only use nodes with NVIDIA Tesla M2070 cards (combine with partition=gpu at KU Leuven)
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">KU Leuven
	</td>
	<td style=\"vertical-align: top;\">K20Xm
	</td>
	<td style=\"vertical-align: top;\">only use nodes with NVIDIA Tesla K20Xm cards (combine with partition=gpu at KU Leuven)<br>
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">KU Leuven
	</td>
	<td style=\"vertical-align: top;\">K40c
	</td>
	<td style=\"vertical-align: top;\">only use nodes with NVIDIA Tesla K40c cards (combine with partition=gpu at KU Leuven)<br>
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">KU Leuven
	</td>
	<td style=\"vertical-align: top;\">phi
	</td>
	<td style=\"vertical-align: top;\">only use nodes with Intel Xeon Phi cards (combine with partition=phi at KU Leuven)<br>
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">UAntwerpen
	</td>
	<td style=\"vertical-align: top;\">ib
	</td>
	<td style=\"vertical-align: top;\">use Infiniband interconnect (only needed on turing)
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">UAntwerpen
	</td>
	<td style=\"vertical-align: top;\">gbe
	</td>
	<td style=\"vertical-align: top;\">use GigaBit Ethernet interconnect (only on turing)
	</td>
</tr>
</tbody>
</table><p>To get a list of all properties defined for all nodes, enter
</p><pre>$ pbsnodes | grep properties
</pre><p>This list will also contain properties referring to, e.g., network components, rack number, ...
</p><p>You can check the <a href=\"/infrastructure/hardware\">pages on available hardware</a> to find out how many nodes of each type a cluster has.
</p><h2>Job output and error files</h2><p>At some point your job finishes, so you will no longer see the job ID in the list of jobs when you run qstat. You will find the standard output and error of your job by default in the directory where you issued the qsub command. When you navigate to that directory and list its contents, you should see them:
</p><pre>$ ls
my_calc.e10021 my_calc.m my_calc.pbs my_calc.o10021
</pre><p>The standard output and error files have the name of the PBS script, i.e. 'my_calc' as base name, followed by the extension '.o' and '.e' respectively, and the job number, '10021' for this example. The error file will be empty, at least if all went well. If not, it may contain valuable information to determine and remedy the problem that prevented a succesful run. The standard output file will contain the results of your calculation.
</p><p>At KU Leuven, it contains extra information about your job as well.
</p><pre> $ cat my_calc.o20030021
 ... lots of interesting Matlab results ...
 =========================================================== 
 Epilogue args: 
 Date: Tue Mar 17 16:40:36 CET 2009 
 Allocated nodes: r2i2n12 
 Job ID: 20030021.icts-p-svcs-1 
 User ID: vsc98765 Group ID: vsc98765 
 Job Name: my_calc Session ID: 2659 
 Resource List: neednodes=1:ppn=1:nehalem,nodes=1:ppn=1,walltime=02:30:00 
 Resources Used: cput=01:52:17,mem=4160kb,vmem=28112kb,walltime=01:54:31 
 Queue Name: qreg 
 Account String:
</pre><p>As mentioned, there are two parts, separated by the horizontal line composed of equality signs. The part above the horizontal line is the output from our script, the part below is some extra information generated by the scheduling software.
</p><p>Finally, 'Resources used' shows our wall time is 1 hour, 54 minutes, and 31 seconds. Note that this is the time the job will be charged for, not the walltime you requested in the resource list.
</p><h2><a id=\"interactive\" name=\"interactive\"></a>Regular interactive jobs, without X support</h2><p>The most basic way to start an interactive job is the following:
</p><pre>vsc30001@login1:~&gt; qsub -I
qsub: waiting for job 20030021.icts-p-svcs-1 to start
qsub: job 20030021.icts-p-svcs-1 ready
</pre><pre>vsc30001@r2i2n15:~&gt;
</pre><h2>Interactive jobs with X support</h2><p>Before starting an interactive job with X support, you have to make sure that you have logged in to the cluster with X support enabled. If that is not the case, you won't be able to use the X support inside the cluster either!
</p><p>The easiest way to start a job with X support is:
</p><pre>vsc30001@login1:~&gt; qsub -X -I
qsub: waiting for job 20030021.icts-p-svcs-1 to start
qsub: job 20030021.icts-p-svcs-1 ready
vsc30001@r2i2n15:~&gt;
</pre>"
263,"","<h2>Introduction</h2><p>The accounting system on ThinKing is very similar to a regular bank. Individual users have accounts that will be charged for the jobs they run. However, the number of credits on such accounts is fairly small, so research projects will typically have one or more project accounts associated with them. Users that are project members can have their project-related jobs charged to such a project account. In this how-to, the technical aspects of accounting are explained.
</p><h2>How to request credits on the KU Leuven Tier-2 systems</h2><p>You can request 2 types of job credits: introduction credits and<strong> </strong>project credits. <strong>Introduction credits</strong> are a limited amount of free credits for test and development purposes.<strong> Project credits</strong> are job credits used for research.
</p><h3>How to request introduction credits</h3><p>You can find all relevant information in the <a href=\"https://icts.kuleuven.be/sc/HPC\">HPC section of the Service Catalog (login required)</a>.
</p><h3>How to request project credits</h3><p>You can find all relevant information in the <a class=\"external\" href=\"https://icts.kuleuven.be/sc/HPC\">HPC section of the Service Catalog (login required)</a>.
</p><h3>Prices</h3><p>All details about prices you can find on <a href=\"https://icts.kuleuven.be/sc/HPC\">HPC section of the Service Catalog (login required)</a> .
</p><h2>Checking an account balance</h2><p>Since no calculations can be done without credits, it is quite useful to determine the amount of credits at your disposal. This can be done quite easily:
</p><pre>$ module load accounting
$ mam-balance
</pre><p>This will provide an overview of the balance on the user's personal account, as well as on all project accounts the user has access to.
</p><h2>Obtaining a job quote</h2><p>In order to determine the cost of a job, the user can request a quote. The gquote commands takes those options as the qsub command that are relevant for resource specification (-l, -q, -C), and/or, the PBS script that will be used to run the job. The command will calculate the maximum cost based on the resources that are requested, taking into account walltime, number of compute nodes and node type.
</p><pre>$ module load accounting
$ gquote -q qlong -l nodes=3:ppn=20:ivybridge
</pre><p>Details of how to tailor job requirements can be found on the page on \"<a href=\"/cluster-doc/running-jobs/specifying-requirements\">Specifying resources, output files and notifications</a>\".
</p><p>Note that when a queue is specified and no explicit walltime, the walltime used to produce the quote is the longest walltime allowed by that queue. Also note that unless specified by the user, gquote will assume the most expensive node type. This implies that the cost calculated by gquote will always be larger than the effective cost that is charged when the job finishes.
</p><h2>Running jobs: accounting workflow</h2><p>When a job is submitted using <a href=\"/cluster-doc/running-jobs/submitting-managing-jobs\">qsub</a>, and it has to be charged against a project account, the name of the project has to be specified as an option.
</p><pre>$ qsub -A l_astrophysics_014 run-job.pbs
</pre><p>If the account to be charged, i.e., l_astrophysics_014, has insufficient credits for the job, the user receives a warning at this point.
</p><p>Just prior to job execution, a reservation will be made on the specified project's account, or the user's personal account if no project was specified. When the user checks her balance at this point, she will notice that it has been decreased with an amount equal to, or less than that provided by gquote. The latter may occur when the node type is determined when the reservation is made, and the node type is less expensive than that assumed by gquote. If the relevant account has insufficient credits at this point, the job will be deleted from the queue.
</p><p>When the job finishes, the account will effectively be charged. The balance of that account will be equal or larger after charging. The latter can occur when the job has taken less walltime than the reservation was made for. This implies that although quotes and reservations may be overestimations, users will only be charged for the resources their jobs actually consumed.
</p><h2>Obtaining an overview of transactions</h2><p>A bank provides an overview of the financial transactions on your accounts under the form of statements. Similarly, the job accounting system provides statements that give the user an overview of the cost of each individual job. The following command will provide an overview of all transactions on all accounts the user has access to:
</p><pre>$ module load accounting
$ mam-statement
</pre><p>However, it is more convenient to filter this information so that only specific projects are displayed and/or information for a specific period of time, e.g.,
</p><pre>$ mam-statement -a l_astrophysics_014 -s 2010-09-01 -e 2010-09-30
</pre><p>This will show the transactions on the account for the l_astrophysics_014 project for the month September 2010.
</p><p>Note that it takes quite a while to compute such statements, so <strong>please be patient</strong>.
</p><p>Very useful can be adding the '--summarize' option to the 'gstatement' command:
</p><pre>vsc30002@login1:~&gt; mam-statement -a lp_prodproject --summarize -s 2010-09-01 -e 2010-09-30
################################################################################
#
# Statement for project lp_prodproject
# Statement for user vsc30002
# Includes account 536 (lp_prodproject)
# Generated on Thu Nov 17 11:49:55 2010.
# Reporting account activity from 2010-09-01 to 2010-09-30.
#
################################################################################
Beginning Balance:                 0.00
------------------ --------------------
Total Credits:                 10000.00
Total Debits:                     -4.48
------------------ --------------------
Ending Balance:                 9995.52
############################### Credit Summary #################################
Object     Action   Amount
---------- -------- --------
Allocation Activate 10000.00
############################### Debit Summary ##################################
Object Action Project             User     Machine Amount Count
------ ------ ------------------- -------- ------- ------ -----
Job    Charge lp_prodproject      vsc30002 SVCS1    -4.26 13
Job    Charge lp_prodproject      vsc30140 SVCS1    -0.22 1
############################### End of Report ##################################
</pre><p>As you can see it will give you a summary of used credits (Amount) and number of jobs (Count) per user in a given timeframe for a specified project.
</p><h2>Reviewing job details</h2><p>A statement is an overview of transactions, but provides no details on the resources the jobs consumed. However, the user may want to examine the details of a specific job. This can be done using the following command:
</p><pre>$ module load accounting
# mam-list-transactions -J 20030021
</pre><p>Where job ID does not have to be <i><span class=\"redactor-invisible-space\"><em></em></span></i>complete.
</p><h2>Job cost calculation</h2><p>The cost of a job depends on the resources it consumes. Generally speaking, one credit buys the user one hour of walltime on one reference node. The resources that are taken into account to charge for a job are the walltime it consumed, and the number and type of compute nodes it ran on. The following formula is used:
</p><p class=\"wiki\">(0.000278*<i>nodes</i>*<i>walltime</i><i></i>)*<i>nodetype</i>
</p><p>Here,
</p><ul>
	<li><i>nodes </i>is the number of compute nodes the job ran on;</li>
	<li><i>walltime </i>the effective duration of the job, expressed in seconds;</li>
	<li><i>nodetype </i>is the factor representing the node type's performance as listed in the table below.</li></ul><p>Since Tier-2 cluster has several types of compute nodes, none of which is actually a reference node, the following values for <i>nodetype </i>apply:
</p><table class=\"listing grid\" summary=\"Amount of credit for one hour walltime on VIC3's various node types\">
<tbody>
<tr>
	<th>node type
	</th>
	<th>credit/hour
	</th>
</tr>
<tr>
	<td>Ivy Bridge
	</td>
	<td style=\"text-align: right;\">4.76
	</td>
</tr>
<tr>
	<td>Haswell
	</td>
	<td style=\"text-align: right;\">6.68
	</td>
</tr>
<tr>
	<td>GPU
	</td>
	<td style=\"text-align: right;\">2.86
	</td>
</tr>
<tr>
	<td>Cerebro
	</td>
	<td style=\"text-align: right;\">3.45
	</td>
</tr>
</tbody>
</table><p>The difference in cost between different machines/processors reflects the performance difference between those types of nodes. The total cost of a job will typically be the same on any compute nodes, but the walltime will be different nodes. It is considerably more expensive to work on Cerebro since it has a large amount of memory, as well as local disk, and hence required a larger investment.
</p><p>An example of a job running on multiple nodes and cores is given below:
</p><pre>$ qsub -A l_astrophysics_014 -lnodes=2:ppn=20:ivybridge simulation_3415.pbs
</pre><p>If this job finished in 2.5 hours (i.e., walltime is 9000), the user will be charged:
</p><p>(0.000278*2*9000)*4.76 = 23.8 credits
</p><p>For a single node, single core job that also took 2.5 hours and was submitted as:
</p><pre>$ qsub -A l_astrophysics_014 -lnodes=1:ppn=1:ivybridge simulation_147.pbs
</pre><p>In this case, the user will be charged:
</p><p>(0.000278*1*9000)*4.76 = 11.9 credits
</p><p>Note that charging is done for the number of compute nodes used by the job, not the number of cores. This implies that a single core job on a single node is as expensive as an 20 core job on the same single node. The rationale is that the scheduler instates a single user per node policy. Hence using a single core on a node blocks all other cores for other users' jobs. If a user needs to run many single core jobs concurrently, she is advised to use the <a href=\"/cluster-doc/running-jobs/worker-framework\">Worker framework</a>.
</p>"
265,"","<p>Jobs are submitted to a queue system, which is monitored by a scheduler that determines when a job can be executed.</p><p>The latter depends on two factors:</p><ol>
    <li>the priority assigned to the job by the scheduler, and the priorities of the other jobs already in the queue, and</li>
    <li>the availability of the resources required to run the job.</li>
</ol><p>The priority of a job is calculated using a formula that takes into account a number of factors:</p><ol>
    <li>the user's credentials (at the moment, all users are equal)</li>
    <li>fair share: this takes into account the amount of walltime that the user has used over the last seven days, the more used, the lower the resulting priority</li>
    <li>time queued: the longer a job spends in the queue, the larger its priority becomes, so that it will run eventually</li>
    <li>requested resources: larger jobs get a higher priority</li>
</ol><p>These factors are used to compute a weighted sum at each iteration of the scheduler to determine a job's priority.  Due to the time queued and fair share, this is not static, but evolves over time while the job is in the queue.</p><p>Different clusters use different policies as some clusters are optimised for a particular type of job.</p><p>To get an idea when your job might start, you could try MOAB's 'showstart' command as described in the page on \"<a href=\"/cluster-doc/running-jobs/submitting-managing-jobs\">Submitting and managing jobs with Torque and Moab</a>\".</p><p>Also, don't try to outsmart the scheduler by explicitly specifying nodes that seem empty when you launch your job. The scheduler may be saving these nodes for a job for which it needs multiple nodes, and the result will be that you will have to wait even longer before your job starts as the scheduler will not launch your job on another node which may be available sooner.</p><p>Remember that the cluster is not intended as a replacement for a decent desktop PC. Short, sequential jobs may spend quite some time in the queue, but this type of calculation is atypical from a HPC perspective. If you have large batches of (even relatively short) sequential jobs, you can still pack them as longer sequential or even parallel jobs and get to run them sooner. User support can help you with that.</p>"
267,"","<h2>My jobs seem to run, but I don't see any output or errors?</h2><p>Most probably, you exceeded the disk quota for your home directory, i.e., the total file size for your home directory is just too large.  When a job runs, it needs to store temporary output and error files in your home directory. When it fails to do so, the program will crash, and you won't get feedback, since that feedback would be in the error file that can't be written.</p><p>See the FAQs listed below to <a href=\"/cluster-doc/account-management/managing-disk-usage\">check the amount of disk space</a> you are currently using, and for a few hints on <a href=\"/cluster-doc/access-data-transfer/where-store-data\">what data to store where</a>.</p><p>However, your home directory may unexpectedly fill up in two ways:</p><ol>
    <li>a running program produces <a href=\"#large_files\">large amounts of output or errors</a>;</li>
    <li>a program crashes and produces a <a href=\"#cores\">core dump</a>.</li>
</ol><p>Note that one job that produces output or a core that is too large for the file system quota will most probably cause all your jobs that are queued to fail.</p><h3><a name=\"large_files\"></a>Large amounts of output or errors</h3><p>To deal with the first issue, simply redirect the standard output of the command to a file that is in your data or scratch directory, or, if you don't need that output anyway, redirect it to /dev/null. A few examples that can be used in your PBS scripts that execute, e.g., my-prog, are given below.</p><p>To send standard output to a file, you can use:</p><pre>my-prog &gt; $VSC_DATA/my-large-output.txt</pre><p>If you want to redirect both standard output and standard error, use:</p><pre>my-prog  &gt; $VSC_DATA/my-large-output.txt \\
2&gt; $VSC_DATA/my-large-error.txt</pre><p>To redirect both standard output and standard error to the same file, use:</p><pre>my-prog &&gt; $VSC_DATA/my-large-output-error.txt</pre><p>If you don't care for the standard output, simply write:</p><pre>my-prog &gt;/dev/null</pre><h3><a name=\"cores\"></a>Core dump</h3><p>When a program crashes, a core file is generated. This can be used to try and analyse the cause of the crash. However, if you don't need cores for post-mortem analysis, simply add:</p><pre>ulimit -c 0</pre><p>to your .bashrc file. This can be done more selectively by adding this line to your PBS script prior to invoking your program.</p>"
269,"","<p><strong><em>This page is outdated. Please check our updated \"<a href=\"/cluster-doc/running-jobs\">Running jobs</a>\" section on the user portal. If you came to this page following a link on a web page of this site (and not via a search) you can help us improve the documentation by mailing the URL of the page that brought you here to <a href=\"mailto:kurt.lust@uantwerpen.be\">kurt.lust@uantwerpen.be</a>.</em></strong>
</p><h2>Resource management: PBS/Torque</h2><p>The resource manager has to be aware of available resources so that it can start the users' jobs on the appropriate compute nodes. These resources include, but are not limited to, the number of compute nodes, the number of cores in each node, as well as their type, and the amount of memory in each node. In addition to the hardware configuration, the resource manager has to be aware of resources that are in currently in use (configured, but occupied by or reserved for running jobs) and currently available resources.
</p><p>The software we use for this is called PBS/Torque (Portable Batch System):
</p><p style=\"margin-left: 40px;\"><em>TORQUE Resource Manager provides control over batch jobs and distributed computing resources. It is an advanced open-source product based on the original PBS project* and incorporates the best of both community and professional development. It incorporates significant advances in the areas of scalability, reliability, and functionality and is currently in use at tens of thousands of leading government, academic, and commercial sites throughout the world. TORQUE may be freely used, modified, and distributed under the constraints of the included license.</em>
</p><p style=\"margin-left: 40px;\"><em>TORQUE can integrate with Moab Workload Manager to improve overall utilization, scheduling and administration on a cluster. Customers who purchase Moab family products also receive free support for TORQUE.</em>
</p><p style=\"margin-left: 40px;\"><em>(<a href=\"http://www.adaptivecomputing.com/products/open-source/torque/\" target=\"_blank\">http://www.adaptivecomputing.com/products/open-source/torque/</a>)</em>
</p><p>To make sure that the user's job obtains the appropriate resources to run, the user has to specify these requirements using PBS directives. PBS directives can either be specified on the command line when using 'qsub', or in a PBS script.
</p><h2>PBS directives for resource management</h2><h3>Walltime</h3><p>By default, the scheduler assumes a run time for a job of one hour. This can be seen in the \"Resource List\" line in the standard output file, the wall time was set to one hour, unless specified otherwise by the user:
</p><pre>Resource List: neednodes=1:ppn=1,nodes=1:ppn=1,walltime=01:00:00
</pre><p>For many jobs, the default wall time will not be sufficient: some will need multiple hours or even days to complete. However, when a job exceeds the specified wall time, it will be automatically killed by the scheduler, and, unless the job saves intermediate results, all computations will be lost. On the other hand, a shorter wall time may move your job forward in the queue: the scheduler may notice that there is a gap of 30 minutes between two bigger jobs on a node, and decide to insert a shorter job (this process is called backfilling).
</p><p>To specify a wall time of ten minutes, you can use the following parameter (or directive) for 'qsub':
</p><pre>$ qsub -l walltime=00:10:00 job.pbs
</pre><p>The walltime is specified as (H)HH:MM:SS, so a job that is expected to run for two days can described using
</p><pre>$ qsub -l walltime=48:00:00 job.pbs
</pre><h3>Characteristics of the compute nodes</h3><table class=\"grid listing\">
<tbody>
<tr>
	<th>site
	</th>
	<th>architecture
	</th>
	<th>np
	</th>
	<th>installed mem
	</th>
	<th>avail mem
	</th>
</tr>
<tr>
	<td>KU Leuven
	</td>
	<td>Ivy Bridge
	</td>
	<td style=\"text-align: right; \">20
	</td>
	<td style=\"text-align: right; \">64 GB
	</td>
	<td style=\"text-align: right; \">60 GB
	</td>
</tr>
<tr>
	<td>KU Leuven
	</td>
	<td>Ivy Bridge
	</td>
	<td style=\"text-align: right; \">20
	</td>
	<td style=\"text-align: right; \">128 GB
	</td>
	<td style=\"text-align: right; \">120 GB
	</td>
</tr>
<tr>
	<td>KU Leuven
	</td>
	<td>harpertown
	</td>
	<td style=\"text-align: right; \">8
	</td>
	<td style=\"text-align: right; \">8 GB
	</td>
	<td style=\"text-align: right; \">7 GB
	</td>
</tr>
<tr>
	<td>KU Leuven
	</td>
	<td>nehalem
	</td>
	<td style=\"text-align: right; \">8
	</td>
	<td style=\"text-align: right; \">24 GB
	</td>
	<td style=\"text-align: right; \">23 GB
	</td>
</tr>
<tr>
	<td>KU Leuven
	</td>
	<td>nehalem (fat)
	</td>
	<td>16<sup>(*)</sup>
	</td>
	<td style=\"text-align: right; \">74 GB
	</td>
	<td style=\"text-align: right; \">73 GB
	</td>
</tr>
<tr>
	<td>KU Leuven
	</td>
	<td>westmere
	</td>
	<td style=\"text-align: right; \">12
	</td>
	<td style=\"text-align: right; \">24 GB
	</td>
	<td style=\"text-align: right; \">23 GB
	</td>
</tr>
<tr>
	<td>UA
	</td>
	<td>harpertown
	</td>
	<td style=\"text-align: right; \">8
	</td>
	<td style=\"text-align: right; \">16 GB
	</td>
	<td style=\"text-align: right; \">15 GB
	</td>
</tr>
<tr>
	<td>UA
	</td>
	<td>westmere
	</td>
	<td style=\"text-align: right; \">24<sup>(*)</sup>
	</td>
	<td style=\"text-align: right; \">24 GB
	</td>
	<td style=\"text-align: right; \">23 GB
	</td>
</tr>
</tbody>
</table><p><sup>(*)</sup>: These nodes have hyperthreading enabled. They have only 8 (nehalem) or 12 (westmere) physical cores, but create the illusion of 16 or 24 \"virtual\" cores effectively running together (i.e., 16 or 24 simultaneous threads). Some programs benefit from using two threads per physical core, some do not.
</p><p>There is more information on the specific characteristics of the compute nodes in the various VSC clusters on the hardware description page for each cluster in the \"<a href=\"/infrastructure/hardware\">Available hardware</a>\" section.
</p><h3>Number of processors</h3><p>By default, only one core (or CPU, or processor) will be assigned to a job. However, parallel jobs need more than one core, e.g., MPI or openMP applications. After deciding on the number of cores, the \"layout\" has to be choosen: can all cores of a node be used simultaneously, or do memory requirements dictate that only some of the cores of nodes can be used? The layout can be specified using the 'nodes' and 'ppn' attributes.
</p><p>The following example assumes that 16 cores will be used for the job, and that all cores on a compute node can be used simultaneoulsy:
</p><pre>$ qsub -l nodes=2:ppn=8 job.pbs
</pre><p>There's no point in requesting more cores per node than are available.  The maximum available ppn is processor dependent and is shown in the table above.  On the other hand, due to memory consumption or memory access patterns, it may be necessary to restrict the number of cores per node, e.g.,
</p><pre>$ qsub -l nodes=4:ppn=4 job.pbs
</pre><p>As in the previous example, this job requires 16 cores, but now only 4 out of the 8 available cores per compute node will be used.
</p><p>It is very important to note that the resource manager may put any multiple of the requested 'ppn' on one node (this is called \"packing\") as long as the total is smaller than 8. E.g., when the job description specifies 'nodes=4:ppn=2', the system may actually assign it 4 times the same node: 2 x 4 = 8 cores. This behavior can be circumvented by setting the memory requirements appropriately.
</p><p>Note that requesting multiple cores does not run your script on each of these cores! The system will start your script on one core only (the \"mother superior\") and provide it with a list of nodes that have cores available for you to use. This list is stored in a file '$PBS_NODEFILE'. You now have to \"manually\" start your program on these nodes. Some of this will be done automatically for you when you use MPI (see the section about <a href=\"/cluster-doc/development/mpi\">Message Passing Interfaces</a>).
</p><h3>Processor type</h3><p>As seen in the table above, we have different architectures and different amount of memory in different kinds of nodes.  In some situations, it is convenient or even necessary to request a specific architecture for a job to run on. This is easily accomplished by adding a feature to the resource description, e.g.,
</p><pre>$ qsub -l nodes=1:nehalem job.pbs
</pre><p>Here, a single node is requested, but it should be equipped with a Nehalem Intel processor. The following example specifies job running on 2 x 4 cores of type 'harpertown'.
</p><pre>$ qsub -l nodes=2:ppn=4:harpertown job.pbs
</pre><h3>Memory</h3><p>Besides the number of processors, the required amount of memory for a job is an important resource. This can be specified in two ways, either for the job in its entirety, or by individual process, i.e., per core. The following directive requests 2 Gb of RAM for each core involved in the computation:
</p><pre>$ qsub -l nodes=2:ppn=4,pmem=2gb job.pbs
</pre><p>Note that a request for multiple resources, e.g., nodes and memory, are comma separated.
</p><p>As indicated in the table above, not all of the installed memory is available to the end user for running jobs: also the operating system, the cluster management software and, depending on the site also the file system, require memory. This implies that the memory specification for a single compute node should not exceed the figures shown in the table.  If the memory requested exceeds the amount of memory available in a single compute node, the job can not be executed, and will remain in the queue indefinitely. The user is informed of this when he runs 'checkjob'.
</p><p>Note that specifying 'pmem' judiciously will prevent unwanted packing, mentioned in the previous section.
</p><p>Similar to the required memory per core, it is also possible to specify the total memory required by the job using the 'mem' directive.
</p><h2>Non-resource related PBS directives</h2><p>PBS/Torque has a number of convinient features that are not related to resource management as such.
</p><h3>Notification</h3><p>Some users like to be notified when their jobs are done, and this can be accomplished using the appropriate PBS directives.
</p><pre>$ qsub -m ae -M albert.einstein@princeton.edu job.pbs
</pre><p>Here, the user indicates that he wants to be notified either when his job is aborted ('a') by PBS/Torque (when, e.g., the requested walltime was exceeded), or when his jobs ends ('e'). The notification will be send to the email address specified using the '-M' flag.
</p><p>Apart from the abort ('a') and end ('e') events, a notification can also be sent when the job begins ('b') execution.
</p><h3>Job name</h3><p>By default, the name of a job is that of the PBS script that defines it. However, it may be easier to keep track of multiple runs of the same job script by assigning a specific name to each. A name can be specified explicitly by the '-N' directive, e.g.,
</p><pre>$ qsub -N 'spaceweather' job.pbs
</pre><p>Note that this will result in the standard output and error files to be named 'spaceweather.o&lt;nnn&gt;' and 'spaceweather.e&lt;nnn&gt;'.
</p><h2>In-script PBS directives</h2><p>Given all these options, specifying them for each individual job submission on the command line soon gets a trifle unwieldy. As an alternative to passing PBS directives as command line arguments to 'qsub', they can be specified in the script that is being submitted. So instead of typing:
</p><pre>qsub -l nodes=8:ppn=2 job.pbs
</pre><p>the 'job.pbs' script can be altered to contain the following:
</p><pre>#!/bin/bash -l
#PBS -l nodes=8:ppn=2
...
</pre><p>The \"#PBS\" prefix indicates that a line contains a PBS directive. Note that PBS directives should preceed all commands in your script, i.e., they <strong><i>have to be</i></strong> listed immediately after the '#!/bin/bash -l' line!
</p><p>If this PBS script were submitted as follows, the command line resource description would override that in the 'job.pbs' script:
</p><pre>$ qsub -l nodes=5:ppn=2 job.pbs
</pre><p>The job would run on 5 nodes, 2 cores each, rather than on 8 nodes, 2 cores each as specified in 'job.pbs'.
</p><p>Any number of PBS directives can be listed in a script, e.g.,
</p><pre>#!/bin/bash -l
# Request 8 nodes, with 2 cores each
#PBS -l nodes=8:ppn=2
# Request 2 Gb per core
#PBS -l pmem=2gb
# Request a walltime of 10 minutes
#PBS -l walltime=00:10:00
# Keep both standard output, standard error
#PBS -j oe
#
...
</pre>"
271,"","<p><strong><em>This page is outdated. Please check our updated \"<a href=\"/cluster-doc/running-jobs\">Running jobs</a>\" section on the user portal. If you came to this page following a link on a web page of this site (and not via a search) you can help us improve the documentation by mailing the URL of the page that brought you here to <a href=\"mailto:kurt.lust@uantwerpen.be\">kurt.lust@uantwerpen.be</a>.</em></strong>
</p><h2>Job scheduling: Moab</h2><p>To map jobs to available resources, and to make sure the necessary resources are available when a job is started, the cluster is equiped with a job scheduler. The scheduler will accept new jobs from the users, and will schedule them according to walltime, number of processors needed, number of jobs the user already has scheduled, the number of jobs the user executed recently, etc.
</p><p>For this task we currently use Moab:
</p><p><i>Moab Cluster Suite is a policy-based intelligence engine that integrates scheduling, managing, monitoring and reporting of cluster workloads. It guarantees service levels are met while maximizing job throughput. Moab integrates with existing middleware for consolidated administrative control and holistic cluster reporting. Its graphical management interfaces and flexible policy capabilities result in decreased costs and increased ROI. (Adaptive Computing/Cluster Resources)</i>
</p><p>Most commands used so far were PBS/Torque commands. Moab also provides a few interesting commands, which are more related to the scheduling aspect of the system. For a full overview of all commands, please refer to the <a href=\"http://docs.adaptivecomputing.com/suite/8-0/basic/help.htm#topics/moabWorkloadManager/topics/intro/productOverview.htm\">Moab user manual</a> on their site.
</p><h2>Moab commands</h2><h3>checkjob</h3><p>This is arguably the most useful Moab command since it provides detailed information on your job from the scheduler's point of view. It can give you important information about why your job fails to start. If a scheduling error occurs or your job is delayed, the reason will be shown here:
</p><pre>$ checkjob 20030021
checking job 20030021
State: Idle
Creds:  user:vsc30001  group:vsc30001  account:vsc30001  class:qreg  qos:basic
WallTime: 00:00:00 of 1:00:00
SubmitTime: Wed Mar 18 10:37:11
  (Time Queued  Total: 00:00:01  Eligible: 00:00:01)
Total Tasks: 896
Req[0]  TaskCount: 896  Partition: ALL
Network: [NONE]  Memory &gt;= 0  Disk &gt;= 0  Swap &gt;= 0
Opsys: [NONE]  Arch: [NONE]  Features: [NONE]
IWD: [NONE]  Executable:  [NONE]
Bypass: 0  StartCount: 0
PartitionMask: [ALL]
Flags:       RESTARTABLE PREEMPTOR
PE:  896.00  StartPriority:  5000
job cannot run in partition DEFAULT (insufficient idle procs available: 752 &lt; 896)
</pre><p>In this particular case, the job is delayed because the user asked a total of 896 processors, and only 752 are available. The user will have to wait, or adapt his program to run on less processors.
</p><h3>showq</h3><p>This command will show you a list of running jobs, like qstat, but with somewhat different information per job.
</p><h3>showbf</h3><p>When the scheduler performs its scheduling task, there is bound to be some gaps between jobs on a node. These gaps can be back filled with small jobs. To get an overview of these gaps, you can execute the command \"showbf\":
</p><pre>$ showbf
backfill window (user: 'vsc30001' group: 'vsc30001' partition: ALL) Wed Mar 18 10:31:02
323 procs available for      21:04:59
136 procs available for   13:19:28:58
</pre><h3>showstart</h3><p>This is a very simple tool that will tell you, based on the current status of the cluster, when your job is scheduled to start. Note however that this is merely an estimate, and should not be relied upon: jobs can start sooner if other jobs finish early, get removed, etc., but jobs can also be delayed when other jobs with higher priority are submitted.
</p><pre>$ showstart 20030021
job 20030021 requires 896 procs for 1:00:00
Earliest start in       5:20:52:52 on Tue Mar 24 07:36:36
Earliest completion in  5:21:52:52 on Tue Mar 24 08:36:36
Best Partition: DEFAULT
</pre>"
273,"","<h2>Purpose</h2><p>The Worker framework has been developed to meet two specific use cases:
</p><ul>
	<li>many small jobs determined by parameter variations; the scheduler's task is easier when it does not have to deal with too many jobs.</li>
	<li>job arrays: replace the <tt>-t</tt> for array requests; this was an experimental feature provided by the torque queue system, but it is not supported by Moab, the current scheduler.</li>
</ul><p>Both use cases often have a common root: the user wants to run a program with a large number of parameter settings, and the program does not allow for aggregation, i.e., it has to be run once for each instance of the parameter values. However, Worker's scope is wider: it can be used for any scenario that can be reduced to a <a href=\"https://en.wikipedia.org/wiki/MapReduce\">MapReduce</a> approach.
</p><p>This how-to shows you how to use the Worker framework.
</p><h2>Prerequisites</h2><p>A (sequential) job you have to run many times for various parameter values. We will use a non-existent program cfd-test by way of running example.
</p><h2>Step by step</h2><p>We will consider the following use cases already mentioned above:
</p><ul>
	<li><a href=\"#parameter-variations\">parameter variations</a>, i.e., many small jobs determined by a specific parameter set;</li>
	<li><a href=\"#job-arrays\">job arrays</a>, i.e., each individual job got a unique numeric identifier.</li>
</ul><h3><a name=\"parameter-variations\"></a>Parameter variations</h3><p>Suppose the program the user wishes to run is 'cfd-test' (this program does not exist, it is just an example) that takes three parameters, a temperature, a pressure and a volume. A typical call of the program looks like:
</p><pre>cfd-test -t 20 -p 1.05 -v 4.3
</pre><p>The program will write its results to standard output. A PBS script (say run.pbs) that would run this as a job would then look like:
</p><pre>#!/bin/bash -l
#PBS -l nodes=1:ppn=1
#PBS -l walltime=00:15:00
cd $PBS_O_WORKDIR
cfd-test -t 20  -p 1.05  -v 4.3
</pre><p>When submitting this job, the calculation is performed or this particular instance of the parameters, i.e., temperature = 20, pressure = 1.05, and volume = 4.3. To submit the job, the user would use:
</p><pre>$ qsub run.pbs
</pre><p>However, the user wants to run this program for many parameter instances, e.g., he wants to run the program on 100 instances of temperature, pressure and volume. To this end, the PBS file can be modified as follows:
</p><pre>#!/bin/bash -l
#PBS -l nodes=1:ppn=8
#PBS -l walltime=04:00:00
cd $PBS_O_WORKDIR
cfd-test -t $temperature  -p $pressure  -v $volume
</pre><p>Note that
</p><ol>
	<li>the parameter values 20, 1.05, 4.3 have been replaced by variables $temperature, $pressure and $volume respectively;</li>
	<li>the <i>number of processors per node</i> has been increased to 8 (i.e., ppn=1 is replaced by ppn=8); and</li>
	<li>the <i>walltime </i>has been increased to 4 hours (i.e., walltime=00:15:00 is replaced by walltime=04:00:00).</li>
</ol><p>The walltime is calculated as follows: one calculation takes 15 minutes, so 100 calculations take 1,500 minutes on one CPU. However, this job will use 7 CPUs (1 is reserved for delegating work), so the 100 calculations will be done in 1,500/7 = 215 minutes, i.e., 4 hours to be on the safe side. Note that starting from version 1.3, a dedicated core is no longer required for delegating work when using the <code>-master</code> flag.  This is however not the default behavior since it is implemented using features that are not standard. This implies that in the previous example, the 100 calculations would be completed in 1,500/8 = 188 minutes.
</p><p>The 100 parameter instances can be stored in a comma separated value file (CSV) that can be generated using a spreadsheet program such as Microsoft Excel, or just by hand using any text editor (do <i><strong>not </strong></i>use a word processor such as Microsoft Word). The first few lines of the file data.txt would look like:
</p><pre>temperature,pressure,volume
20,1.05,4.3
21,1.05,4.3
20,1.15,4.3
21,1.25,4.3
...
</pre><p>It has to contain the names of the variables on the first line, followed by 100 parameter instances in the current example. Items on a line are separated by commas.
</p><p>The job can now be submitted as follows:
</p><pre>$ module load worker/1.5.0-intel-2014a
$ wsub -batch run.pbs -data data.txt
</pre><p>Note that the PBS file is the value of the -batch option . The cfd-test program will now be run for all 100 parameter instances—7 concurrently—until all computations are done. A computation for such a parameter instance is called a work item in Worker parlance.
</p><h3><a name=\"job-arrays\"></a>Job arrays</h3><p>Worker also supports job array-like usage pattern since it offers a convenient workflow.
</p><p>A typical PBS script run.pbs for use with job arrays would look like this:
</p><pre>#!/bin/bash -l
#PBS -l nodes=1:ppn=1
#PBS -l walltime=00:15:00
cd $PBS_O_WORKDIR
INPUT_FILE=\"input_${PBS_ARRAYID}.dat\"
OUTPUT_FILE=\"output_${PBS_ARRAYID}.dat\"
word-count -input ${INPUT_FILE}  -output ${OUTPUT_FILE}
</pre><p>As in the previous section, the word-count program does not exist. Input for this fictitious program is stored in files with names such as input_1.dat, input_2.dat, ..., input_100.dat that the user produced by whatever means, and the corresponding output computed by word-count is written to output_1.dat, output_2.dat, ..., output_100.dat. (Here we assume that the non-existent word-count program takes options -input and -output.)
</p><p>The job would be submitted using:
</p><pre>$ qsub -t 1-100 run.pbs
</pre><p>The effect was that rather than 1 job, the user would actually submit 100 jobs to the queue system (since this puts quite a burden on the scheduler, this is precisely why the scheduler doesn't support job arrays).
</p><p>Using worker, a feature akin to job arrays can be used with minimal modifications to the PBS script:
</p><pre>#!/bin/bash -l
#PBS -l nodes=1:ppn=8
#PBS -l walltime=04:00:00
cd $PBS_O_WORKDIR
INPUT_FILE=\"input_${PBS_ARRAYID}.dat\"
OUTPUT_FILE=\"output_${PBS_ARRAYID}.dat\"
word-count -input ${INPUT_FILE}  -output ${OUTPUT_FILE}
</pre><p>Note that
</p><ol>
	<li>the <i>number of CPUs</i> is increased to 8 (ppn=1 is replaced by ppn=8); and</li>
	<li>the <i>walltime </i>has been modified (walltime=00:15:00 is replaced by walltime=04:00:00).</li>
</ol><p>The walltime is calculated as follows: one calculation takes 15 minutes, so 100 calculation take 1,500 minutes on one CPU. However, this job will use 7 CPUs (1 is reserved for delegating work), so the 100 calculations will be done in 1,500/7 = 215 minutes, i.e., 4 hours to be on the safe side.  Note that starting from version 1.3 when using the <code>-master</code> flag, a dedicated core for delegating work is no longer required. This is however not the default behavior since it is implemented using features that are not standard. So in the previous example, the 100 calculations would be done in 1,500/8 = 188 minutes.
</p><p>The job is now submitted as follows:
</p><pre>$ module load worker/1.5.0-intel-2014a
$ wsub -t 1-100  -batch run.pbs
</pre><p>The word-count program will now be run for all 100 input files—7 concurrently—until all computations are done. Again, a computation for an individual input file, or, equivalently, an array id, is called a work item in Worker speak. Note that in constrast to torque job arrays, a worker job array submits a single job.
</p><h2><a name=\"mapreduce--prologus-and-epilogue\"></a>MapReduce: prologues and epilogue</h2><p>Often, an embarrassingly parallel computation can be abstracted to three simple steps:
</p><ol>
	<li>a preparation phase in which the data is split up into smaller, more manageable chuncks;</li>
	<li>on these chuncks, the same algorithm is applied independently (these are the work items); and</li>
	<li>the results of the computations on those chuncks are aggregated into, e.g., a statistical description of some sort.</li>
</ol><p>The Worker framework directly supports this scenario by using a prologue and an epilogue. The former is executed just once before work is started on the work items, the latter is executed just once after the work on all work items has finished. Technically, the prologue and epilogue are executed by the master, i.e., the process that is responsible for dispatching work and logging progress.
</p><p>Suppose that 'split-data.sh' is a script that prepares the data by splitting it into 100 chuncks, and 'distr.sh' aggregates the data, then one can submit a MapReduce style job as follows:
</p><pre>$ wsub -prolog split-data.sh  -batch run.pbs  -epilog distr.sh -t 1-100
</pre><p>Note that the time taken for executing the prologue and the epilogue should be added to the job's total walltime.
</p><h2>Some notes on using Worker efficiently</h2><ol>
	<li>Worker is implemented using MPI, so it is not restricted to a single compute node, it scales well to many nodes. However, remember that jobs requesting a large number of nodes typically spend quite some time in the queue.</li>
	<li>Worker will be effective when
	<ul>
		<li>work items, i.e., individual computations, are neither too short, nor too long (i.e., from a few minutes to a few hours); and,</li>
		<li>when the number of work items is larger than the number of CPUs involved in the job (e.g., more than 30 for 8 CPUs).</li>
	</ul>
	</li>
</ol><h2><a name=\"monitoring-a-worker-job\"></a>Monitoring a worker job</h2><p>Since a Worker job will typically run for several hours, it may be reassuring to monitor its progress. Worker keeps a log of its activity in the directory where the job was submitted. The log's name is derived from the job's name and the job's ID, i.e., it has the form &lt;jobname&gt;.log&lt;jobid&gt;. For the running example, this could be 'run.pbs.log445948', assuming the job's ID is 445948. To keep an eye on the progress, one can use:
</p><pre>$ tail -f run.pbs.log445948
</pre><p>Alternatively, a Worker command that summarizes a log file can be used:
</p><pre>$ watch -n 60 wsummarize run.pbs.log445948
</pre><p>This will summarize the log file every 60 seconds.
</p><h2><a name=\"time-limits-for-work-items\"></a>Time limits for work items</h2><p>Sometimes, the execution of a work item takes long than expected, or worse, some work items get stuck in an infinite loop. This situation is unfortunate, since it implies that work items that could successfully are not even started. Again, a simple and yet versatile solution is offered by the Worker framework. If we want to limit the execution of each work item to at most 20 minutes, this can be accomplished by modifying the script of the running example.
</p><pre>#!/bin/bash -l
#PBS -l nodes=1:ppn=8
#PBS -l walltime=04:00:00
module load timedrun/1.0.1
cd $PBS_O_WORKDIR
timedrun -t 00:20:00 cfd-test -t $temperature  -p $pressure  -v $volume
</pre><p>Note that it is trivial to set individual time constraints for work items by introducing a parameter, and including the values of the latter in the CSV file, along with those for the temperature, pressure and volume.
</p><p>Also note that 'timedrun' is in fact offered in a module of its own, so it can be used outside the Worker framework as well.
</p><h2><a name=\"resuming-a-worker-job\"></a>Resuming a Worker job</h2><p>Unfortunately, it is not always easy to estimate the walltime for a job, and consequently, sometimes the latter is underestimated. When using the Worker framework, this implies that not all work items will have been processed. Worker makes it very easy to resume such a job without having to figure out which work items did complete successfully, and which remain to be computed. Suppose the job that did not complete all its work items had ID '445948'.
</p><pre>$ wresume -jobid 445948
</pre><p>This will submit a new job that will start to work on the work items that were not done yet. Note that it is possible to change almost all job parameters when resuming, specifically the requested resources such as the number of cores and the walltime.
</p><pre>$ wresume -l walltime=1:30:00 -jobid 445948
</pre><p>Work items may fail to complete successfully for a variety of reasons, e.g., a data file that is missing, a (minor) programming error, etc. Upon resuming a job, the work items that failed are considered to be done, so resuming a job will only execute work items that did not terminate either successfully, or reporting a failure. It is also possible to retry work items that failed (preferably after the glitch why they failed was fixed).
</p><pre>$ wresume -jobid 445948 -retry
</pre><p>By default, a job's prologue is not executed when it is resumed, while its epilogue is. 'wresume' has options to modify this default behavior.
</p><h2>Aggregating result data</h2><p>In some settings, each work item produces a file as output, but the final result should be an aggregation of those files. Although this is not necessarily hard, it is tedious, but Worker can help you achieve this easily since, typically, the file name produced by a work item is based on the parameters of that work item.
</p><p>Consider the following data file <code>data.csv</code>:
</p><pre>a,   b
1.3, 5.7
2.7, 1.4
3.4, 2.1
4.1, 3.8
</pre><p>Processing it would produce 4 files, i.e., <code>output-1.3-5.7.txt</code>, <code>output-2.7-1.4.txt</code>, <code>output-3.4-2.1.txt</code>, <code>output-4.1-3.8.txt</code>.  
To obtain the final data, these files should be concatenated into a single file 
	<code>output.txt</code>.  This  can be done easily using <code>wcat</code>:
</p><pre>$ wcat  -data data.csv  -pattern output-[%a%]-[%b%].txt  -output output.txt
</pre><p>The pattern describes the file names as generated by each work item in terms of the parameter names and values defined in the data file <code>data.csv</code>.
</p><p><code>wcat</code> optionally skips headers of all of the first file when the <code>-skip_first <em>n</em></code> option is used (<em>n</em> is the number of lines to skip). By default, blank lines are omitted, but by using the <code>-keep_blank</code> options, they will be written to the output file. 
Help is available using the 
	<code>-help</code> flag.
</p><h2>Multithreaded work items</h2><p>When a cluster is configured to use CPU sets, using Worker to execute multithreaded work items doesn't work by default. Suppose a node has 20 cores, and each work item runs most efficiently on 4 cores, then one would expect that the following resource specification would work:
</p><pre>$ wsub  -l nodes=10:ppn=5 -W x=nmatchpolicy=exactnode  -batch run.pbs  \\
        -data my_data.csv
</pre><p>This would run 5 work items per node, so that each work item would have 4 cores at its disposal. However, this will not work when CPU sets are active since the four work item threads would all run on a single core, which is detrimental for application performance, and leaves 15 out of the 20 cores idle. Simply adding the -threaded option will ensure that the behavior and performance is as expected:
</p><pre> $ wsub -l nodes=10:ppn=5 -batch run.pbs -data my_data.csv -threaded 4
</pre><p>Note however that using multihreaded work items may actually be less efficient than single threaded execution in this setting of many work items since the thread management overhead will be accumulated.
</p><p>Also note that this feature is new since Worker version 1.5.x.
</p><h2>Further information</h2><p>For the information about the most recent version and new features please check <a href=\"http://worker.readthedocs.io/en/latest/\" target=\"_blank\">the official worker documentation webpage</a>.</p><p>For information on how to MPI programs as work items, please contact your friendly system administrator.</p><p>This how-to introduces only Worker's basic features. The <code>wsub</code> command and all other Worker commands have some usage information that is printed when the <code>-help</code> option is specified:
</p><pre>### error: batch file template should be specified
### usage: wsub  -batch &lt;batch-file&gt;          \\
#                [-data &lt;data-files&gt;]         \\
#                [-prolog &lt;prolog-file&gt;]      \\
#                [-epilog &lt;epilog-file&gt;]      \\
#                [-log &lt;log-file&gt;]            \\
#                [-mpiverbose]                \\
#                [-master]                    \\
#                [-threaded]                  \\
#                [-dryrun] [-verbose]         \\
#                [-quiet] [-help]             \\
#                [-t &lt;array-req&gt;]             \\
#                [&lt;pbs-qsub-options&gt;]
#
#   -batch &lt;batch-file&gt;   : batch file template, containing variables to be
#                           replaced with data from the data file(s) or the
#                           PBS array request option
#   -data &lt;data-files&gt;    : comma-separated list of data files (default CSV
#                           files) used to provide the data for the work
#                           items
#   -prolog &lt;prolog-file&gt; : prolog script to be executed before any of the
#                           work items are executed
#   -epilog &lt;epilog-file&gt; : epilog script to be executed after all the work
#                           items are executed
#   -mpiverbose           : pass verbose flag to the underlying MPI program
#   -verbose              : feedback information is written to standard error
#   -dryrun               : run without actually submitting the job, useful
#   -quiet                : don't show information
#   -help                 : print this help message
#   -master               : start an extra master process, i.e.,
#                           the number of slaves will be nodes*ppn
#   -threaded             : indicates that work items are multithreaded,
#                           ensures that CPU sets will have all cores,
#                           regardless of ppn, hence each work item will
#                           have &lt;total node cores&gt;/ppn cores for its
#                           threads
#   -t &lt;array-req&gt;        : qsub's PBS array request options, e.g., 1-10
#   &lt;pbs-qsub-options&gt;    : options passed on to the queue submission
#                           command
</pre><h2>Troubleshooting</h2><p>The most common problem with the Worker framework is that it doesn't seem to work at all, showing messages in the error file about module failing to work.  The cause is trivial, and easy to remedy.
</p><p>Like any PBS script, a worker PBS file <i>has to be</i> in UNIX format!
</p><p>If you edited a PBS script on your desktop, or something went wrong during sftp/scp, the PBS file may end up in DOS/Windows format, i.e., it has the wrong line endings.  The PBS/torque queue system can not deal with that, so you will have to convert the file, e.g., for file 'run.pbs'
</p><pre>$ dos2unix run.pbs</pre>"
275,"","<h2>Purpose</h2><p>Estimating the amount of memory an application will use during execution is often non trivial, especially when one uses third-party software. However, this information is valuable, since it helps to determine the characteristics of the compute nodes a job using this application should run on.</p><p>Although the tool presented here can also be used to support the software development process, better tools are almost certainly available.</p><p>Note that currently only single node jobs are supported, MPI support may be added in a future release.</p><h2>Prerequisites</h2><p>The user should be familiar with the linux bash shell.</p><h2>Monitoring a program</h2><p>To start using monitor, first load the appropriate module:</p><pre>$ module load monitor
</pre><p>Starting a program, e.g., simulation, to monitor is very straightforward</p><pre>$ monitor simulation</pre><p>monitor will write the CPU usage and memory consumption of simulation to standard error.  Values will be displayed every 5 seconds.  This is the rate at which monitor samples the program's metrics.</p><h3>Log file</h3><p>Since monitor's output may interfere with that of the program to monitor, it is often convenient to use a log file.  The latter can be specified as follows:</p><pre>$ monitor -l simulation.log simulation</pre><p>For long running programs, it may be convenient to limit the output to, e.g., the last minute of the programs execution.  Since monitor provides metrics every 5 seconds, this implies we want to limit the output to the last 12 values to cover a minute:</p><pre>$ monitor -l simulation.log -n 12 simulation</pre><p>Note that this option is only available when monitor writes its metrics to a log file, not when standard error is used.</p><h3>Modifying the sample resolution</h3><p>The interval at which monitor will show the metrics can be modified by specifying delta, the sample rate:</p><pre>$ monitor -d 60 simulation</pre><p>monitor will now print the program's metrics every 60 seconds.  Note that the minimum delta value is 1 second.</p><h3>File sizes</h3><p>Some programs use temporary files, the size of which may also be a useful metric.  monitor provides an option to display the size of one or more files:</p><pre>$ monitor -f tmp/simulation.tmp,cache simulation</pre><p>Here, the size of the file simulation.tmp in directory tmp, as well as the size of the file cache will be monitored. Files can be specified by absolute as well as relative path, and multiple files are separated by ','.</p><h3>Programs with command line options</h3><p>Many programs, e.g., matlab, take command line options.  To make sure these do not interfere with those of monitor and vice versa, the program can for instance be started in the following way:</p><pre>$ monitor -delta 60 -- matlab -nojvm -nodisplay computation.m</pre><p>The use of '--' will ensure that monitor does not get confused by matlab's '-nojvm' and '-nodisplay' options.</p><h3>Subprocesses and multicore programs</h3><p>Some processes spawn one or more subprocesses.  In that case, the metrics shown by monitor are aggregated over the process and all of its subprocesses (recursively).  The reported CPU usage is the sum of all these processes, and can thus exceed 100 %.</p><p>Some (well, since this is a HPC cluster, we hope most) programs use more than one core to perform their computations.  Hence, it should not come as a surprise that the CPU usage is reported as larger than 100 %.</p><p>When programs of this type are running on a computer with <i>n</i> cores, the CPU usage can go up to <i>n</i> x 100 %.</p><h3>Exit codes</h3><p>monitor will propagate the exit code of the program it is watching.  Suppose the latter ends normally, then monitor's exit code will be 0.  On the other hand, when the program terminates abnormally with a non-zero exit code, e.g., 3, then this will be monitor's exit code as well.</p><p>When monitor has  to terminate in an abnormal state, for instance if it can't create the log file, its exit code will be 65.  If this interferes with an exit code of the program to be monitored, it can be modified by setting the environment variable MONITOR_EXIT_ERROR to a more suitable value.</p><h2>Monitoring a running process</h2><p>It is also possible to \"attach\" monitor to a program or process that is already running.  One simply determines the relevant process ID using the ps command, e.g., 18749, and starts monitor:</p><pre>$ monitor -p 18749</pre><p>Note that this feature can be (ab)used to monitor specific subprocesses.</p><h2>More information</h2><p>Help is avaible for monitor by issuing:</p><pre>$ monitor -h
### usage: monitor [-d &lt;delta&gt;] [-l &lt;logfile&gt;] [-f &lt;files&gt;]
#                  [-h] [-v] &lt;cmd&gt; | -p &lt;pid&gt;
# Monitor can be used to sample resource utilization of a process
# over time.  Monitor can sample a running process if the latter's PID
# is specified using the -p option, or it can start a command with
# parameters passed as arguments.  When one has to specify flags for
# the command to run, '--' can be used to delimit monitor's options, e.g.,
#    monitor -delta 5 -- matlab -nojvm -nodisplay calc.m
# Resources that can be monitored are memory and CPU utilization, as
# well as file sizes.
# The sampling resolution is determined by delta, i.e., monitor samples
# every &lt;delta&gt; seconds.
# -d &lt;delta&gt;   : sampling interval, specified in
#                seconds, or as [[dd:]hh:]mm:ss
# -l &lt;logfile&gt; : file to store sampling information; if omitted,
#                monitor information is printed on stderr
# -n &lt;lines&gt;   : retain only the last &lt;lines&gt; lines in the log file,
#                note that this option only makes sense when combined
#                with -l, and that the log file lines will not be sorted
#                according to time
# -f &lt;files&gt;   : comma-separated list of file names that are monitored
#                for size; if a file doesn't exist at a given time, the
#                entry will be 'N/A'
# -v           : give verbose feedback
# -h           : print this help message and exit
# &lt;cmd&gt;        : actual command to run, followed by whatever
#                parameters needed
# -p &lt;pid&gt;     : process ID to monitor
#
# Exit status: * 65 for any montor related error
#              * exit status of &lt;cmd&gt; otherwise
# Note: if the exit code 65 conflicts with those of the
#       command to run, it can be customized by setting the
#       environment variables 'MONITOR_EXIT_ERROR' to any value
#       between 1 and 255 (0 is not prohibited, but this is probably.
#       not what you want).</pre>"
277,"","<h2>What is checkpointing</h2><p>Checkpointing allows for running jobs that run for weeks or months. Each time a subjob is running out of requested wall time, a snapshot of the application memory (and much more) is taken and stored, after which a subsequent subjob will pick up the checkpoint and continue.</p><p>If the compute nodes have support for BLCR, checkpointing can be used.</p><h2>How to use it</h2><p>Using checkpointing is very simple: just use <strong>csub</strong> instead of qsub to submit a job.</p><p>The csub command creates a wrapper around your job script, to take care of all the checkpointing stuff.  In practice, you (usually) don't need to adjust anything, except for the command used to submit your job. Checkpointing does not require any changes to the application you are running, and should support most software. There are a few corner cases however (see <a href=\"https://upc-bugs.lbl.gov/blcr/doc/html/FAQ.html\" target=\"_blank\">the BLCR Frequently Asked Questions</a>).</p><h2>The csub command</h2><p>Typically, a job script is submitting with checkpointing support enabled by running:</p><pre>$ csub -s job_script.sh</pre><p>One important caveat is that the job script (or the applications run in the script) should not create it's own local temporary directories.</p><p>Also note that adding PBS directives (#PBS) in the job script is useless, as they will be ignored by csub. Controlling job parameters should be done via the csub command line.</p><p>Help on the various command line parameters supported by csub can be obtained using -h:</p><pre> $ csub -h

    csub [opts] [-s jobscript]
    
    Options:
        -h or --help               Display this message
        
        -s                         Name of jobscript used for job.
                                   Warning: The jobscript should not create it's own local temporary directories.
        
        -q                         Queue to submit job in [default: scheduler default queue]
        
        -t                         Array job specification (see -t in man qsub) [default: none]
        
        --pre                      Run prestage script (Current: copy local files) [default: no prestage]

        --post                     Run poststage script (Current: copy results to localdir/result.) [default: no poststage]

        --shared                   Run in shared directory (no pro/epilogue, shared checkpoint) [default: run in local dir]

        --no_mimic_pro_epi         Do not mimic prologue/epilogue scripts [default: mimic pro/epi (bug workaround)]
        
        --job_time=&lt;string&gt;        Specify wall time for job (format: &lt;hours&gt;:&lt;minutes&gt;:&lt;seconds&gt;s, e.g. 3:12:47) [default: 10h]

        --chkpt_time=&lt;string&gt;      Specify time for checkpointing a job (format: see --job_time) [default: 15m]
        
        --cleanup_after_restart    Specify whether checkpoint file and tarball should be cleaned up after a successful restart
                                   (NOT RECOMMENDED!) [default: no cleanup]
        
        --no_cleanup_chkpt         Don't clean up checkpoint stuff in $VSC_SCRATCH/chkpt after job completion [default: do cleanup]
        
        --resume=&lt;string&gt;          Try to resume a checkpointed job; argument should be unique name of job to resume [default: none]
        
        --chkpt_save_opt=&lt;string&gt;  Save option to use for cr_checkpoint (all|exe|none) [default: exe]
        
        --term_kill_mode           Kill checkpointed process with SIGTERM instead of SIGKILL after checkpointing [defailt: SIGKILL]
        
        --vmem=&lt;string&gt;            Specify amount of virtual memory required [default: none specified]\"

</pre><p>Below we discuss various command line parameters.</p><dl>
    <dt><strong>--pre</strong> and <strong>--post</strong></dt>
    <dd>The --pre and --post parameters steer whether local files are copied or not. The job submitted using csub is (by default) runs on the local storage provided by a particular compute node. Thus, no changes will be made to the files on the shared storage (e.g. $VSC_SCRATCH).<br>
    If the job script needs (local) access to the files of the directory <i>where csub is executed</i>, <strong>--pre</strong> should be specified. This will copy all the files in the job script directory to the location where the job script will execute.<br>
    If the output of the job that was run, or additional output files created by the job in it's working directory are required, <strong>--post</strong> should be used. This will copy the entire job working directory<i> to the location where csub was executed</i>, in a directory named result.&lt;jobname&gt;. An alternative is to copy the interesting files to the shared storage at the end of the job script.</dd>
    <dt><strong>--shared</strong></dt>
    <dd>If the job needs to be run <i>on the shared storage</i> and not on the local storage of the workernode (for whatever reason), <strong>--shared</strong> should be specified. In this case, the job will be run in a subdirectory of $VSC_SCRATCH/chkpt. This will also disable the execution of the prologue and epilogue scripts, which prepare the job directory on the local storage.</dd>
    <dt><strong>--job_time</strong> and <strong>--chkpt_time</strong></dt>
    <dd>To specify the requested wall time <i>per subjob</i>, use the <strong>--job-time</strong> parameter. The default settings is 10 hours per subjob. Lowering this will result in more frequent checkpointing, and thus more subjobs.<br>
    To specify <i>the time that is reserved for checkpointing</i> the job, use <strong>--chkpt_time</strong>. By default, this is set to 15 minutes which should be enough for most applications/jobs. Don't change this unless you really need to.<br>
    <strong>The total requested wall time per subjob is the sum of both job_time and chkpt_time.</strong> This should be taken into account when submitting to a specific job queue (e.g., queues which only support jobs of up to 1 hour).</dd>
    <dt><strong>--no_mimic_pro_epi</strong></dt>
    <dd>The option <strong>--no_mimic_pro_epi</strong> disables the workaround currently implemented for a permissions problem when using actual Torque prologue/epilogue scripts. Don't use this option unless you really know what you're doing!</dd>
</dl><h2>Support for csub</h2><ul>
    <li><strong>Array jobs</strong><br>
    csub has support for checkpointing array jobs.  Just specify \"-t &lt;spec&gt;\" on the csub command line (see qsub for details).</li>
    <li><strong>MPI support</strong><br>
    The BLCR checkpointing mechanism behind csub has support for checkpointing MPI applications.  However, checkpointing MPI applications is pretty much untested up until now.  If you would like to use csub with your MPI applications, please <a href=\"/support/contact-support\">contact user support</a>.</li>
</ul><h2>Notes</h2><p>If you would like to time how long the complete job executes, just prepend the main command in your job script with time, e.g.: <strong>time &lt;command&gt;</strong>. The <i>real</i> time will not make sense as it will also include the time passes between two checkpointed subjobs. However, the <i>user</i> time should give a good indication of the actual time it took to run your command, even if multiple checkpoints were performed.</p>"
279,"","<ul><li>
<a href=\"/cluster-doc/running-jobs/credit-system-basics\">Credit system basics</a> (KU Leuven-only currently, Tier-1 and Tier-2)
</li></ul>"
281,"","<p>This section is still rather empty. It will be expanded over time.
</p><h2>Visualization software</h2><ul>
	<li><a href=\"https://www.paraview.org/\" target=\"_blank\">ParaView</a> is a free visualization package. It can be used in three modes:
	<ul>
		<li>Installed on your desktop: you have to transfer your data to your desktop system</li>
		<li>As an interactive process on the cluster: this option is available only for <a href=\"/client/multiplatform/nx-start-guide\">NoMachine NX users</a> (go to the Applications menu -&gt; HPC -&gt; Visualisation -&gt; Paraview).</li>
		<li>In client-server mode: The interactive part of ParaView is running on your desktop, while the server part that reads the data and renders the images (no GPU required as ParaView also contains a software OpenGL renderer) and sends the rendered images to the client on the desktop. Setting up ParaView for this scenario is explained in the <a href=\"/cluster-doc/postprocessing/paraview-remote-visualization\">page on ParaView remote visualization</a>.</li>
	</ul>
	</li>
</ul>"
283,"","<h2>Prerequisits</h2><p>You should have ParaView installed on your desktop, and know how to use it (the latter is outside the scope of this page). <strong>Note</strong>: the client and server version should match to avoid problems!
</p><h2>Overview</h2><p>Working with ParaView to remotely visualize data requires the following steps which will be explained in turn in the subsections below:
</p><ol>
    <li>start ParaView on the cluster;</li>
    <li>establish an SSH tunnel;</li>
    <li>connect to the remote server using ParaView on your desktop; and</li>
    <li>terminating the server session on the compute node.</li>
</ol><h3>Start ParaView on the cluster</h3><p>First, start an interactive job on the cluster, e.g.,
</p><pre>$ qsub  -I  -l nodes=1,ppn=20
</pre><p>Given that remote visualization makes sense most for large data sets, 64 GB of RAM is probably the minimum you will need. To use a node with more memory, add a memory specification, e.g., <code>-l mem=120gb</code>.   If this is not sufficient, you should consider using Cerebro.
</p><p>Once this interactive session is active, you can optionally navigate to the directory containing the data to visualize (not shown below), load the appropriate module, and start the server:
</p><pre>$ module load Paraview/4.1.0-foss-2014a
$ n_proc=$(cat $PBS_NODEFILE  |  wc  -l)
$ mpirun  -np $n_proc pvserver  --use-offscreen-rendering \\
                                --server-port=11111
</pre><p>Note the compute node's name your job is running on, you will need it in the next step to establish the required SSH tunnel.
</p><h3>Establish an SSH tunnel</h3><p>To connect the desktop ParaView client with the desktop with the ParaView server on the compute node, an SSH tunnel has to be established between your desktop and that compute node.  Details for <a href=\"/client/windows/creating-an-ssh-tunnel\">Windows using PuTTY</a> and <a href=\"/client/linux/creating-an-ssh-tunnel\">Linux using ssh</a> are given in the appropriate client software sections.
</p><h3>Connect to the remote server using ParaView on your desktop</h3><p>Since ParaView's user interface is identical on all platforms, connecting from the client side is documented on this page. Note that this configuration step has to be performed only once if you always use the same local port.
</p><ul>
    <li>Start ParaView on your Desktop machine;</li>
    <li>From the 'File' menu, choose 'Connect', this opens the dialog below:</li>
</ul><p style=\"text-align: center; \"><img src=\"/assets/113\" alt=\"&quot;Choose Server&quot; dialog box\">
</p><ul>
    <li>Click the 'Add Server' button, the following dialog will appear:</li>
</ul><p style=\"text-align: center; \"><img src=\"/assets/115\" alt=\"&quot;Configure New Server&quot; dialog box\">
</p><ul>
    <li>Enter a name in the 'Name' field, e.g., 'Thinking'. If you have used 11111 as the local port to establish the tunnen, just click the 'Configure' button, otherwise modify the 'Port' field appropriately and click 'Configure'. This opens the 'Configure Server' dialog:</li>
</ul><p style=\"text-align: center; \"><img src=\"/assets/117\" alt=\"&quot;Configure Server&quot; dialog box\"></p><ul>
    <li>Set the 'Startup Type' from 'Command' to 'Manual' in the drop-down menu, and click 'Save'.</li>
    <li>In the 'Choose Server' dialog, select the server, i.e., 'Thinking' and click the 'Connect' button.</li>
</ul><p style=\"text-align: center; \"><img src=\"/assets/119\" alt=\"&quot;Choose Server Configuration&quot; dialog box\" style=\"width: 400px;\">
</p><p style=\"text-align: left; \">You can now work with ParaView as you would when visualizing local files.
</p><h3 style=\"text-align: left; \">Terminating the server session on the compute node</h3><p>Once you've quit ParaView on the desktop the server process will terminate automatically.  However, don't forget to close your session on the compute node since leaving it open will consume credits.
</p><pre>$ logout
</pre><h2>Further information</h2><p><a href=\"https://www.paraview.org\">More information on ParaView can be found on its website</a><a href=\"https://www.paraview.org/\" target=\"_blank\"></a>. A <a href=\"https://www.vtk.org/Wiki/images/8/88/ParaViewTutorial38.pdf\" target=\"_blank\">decent tutorial</a> on using Paraview is also available from the VTK public wiki.
</p>"
285,"","<p>BEgrid is currently documented by BELNET. Some useful links are:</p><ul>
<li><a href=\"http://wiki.begrid.be/\" target=\"_blank\">BEgrid Wiki</a></li>
<li><a href=\"https://edms.cern.ch/file/722398/1.2/gLite-3-UserGuide.pdf\">gLite 3.1 User Guide (PDF, op CERN)</a><br>gLite is the grid middleware used on BEgrid.</li>
<li><a href=\"http://www.begrid.be/index.php?module=webpage&amp;id=16\">Other related links on the Belnet site.</a></li>
</ul>"
287,"","<p>Data on the VSC clusters can be stored in several locations, depending on the size and usage of these data. Following locations are available:</p><ul>
    <li><a href=\"#Home\">Home directory</a>

    <ul>
        <li>Location available as $VSC_HOME</li>
        <li>The data stored here should be relatively small (e.g., no files or directories larger than a gigabyte, although this is allowed), and not generating very intense I/O during jobs. <br>
        Also all kinds of configuration files are stored here, e.g., ssh-keys, .bashrc, or Matlab, and Eclipse configuration, ...</li>
    </ul>
    </li>
    <li><a href=\"#Data\">Data directory</a>
    <ul>
        <li>Location available as $VSC_DATA</li>
        <li>A bigger 'workspace', for datasets, results, logfiles, ... . This filesystem can be used for higher I/O loads, but for I/O bound jobs, you might be better of using one of the 'scratch' filesystems.</li>
    </ul>
    </li>
    <li><a href=\"#Scratch\">Scratch directories</a>
    <ul>
        <li>Several types exist, available in $VSC_SCRATCH_XXX variables</li>
        <li>For temporary or transient data; there is typically no backup for these filesystems, and 'old' data may be removed automatically.</li>
        <li>Currently, $VSC_SCRATCH_NODE, $VSC_SCRATCH_SITE and $VSC_SCRATCH_GLOBAL are defined, for space that is available per node, per site, or globally on all nodes of the VSC (currenlty, there is no real 'global' scratch filesystem yet).</li>
    </ul>
    </li>
</ul><p>Since these directories are not necessarily mounted on the same locations over all sites, you should always (try to) use the environment variables that have been created.</p><p>Quota is enabled on the three directories, which means the amount of data you can store here is limited by the operating system, and not by \"the boundaries of the hard disk\". You can see your current usage and the current limits with the appropriate quota command as explained on <a href=\"/cluster-doc/check-disk-usage\">How do I know how much disk space I am using?</a>. The actual disk capacity, shared by <i>all</i> users, can be found on the  <a href=\"/infrastructure/hardware\">Available hardware</a> page.</p><p>You will only receive a warning when you reach the soft limit of either quota. You will only start losing data when you reach the hard limit. Data loss occurs when you try to save new files: this will not work because you have no space left, and thus you will lose these new files. You will however not be warned when data loss occurs, so keep an eye open for the general quota warnings! The same holds for running jobs that need to write files: when you reach your hard quota, jobs will crash.</p><h2><a name=\"Home\"></a>Home directory</h2><p>This directory is where you arrive by default when you login to the cluster. Your shell refers to it as \"~\" (tilde), or via the environment variable $VSC_HOME.</p><p>The data stored here should be relatively small (e.g., no files or directories larger than a gigabyte, although this is allowed), and usually used frequently. Also all kinds of configuration files are stored here, e.g., by Matlab, Eclipse, ...</p><p>The operating system also creates a few files and folders here to manage your account. Examples are:</p><table class=\"prettytable\">
    <tbody>
        <tr>
            <td>.ssh/</td>
            <td>This directory contains some files necessary for you to login to the cluster and to submit jobs on the cluster. Do not remove them, and do not alter anything if you don't know what you're doing!</td>
        </tr>
        <tr>
            <td>.profile</td>
            <td>This script defines some general settings about your sessions,</td>
        </tr>
        <tr>
            <td>.bashrc</td>
            <td>This script is executed everytime you start a session on the cluster: when you login to the cluster and when a job starts. You could edit this file and, e.g., add \"module load XYZ\" if you want to automatically load module XYZ whenever you login to the cluster, although we do not recommend to load modules in your .bashrc.</td>
        </tr>
        <tr>
            <td>.bash_history</td>
            <td>This file contains the commands you typed at your shell prompt, in case you need them again.</td>
        </tr>
    </tbody>
</table><h2><a name=\"Data\"></a>Data directory</h2><p>In this directory you can store all other data that you need for longer terms. The environment variable pointing to it is $VSC_DATA. There are no guarantees about the speed you'll achieve on this volume.</p><h2><a name=\"Scratch\"></a>Scratch space</h2><p>To enable quick writing from your job, a few extra file systems are available on the work nodes. These extra file systems are called scratch folders, and can be used for storage of temporary and/or transient data (temporary results, anything you just need during your job, or your batch of jobs).</p><p>You should remove any data from these systems after your processing them has finished. There are no gurarantees about the time your data will be stored on this system, and we plan to clean these automatically on a regular base. The maximum allowed age of files on these scratch file systems depends on the type of scratch, and can be anywhere between a day and a few weeks. We don't guarantee that these policies remain forever, and may change them if this seems necessary for the healthy operation of the cluster.</p><p>Each type of scratch has his own use:</p><ul>
    <li><strong>Node scratch ($VSC_SCRATCH_NODE)</strong><br>
    Every node has its own scratch space, which is completely seperated from the other nodes. Every job automatically gets its own temporary directory on this node scratch, available through the environment variable $TMPDIR. $TMPDIR is guaranteed to be unique for each job.<br>
    Note however that when your job requests multiple cores and these cores happen to be in the same node, this $TMPDIR is shared among the cores!</li>
    <li><strong>Site scratch ($VSC_SCRATCH_SITE, $VSC_SCRATCH)</strong><br>
    To allow a job running on multiple nodes (or multiple jobs running on seperate nodes) to share data as files, every node of the cluster (including the login nodes) has access to this shared scratch directory. Just like the home and data directories, every user has its own scratch directory. Because this scratch is also available from the login nodes, you could manually copy results to your data directory after your job has ended.</li>
    <li><strong>Global scratch ($VSC_SCRATCH_GLOBAL)</strong><br>
    In the long term, this scratch space will be available throughout the whole VSC. At the time of writing, the global scratch is just the same volume as the site scratch, and thus contains the same data.</li>
</ul>"
289,"","<p>Data on the VSC clusters can be stored in several locations, depending on the size and usage of these data. Following locations are available:</p><ul>
    <li><a href=\"#Home\">Home directory</a>

    <ul>
        <li>Location available as $VSC_HOME</li>
        <li>The data stored here should be relatively small (e.g., no files or directories larger than a gigabyte, although this is allowed), and not generating very intense I/O during jobs. <br>
        Also all kinds of configuration files are stored here, e.g., ssh-keys, .bashrc, or Matlab, and Eclipse configuration, ...</li>
    </ul>
    </li>
    <li><a href=\"#Data\">Data directory</a>
    <ul>
        <li>Location available as $VSC_DATA</li>
        <li>A bigger 'workspace', for datasets, results, logfiles, ... . This filesystem can be used for higher I/O loads, but for I/O bound jobs, you might be better of using one of the 'scratch' filesystems.</li>
    </ul>
    </li>
    <li><a href=\"#Scratch\">Scratch directories</a>
    <ul>
        <li>Several types exist, available in $VSC_SCRATCH_XXX variables</li>
        <li>For temporary or transient data; there is typically no backup for these filesystems, and 'old' data may be removed automatically.</li>
        <li>Currently, $VSC_SCRATCH_NODE, $VSC_SCRATCH_SITE and $VSC_SCRATCH_GLOBAL are defined, for space that is available per node, per site, or globally on all nodes of the VSC (currenlty, there is no real 'global' scratch filesystem yet).</li>
    </ul>
    </li>
</ul><p>Since these directories are not necessarily mounted on the same locations over all sites, you should always (try to) use the environment variables that have been created.</p><p>Quota is enabled on the three directories, which means the amount of data you can store here is limited by the operating system, and not by \"the boundaries of the hard disk\". You can see your current usage and the current limits with the appropriate quota command as explained on <a href=\"/cluster-doc/check-disk-usage\">How do I know how much disk space I am using?</a>. The actual disk capacity, shared by <i>all</i> users, can be found on the  <a href=\"/infrastructure/hardware\">Available hardware</a> page.</p><p>You will only receive a warning when you reach the soft limit of either quota. You will only start losing data when you reach the hard limit. Data loss occurs when you try to save new files: this will not work because you have no space left, and thus you will lose these new files. You will however not be warned when data loss occurs, so keep an eye open for the general quota warnings! The same holds for running jobs that need to write files: when you reach your hard quota, jobs will crash.</p><h2><a name=\"Home\"></a>Home directory</h2><p>This directory is where you arrive by default when you login to the cluster. Your shell refers to it as \"~\" (tilde), or via the environment variable $VSC_HOME.</p><p>The data stored here should be relatively small (e.g., no files or directories larger than a gigabyte, although this is allowed), and usually used frequently. Also all kinds of configuration files are stored here, e.g., by Matlab, Eclipse, ...</p><p>The operating system also creates a few files and folders here to manage your account. Examples are:</p><table class=\"prettytable\">
    <tbody>
        <tr>
            <td>.ssh/</td>
            <td>This directory contains some files necessary for you to login to the cluster and to submit jobs on the cluster. Do not remove them, and do not alter anything if you don't know what you're doing!</td>
        </tr>
        <tr>
            <td>.profile</td>
            <td>This script defines some general settings about your sessions,</td>
        </tr>
        <tr>
            <td>.bashrc</td>
            <td>This script is executed everytime you start a session on the cluster: when you login to the cluster and when a job starts. You could edit this file and, e.g., add \"module load XYZ\" if you want to automatically load module XYZ whenever you login to the cluster, although we do not recommend to load modules in your .bashrc.</td>
        </tr>
        <tr>
            <td>.bash_history</td>
            <td>This file contains the commands you typed at your shell prompt, in case you need them again.</td>
        </tr>
    </tbody>
</table><h2><a name=\"Data\"></a>Data directory</h2><p>In this directory you can store all other data that you need for longer terms. The environment variable pointing to it is $VSC_DATA. There are no guarantees about the speed you'll achieve on this volume.</p><h2><a name=\"Scratch\"></a>Scratch space</h2><p>To enable quick writing from your job, a few extra file systems are available on the work nodes. These extra file systems are called scratch folders, and can be used for storage of temporary and/or transient data (temporary results, anything you just need during your job, or your batch of jobs).</p><p>You should remove any data from these systems after your processing them has finished. There are no gurarantees about the time your data will be stored on this system, and we plan to clean these automatically on a regular base. The maximum allowed age of files on these scratch file systems depends on the type of scratch, and can be anywhere between a day and a few weeks. We don't guarantee that these policies remain forever, and may change them if this seems necessary for the healthy operation of the cluster.</p><p>Each type of scratch has his own use:</p><ul>
    <li><strong>Node scratch ($VSC_SCRATCH_NODE)</strong><br>
    Every node has its own scratch space, which is completely seperated from the other nodes. Every job automatically gets its own temporary directory on this node scratch, available through the environment variable $TMPDIR. $TMPDIR is guaranteed to be unique for each job.<br>
    Note however that when your job requests multiple cores and these cores happen to be in the same node, this $TMPDIR is shared among the cores!</li>
    <li><strong>Site scratch ($VSC_SCRATCH_SITE, $VSC_SCRATCH)</strong><br>
    To allow a job running on multiple nodes (or multiple jobs running on seperate nodes) to share data as files, every node of the cluster (including the login nodes) has access to this shared scratch directory. Just like the home and data directories, every user has its own scratch directory. Because this scratch is also available from the login nodes, you could manually copy results to your data directory after your job has ended.</li>
    <li><strong>Global scratch ($VSC_SCRATCH_GLOBAL)</strong><br>
    In the long term, this scratch space will be available throughout the whole VSC. At the time of writing, the global scratch is just the same volume as the site scratch, and thus contains the same data.</li>
</ul>"
303,"","<h2>Hardware details</h2><p>The VUB cluster contains a mix of nodes with AMD and Intel processors and different interconnects in different sections of the cluster. The cluster also contains a number of nodes with NVIDIA GPUs.
</p><h3>Login nodes:</h3><ul>
	<li><code>login.hpc.vub.ac.be</code> or <code>hydra.vub.ac.be</code></li>
	<li>use one of those hostnames if you read vsc.login.node in the documentation and want to connect to this login node</li>
</ul><h3>Compute nodes:</h3><table>
<thead>
<tr>
	<th>nodes
	</th>
	<th>processor
	</th>
	<th>memory
	</th>
	<th> disk
	</th>
	<th>network
	</th>
	<th>others
	</th>
</tr>
</thead>
<tbody>
<tr>
	<td>40
	</td>
	<td>2x 8-core AMD 6134 (Magnycours)
	</td>
	<td>64 Gb
	</td>
	<td>900 Gb
	</td>
	<td>QDR-IB
	</td>
	<td>soon will be phased out
	</td>
</tr>
<tr>
	<td>11
	</td>
	<td>2x 10-core INTEL E5-2680v2 (IvyBridge)
	</td>
	<td>128 Gb
	</td>
	<td>900 Gb
	</td>
	<td>QDR-IB
	</td>
	<td>
	</td>
</tr>
<tr>
	<td>20
	</td>
	<td>2x 10-core INTEL E5-2680v2 (IvyBridge)
	</td>
	<td>256 Gb
	</td>
	<td>900 Gb
	</td>
	<td>QDR-IB
	</td>
	<td>
	</td>
</tr>
<tr>
	<td>6
	</td>
	<td>2x 10-core INTEL E5-2680v2 (IvyBridge)
	</td>
	<td>128 Gb
	</td>
	<td>900 Gb
	</td>
	<td>QDR-IB
	</td>
	<td>2x Tesla K20x NVIDIA GPGPUs<br>with 6Gb memory in each node
	</td>
</tr>
<tr>
	<td>27
	</td>
	<td>2x 14-core INTEL E5-2680v4 (Broadwell)
	</td>
	<td>256 Gb
	</td>
	<td>1 Tb
	</td>
	<td>10 Gbps
	</td>
	<td><br>
	</td>
</tr>
<tr>
	<td>1
	</td>
	<td>4x 10-core INTEL E7-8891v4 (Broadwell)
	</td>
	<td>1.5 Tb
	</td>
	<td>4 Tb
	</td>
	<td>10 Gbps
	</td>
	<td><br>
	</td>
</tr>
<tr>
	<td>4
	</td>
	<td>2x 12-core INTEL E5-2650v4 (Broadwell)
	</td>
	<td>256 Gb
	</td>
	<td>2 Tb
	</td>
	<td>10 Gbps
	</td>
	<td>2x Tesla P100 NVIDIA GPGPUs<br>with 16 Gb memory in each node<br>
	</td>
</tr>
<tr>
	<td>1
	</td>
	<td>2x 16-core INTEL E5-2683v4 (Broadwell)
	</td>
	<td>512 Gb
	</td>
	<td>8 Tb
	</td>
	<td>10 Gbps
	</td>
	<td>4x  GeForce GTX 1080 Ti NVIDIA GPUs with 12 Gb memory in each node<br>
	</td>
</tr>
<tr>
	<td>21
	</td>
	<td>2x 20-core INTEL Xeon Gold 6148 (Skylake)
	</td>
	<td>192 Gb
	</td>
	<td>1 Tb
	</td>
	<td>10 Gbps
	</td>
	<td><br>
	</td>
</tr>
</tbody>
</table><h3>Network Storage:</h3><ul>
	<li>19 TB NAS for Home directories (<code>$VSC_HOME</code>) and software storage connected with 1Gb Ethernet</li>
	<li>780 TB GPFS storage for global scratch (<code>$VSC_SCRATCH</code>) connected with QDR-IB, 1Gb and 40 Gb  Ethernet </li>
</ul>"
305,"","<p>UAntwerpen has two clusters. <a href=\"#leibniz\">leibniz</a> and <a href=\"#hopper\">hopper</a>, <a href=\"#turing\">Turing</a>, an older cluster, has been retired in the early 2017.
</p><h2><a name=\"leibniz\"></a>Local documentation</h2><ul>
	<li><a href=\"/assets/1323\">Slides of the information sessions on \"Transitioning to Leibniz and CentOS 7\" (PDF)</a></li>
	<li><a href=\"/infrastructure/hardware/hardware-ua/toolchain-2017a\">The 2017a toolchain at UAntwerp</a>: In preparation of the integration of Leibniz in the UAntwerp infrastructure, the software stack has been rebuild in the 2017a toolchain. Several changes have been made to the naming and the organization of the toolchains. The toolchain is now loaded by default on Hopper, and is the main toolchain on Leibniz and later also on Hopper after an OS upgrade.</li>
	<li><a href=\"/infrastructure/hardware/hardware-ua/intel\">The Intel compiler toolchains</a>: From the 2017a toolchain on, the setup of the toolchains differs on the UAntwerp clusters differs from most other VSC systems. We have set up the Intel compilers, including all libraries, in a single directory structure as intended by Intel. Some scripts, including compiler configuration scripts, expect this setup to work properly.</li>
	<li><a href=\"https://www.vscentrum.be/infrastructure/hardware/hardware-ua/licensed-software\">Licensed software at UAntwerp</a>: Some software has a restricted license and is not available to all users. This page lists some of those packages and explains for some how you can get access to the package.</li>
	<li>Special nodes:
	<ul>
		<li><a href=\"/infrastructure/hardware/hardware-ua/visualization\">GUI programs and remote visualization node</a>: Leibniz offers a remote visualization node with software stack based on TurboVNC and OpenGL. All other login nodes offer the same features minus the OpenGL support (so applications have to link to a OpenGL software emulation library).</li>
		<li><a href=\"/infrastructure/hardware/hardware-ua/gpu-computing\">GPU computing nodes</a></li>
		<li><a href=\"/infrastructure/hardware/hardware-ua/xeonphi\">Xeon Phi testbed</a></li>
	</ul></li>
	<li><a href=\"/infrastructure/hardware/hardware-ua/leibniz-instructions\">Information for Leibniz test users</a></li>
</ul><h2>Leibniz</h2><p>Leibniz was installed in the spring of 2017. It is a NEC system consisting of 152 nodes with 2 14-core intel <a href=\"https://ark.intel.com/nl/products/91754/Intel-Xeon-Processor-E5-2680-v4-35M-Cache-2_40-GHz\">E5-2680v4</a> Broadwell generation CPUs connected through a EDR InfiniBand network. 144 of these nodes have 128 GB RAM, the other 8 have 256 GB RAM. The nodes do not have a sizeable local disk.  The cluster also contains a node for visualisation, 2 nodes for GPU computing (NVIDIA Psscal generation) and one node with an Intel Xeon Phi expansion board.
</p><h3>Access restrictions<a id=\"AccessRestrictions\" name=\"AccessRestrictions\"></a></h3><p>Access ia available for faculty, students (master's projects under faculty supervision), and researchers of the AUHA. The cluster is integrated in the VSC network and runs the standard VSC software setup. It is also available to all VSC-users, though we appreciate that you contact the UAntwerpen support team so that we know why you want to use the cluster.
</p><p>Jobs can have a maximal execution wall time of 3 days (72 hours).
</p><h3>Hardware details</h3><ul>
	<li>Interactive work:
	<ul>
		<li>2 login nodes. These nodes have a very similar architecture to the compute nodes.</li>
	</ul>
	<ul>
		<li>1 visualisation node with a NVIDIA P5000 GPU. This node is meant to be used for interactive visualizations (<a href=\"/infrastructure/hardware/hardware-ua/visualization\">specific instructions</a>).</li>
	</ul></li>
	<li>Compute nodes:
	<ul>
		<li>144 nodes with 2 14-core Intel <a href=\"https://ark.intel.com/nl/products/91754/Intel-Xeon-Processor-E5-2680-v4-35M-Cache-2_40-GHz\">E5-2680v4</a> CPUs (Broadwell generation, 2.4 GHz) and 128 GB RAM. </li>
		<li>8 nodes with 2 14-core Intel <a href=\"https://ark.intel.com/nl/products/91754/Intel-Xeon-Processor-E5-2680-v4-35M-Cache-2_40-GHz\">E5-2680v4</a> CPUs (Broadwell generation, 2.4 GHz) and 256 GB RAM. </li>
		<li>2 nodes with 2 14-core Intel <a href=\"https://ark.intel.com/nl/products/91754/Intel-Xeon-Processor-E5-2680-v4-35M-Cache-2_40-GHz\">E5-2680v4</a> CPUs (Broadwell generation, 2.4 GHz), 128 GB RAM and two NVIDIA Tesla P100 GPUs with 16 GB HBM2 memory per GPU (delivering a peak performance of 4.7 TFlops in double precision per GPU) (<a href=\"/infrastructure/hardware/hardware-ua/gpu-computing\">specific instructions</a>).</li>
		<li>1 node with 2 14-core Intel <a href=\"https://ark.intel.com/nl/products/91754/Intel-Xeon-Processor-E5-2680-v4-35M-Cache-2_40-GHz\">E5-2680v4</a> CPUs (Broadwell generation, 2.4 GHz), 128 GB RAM and Intel Xeon Phi 7220P PCIe card with 16 GB of RAM (<a href=\"/infrastructure/hardware/hardware-ua/xeonphi\">specific instructions</a>).</li>
		<li>All nodes are connected through a EDR InfiniBand network</li>
		<li>All compute nodes contain only a small SSD drive. This implies that swapping is not possible and that users should preferably use the main storage for all temporary files also.</li>
	</ul></li>
	<li>Storage: The cluster relies on the storage provided by Hopper (a 100 TB DDN SFA7700 system with 4 storage servers).</li>
</ul><h3>Login infrastructure</h3><p>Direct login is possible to both login nodes and to the visualization node.
</p><ul>
	<li>From outside the VSC network: Use the external interface names. Currently, one needs to be on the network of UAntwerp or some associated institutions to be able to access the external interfaces. Otherwise a VPN connection is needed to the UAntwerp network.</li>
	<li>From inside the VSC network (e.g., another VSC cluster): Use the internal interface names.</li>
</ul><table>
<tbody>
<tr>
	<td>
	</td>
	<td>External interface
	</td>
	<td>Internal interface
	</td>
</tr>
<tr>
	<td>Login generic
	</td>
	<td>login-leibniz.uantwerpen.be<br>
	</td>
	<td>
	</td>
</tr>
<tr>
	<td>Login
	</td>
	<td>login1-leibniz.uantwerpen.be<br>login2-leibniz.uantwerpen.be
	</td>
	<td>ln1.leibniz.antwerpen.vsc<br>ln2.leibniz.antwerpen.vsc
	</td>
</tr>
<tr>
	<td>Visualisation node
	</td>
	<td>viz1-leibniz.uantwerpen.be
	</td>
	<td>viz1.leibniz.antwerpen.vsc
	</td>
</tr>
</tbody>
</table><h3>Storage organization</h3><p>
	See <a href=\"#hopper-storage\">the section on the storage organization of hopper</a>.
</p><h3>Characteristics of the compute nodes</h3><p>Since leibniz is currently a homogenous system with respect to CPU type and interconnect, it is not needed to specify the corresponding properties (see also the page on <a href=\"https://www.vscentrum.be/cluster-doc/running-jobs/specifying-requirements\">specifying resources, output files and notifications</a>).<br>
</p><p>However, to make it possible to write job scripts that can be used on both hopper and leibniz (or other VSC clusters) and to prepare for future extensions of the cluster, the following features are defined:
</p><table>
<tbody>
<tr>
	<th>property
	</th>
	<th>explanation
	</th>
</tr>
<tr>
	<td>broadwell
	</td>
	<td>only use Intel processors from the Broadwell family (E5-XXXv4) (Not needed at the moment as this is the only CPU type)
	</td>
</tr>
<tr>
	<td>ib
	</td>
	<td>use InfiniBand interconnect (not needed at the moment as all nodes are connected to the InfiniBand interconnect)
	</td>
</tr>
<tr>
	<td>mem128
	</td>
	<td>use nodes with 128 GB RAM (roughly 112 GB available). This is the majority of the nodes on leibniz.
	</td>
</tr>
<tr>
	<td>mem256
	</td>
	<td>use nodes with 256 GB RAM (roughly 240 GB available). This property is useful if you submit a batch of jobs that require more than 4 GB of RAM per processor but do not use all cores and you do not want to use a tool to bundle jobs yourself such as Worker, as it helps the scheduler to put those jobs on nodes that can be further filled with your jobs.
	</td>
</tr>
</tbody>
</table><p>These characteristics map to the following nodes on Hopper:
</p><table class=\"plain\">
<tbody>
<tr>
	<th>Type of node
	</th>
	<th>CPU type
	</th>
	<th>Interconnect
	</th>
	<th># nodes
	</th>
	<th># physical<br>cores<br>(per node)
	</th>
	<th># logical<br>cores<br>(per node)
	</th>
	<th>installed mem<br>(per node)
	</th>
	<th>avail mem<br>(per node)
	</th>
	<th>local disc
	</th>
</tr>
<tr>
	<td>broadwell:ib:mem128
	</td>
	<td><a href=\"https://ark.intel.com/products/75277\">Xeon E5-2680v4</a>
	</td>
	<td>IB-EDR
	</td>
	<td>144
	</td>
	<td>28
	</td>
	<td>28
	</td>
	<td>128 GB
	</td>
	<td>112 GB
	</td>
	<td>~25 GB
	</td>
</tr>
<tr>
	<td>broadwell:ib:mem256
	</td>
	<td><a href=\"https://ark.intel.com/products/75277\">Xeon E5-2680v4</a>
	</td>
	<td>IB-EDR
	</td>
	<td>8
	</td>
	<td>28
	</td>
	<td>28
	</td>
	<td>256 GB
	</td>
	<td>240 GB
	</td>
	<td>~25 GB<br><br>
	</td>
</tr>
</tbody>
</table><h2><a name=\"hopper\"></a>Hopper</h2><p>Hopper is the current UAntwerpen cluster. It is a HP system consisting of 168 nodes with 2 10-core Intel E5-2680v2 Ivy Bridge generation CPUs connected through a FDR10 InfiniBand network. 144 nodes have a memory capacity of 64 GB while 24 nodes have 256 GB of RAM memory. The system has been reconfigured to have a software setup that is essentially the same as on Leibniz.</p><h3>Access restrictions<a id=\"AccessRestrictions\" name=\"AccessRestrictions\"></a></h3><p>Access ia available for faculty, students (master's projects under faculty supervision), and researchers of the AUHA. The cluster is integrated in the VSC network and runs the standard VSC software setup. It is also available to all VSC-users, though we appreciate that you contact the UAntwerpen support team so that we know why you want to use the cluster.
</p><p>Jobs can have a maximal execution wall time of 3 days (72 hours).
</p><h3>Hardware details</h3><ul>
	<li>4 login nodes accessible through the generic name <code>login.hpc.uantwerpen.be</code>.
	<ul>
		<li>Use this hostname if you read <i>vsc.login.node</i> in the documentation and want to connect to this login node</li>
	</ul>
	</li>
	<li>Compute nodes
	<ul>
		<li>144 (96 installed in the first round, 48 in the first expansion) nodes with 2 10-core Intel <a href=\"https://ark.intel.com/products/75277\">E5-2680v2</a> CPUs (Ivy Bridge generation) with 64 GB of RAM.</li>
		<li>24 nodes with 2 10-core Intel <a href=\"https://ark.intel.com/products/75277\">E5-2680v2</a> CPUs (Ivy Bridge generation) with 256 GB of RAM.</li>
		<li>All nodes are connected through an InfiniBand FDR10 interconnect.</li>
	</ul>
	</li>
	<li>Storage
	<ul>
		<li>Storage is provided through a 100 TB DDN SFA7700 disk array with 4 storage servers.</li>
	</ul>
	</li>
</ul><h3>Login infrastructure</h3><p>Direct login is possible to both login nodes and to the visualization node.
</p><ul>
	<li>From outside the VSC network: Use the external interface names. Currently, one needs to be on the network of UAntwerp or some associated institutions to be able to access the external interfaces. Otherwise a VPN connection is needed to the UAntwerp network.</li>
	<li>From inside the VSC network (e.g., another VSC cluster): Use the internal interface names.</li>
</ul><table>
<tbody>
<tr>
	<td>
	</td>
	<td>External interface
	</td>
	<td>Internal interface
	</td>
</tr>
<tr>
	<td>Login generic
	</td>
	<td>login.hpc.uantwerpen.be<br>login-hopper.uantwerpen.be
	</td>
	<td>
	</td>
</tr>
<tr>
	<td>Login nodes
	</td>
	<td>login1-hopper.uantwerpen.be<br>login2-hopper.uantwerpen.be<br>login3-hopper.uantwerpen.be<br>login4-hopper.uantwerpen.be
	</td>
	<td>ln01.hopper.antwerpen.vsc<br>ln02.hopper.antwerpen.vsc<br>ln03.hopper.antwerpen.vsc<br>ln04.hopper.antwerpen.vsc
	</td>
</tr>
</tbody>
</table><h3><a name=\"hopper-storage\"></a>Storage organisation</h3><p>The storage is organised according to the <a href=\"/cluster-doc/access-data-transfer/where-store-data\">VSC storage guidelines</a>.
</p><table>
<tbody>
<tr>
	<th style=\"vertical-align: top;\">Name
	</th>
	<th style=\"vertical-align: top;\">Variable
	</th>
	<th style=\"vertical-align: top;\">Type
	</th>
	<th style=\"vertical-align: top;\">Access
	</th>
	<th style=\"vertical-align: top;\">Backup
	</th>
	<th style=\"vertical-align: top;\">Default quota
	</th>
</tr>
<tr>
	<td style=\"vertical-align: top;\">/user/antwerpen/20X/vsc20XYZ
	</td>
	<td style=\"vertical-align: top;\">$VSC_HOME
	</td>
	<td style=\"vertical-align: top;\">GPFS
	</td>
	<td style=\"vertical-align: top;\">VSC
	</td>
	<td style=\"vertical-align: top;\">NO
	</td>
	<td style=\"vertical-align: top;\">3 GB
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">/data/antwerpen/20X/vsc20XYZ
	</td>
	<td style=\"vertical-align: top;\">$VSC_DATA
	</td>
	<td style=\"vertical-align: top;\">GPFS
	</td>
	<td style=\"vertical-align: top;\">VSC
	</td>
	<td style=\"vertical-align: top;\">NO
	</td>
	<td style=\"vertical-align: top;\">25 GB
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">/scratch/antwerpen/20X/vsc20XYZ
	</td>
	<td style=\"vertical-align: top;\">
		$VSC_SCRATCH<br>
		$VSC_SCRATCH_SITE
	</td>
	<td style=\"vertical-align: top;\">GPFS
	</td>
	<td style=\"vertical-align: top;\">Hopper<br>Leibniz
	</td>
	<td style=\"vertical-align: top;\">NO
	</td>
	<td style=\"vertical-align: top;\">25 GB
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">/small/antwerpen/20X/vsc20XYZ<sup>(*)</sup>
	</td>
	<td style=\"vertical-align: top;\">
	</td>
	<td style=\"vertical-align: top;\">GPFS
	</td>
	<td style=\"vertical-align: top;\">Hopper<br>Leibniz
	</td>
	<td style=\"vertical-align: top;\">NO
	</td>
	<td style=\"vertical-align: top;\">0 GB
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">/tmp
	</td>
	<td style=\"vertical-align: top;\">$VSC_SCRATCH_NODE
	</td>
	<td style=\"vertical-align: top;\">ext4
	</td>
	<td style=\"vertical-align: top;\">Node
	</td>
	<td style=\"vertical-align: top;\">NO
	</td>
	<td style=\"vertical-align: top;\">250 GB hopper
	</td>
</tr>
</tbody>
</table><p><sup>(*)</sup> /small is a file system optimised for the storage of small files of types that do not belong in $VSC_HOME. The file systems pointed at by $VSC_DATA and $VSC_SCRATCH have a large fragment size (128 kB) for optimal performance on larger files and since each file occupies at least one fragment, small files waste a lot of space on those file systems. The file system is available on request.
</p><p>For users from other universities, the quota on $VSC_HOME and $VSC_DATA will be determined by the local policy of your home institution as these file systems are mounted from there. The pathnames will be similar with trivial modifications based on your home institution and VSC account number.
</p><h3>Characteristics of the compute nodes</h3><p>Since hopper is currently a homogenous system with respect to CPU type and interconnect, it is not needed to specify these properties (see also the page on <a href=\"/cluster-doc/running-jobs/specifying-requirements\">specifying resources, output files and notifications</a>).
</p><p>However, to make it possible to write job scripts that can be used on both hopper and turing (or other VSC clusters) and to prepare for future extensions of the cluster, the following features are defined:
</p><table>
<tbody>
<tr>
	<th>property
	</th>
	<th>explanation
	</th>
</tr>
<tr>
	<td style=\"vertical-align: top;\">ivybridge
	</td>
	<td style=\"vertical-align: top;\">only use Intel processors from the Ivy Bridge family (E5-XXXv2) (Not needed at the moment as this is the only CPU type)
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">ib
	</td>
	<td style=\"vertical-align: top;\">use InfiniBand interconnect (only for compatibility with Turing job scripts as all nodes have InfiniBand)
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">mem64
	</td>
	<td style=\"vertical-align: top;\">use nodes with 64 GB RAM (58 GB available)
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">mem256
	</td>
	<td style=\"vertical-align: top;\">use nodes with 256 GB RAM (250 GB available)
	</td>
</tr>
</tbody>
</table><p>These characteristics map to the following nodes on Hopper:
</p><table class=\"plain\">
<tbody>
<tr>
	<th>Type of node
	</th>
	<th>CPU type
	</th>
	<th>Interconnect
	</th>
	<th># nodes
	</th>
	<th># physical<br>cores<br>(per node)
	</th>
	<th># logical<br>cores (per node)
	</th>
	<th>installed mem<br>(per node)
	</th>
	<th>avail mem<br>(per node)
	</th>
	<th>local disc
	</th>
</tr>
<tr>
	<td style=\"vertical-align: top;\">ivybridge:ib:mem64
	</td>
	<td style=\"vertical-align: top;\"><a href=\"https://ark.intel.com/products/75277\">Xeon E5-2680v2</a>
	</td>
	<td style=\"vertical-align: top;\">IB-FDR10
	</td>
	<td style=\"vertical-align: top;\">144
	</td>
	<td style=\"vertical-align: top;\">20
	</td>
	<td style=\"vertical-align: top;\">20
	</td>
	<td style=\"vertical-align: top;\">64 GB
	</td>
	<td style=\"vertical-align: top;\">56 GB
	</td>
	<td style=\"vertical-align: top;\">~360 GB
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">ivybridge:ib:mem256
	</td>
	<td style=\"vertical-align: top;\"><a href=\"https://ark.intel.com/products/75277\">Xeon E5-2680v2</a>
	</td>
	<td style=\"vertical-align: top;\">IB-FDR10
	</td>
	<td style=\"vertical-align: top;\">24
	</td>
	<td style=\"vertical-align: top;\">20
	</td>
	<td style=\"vertical-align: top;\">20
	</td>
	<td style=\"vertical-align: top;\">256 GB
	</td>
	<td style=\"vertical-align: top;\">248 GB
	</td>
	<td style=\"vertical-align: top;\">~360 GB
	</td>
</tr>
</tbody>
</table><h2><a name=\"turing\"></a>Turing</h2><p>In July 2009, the UAntwerpen bought a 768 core cluster (L5420 CPUs, 16 GB RAM/node) from HP, that was installed and configured in December 2009. In December 2010, the cluster was extended with 768 cores (L5640 CPUs, 24 GB RAM/node). In September 2011, another 96 cores (L5640 CPUs, 24 GB RAM/node) have been added. Turing has been retired in January 2017.
</p>"
307,"","<h2>Hardware details</h2>
<ul>
	<li>The cluster <strong>login nodes</strong>:
	<ul>
		<li>login.hpc.kuleuven.be and login2.hpc.kuleuven.be (use this hostname if you read <em>vsc.login.node</em> in the documentation and want to connect to this login node).</li>
		<li>two GUI login nodes through NX server.</li>
	</ul>
	</li>
	<li><strong>Compute nodes</strong>:
	<ul>
		<li>
		<strong>Thin node section</strong>:
		<ul>
			<li>
			208 nodes with two 10-core \"Ivy Bridge\" Xeon E5-2680v2 CPUs (2.8 GHz, 25 MB level 3 cache). 176 of those nodes have 64 GB RAM while 32 are equiped with 128 GB RAM. The nodes are linked to a QDR Infiniband network. All nodes have a small local disk, mostly for swapping and the OS image.
			</li>
			<li>
			144 nodes with two 12-core \"Haswell\" Xeon E5-2680v3 CPUs (2.5 GHz, 30 MB level 3 cache). 48 of those nodes have with 64 GB RAM while 96 are equiped with 128 GB RAM. These nodes are linked to a FDR Infiniband network which offers lower latency and higher bandwidth than QDR.
			</li>
		</ul>
		The total memory capacity of this section is 30 TB, the total peak performance is about 232 Tflops in double precision arithmetic.
		</li>
		<li>
		<strong><a id=\"Cerebro\" name=\"Cerebro\"></a>SMP section</strong> (also known as Cerebro): a SGI UV2000 system with 64 sockets with a 10-core \"Ivy Bridge\" Xeon E5-4650 CPU (2.4 GHz, 25 MB level 3 cache), spread over 32 blades and connected through a SGI-proprietary NUMAlink6 interconnect. The interconnect also offers support for global address spaces across shared memory partitions and offload of some MPI functions. 16 sockets have 128 GB RAM and 48 sockets have 256 GB RAM, for a total RAM capacity of 14 TB. The peak compute performance is 12.3 Tflops in double precision arithmetic. The SMP system also contains a fast 21.8 GB disk storage system for swapping and temporary files.
        The system is partitioned in 2 shared memory partitions. 1 partition has 480 cores and 12 TB and 1 partition with 160 cores and 2TB. Both partitions have 10TB local scratch space.
		<br>
		However, should the need arise it can be reconfigured into a single large 64-socket shared memory machine.More information can be found in the  <a target=\"_blank\" href=\"https://www.vscentrum.be/assets/965\">cerebro quick start guide</a> or <a target=\"_blank\" href=\"https://www.vscentrum.be/assets/947\">slides from the info-session.</a></li>
		<li><strong>Accelerator section:</strong>
		<ul>
			<li>5 nodes with two 10-core \"Haswell\" Xeon E5-2650v3 2.3GHz CPUs, 64 GB of RAM 
and 2 GPUs Tesla K40 (2880 CUDA cores @ Boost clocks 810 MHz and 875 MHz, 1.66 DP Tflops/GPU Boost Clocks).
			</li>
		</ul>
		<ul>
			<li> The central GPU and Xeon Phi system is also integrated in the cluster and available to other sites. Each node has two six-core Intel Xeon E5-2630 CPUs, 64 GB RAM and a
 local hard disk. All nodes are on a QDR Infiniband interconnect. This system consists of:
			</li>
			<li>8 nodes have two nVidia K20x cards each installed. Each K20x has 14
 SMX processors (Kepler family; total of 2688 CUDA cores) that run at 
732MHz and 6 GB of GDDR5 memory with a peak memory bandwidth of 250 GB/s
 (384-bit interface @ 2.6 GHz). The peak floating point performance per 
card is 1.31 Tflops in double and 3.95 Tflops in single precision.
			</li>
			<li>8 nodes have two Intel Xeon Phi 5110P cards each installed. Each 
Xeon Phi board has 60 cores running at 1.053 GHz (of which one is 
reserved for the card OS and 59 are available for applications). Each 
core supports a large subset of the 64-bit Intel architecture 
instructions and a vector extension with 512-bit vector instructions. 
Each board contains 8 GB of RAM, distributed across 16 memory channels,
 with a peak memory bandwidth of 320 GB/s. The peak performance (not 
counting the core reserved for the OS) is 0.994 Tflops in double 
precision and 1.988 Tflops in single precision. The Xeon Phi system is not yet fully operational. MPI applications spanning multiple nodes cannot be used at the moment.
			</li>
			<li>20 nodes have four Nvidia Tesla P100 SXM2 cards each installed (3584 CUDA cores @1328 MHz, 5.3 DP Tflops/GPU).</li>
			<li>To start working with accelerators please refer to <a href=\"https://www.vscentrum.be/infrastructure/hardware/hardware-kul/accelerators\">access webpage</a>.</li>
		</ul>
		<ul>
		</ul></li>
	</ul>
	</li>
	<li><strong>Visualization nodes</strong>: 2 nodes with two 10-core \"Haswell\" Xeon E5-2650v3 2.3GHz CPUs, 2 times 64 GB of RAM 
and 2 GPUs NVIDIA
Quadro
K5200 (2304 CUDA cores @ 667 MHz). To start working on visualization nodes, we refer to the 
	<a target=\"_blank\" href=\"/client/multiplatform/turbovnc\">TurboVNC start guide</a>.</li>
	<li><strong>Central storage</strong> available to all nodes:
	<ul>
		<li>A NetApp NAS system with 30 TB of storage, used for the home- and permanent data directories. All data is mirrored almost instantaneously to the KU Leuven disaster recovery data centre.</li>
		<li>A 284 TB GPFS parallel filesystem from DDN, mostly used for temporary disk space.</li>
		<li>A 600 TB archive storage  optimised for capacity and aimed at long-term storage of very infrequently accessed data. To start using the archive storage, we refer to the 
		<a target=\"_blank\" href=\"https://www.vscentrum.be/infrastructure/hardware/wos-storage\">WOS Storage quick start guide</a>.</li>
	</ul>
	<ul>
	</ul>
	</li>
	<li>For administrative purposes, there are also <strong>service nodes</strong> that are not user-accessible</li>
</ul>
<p style=\"text-align: center;\"><img src=\"/assets/1335\" style=\"width: 1119px; height: 460px;\" width=\"1119\" height=\"560\">
</p>
<h2>Characteristics of the compute nodes</h2>
<p>The following properties allow you to select the appropriate node type for your job (see also the page on <a href=\"/cluster-doc/running-jobs/specifying-requirements\">specifying resources, output files and notifications</a>):
</p>
<table class=\"plain\">
<tbody>
<tr>
	<th>Cluster
	</th>
	<th>Type of node
	</th>
	<th>CPU type
	</th>
	<th>Interconnect
	</th>
	<th># cores
	</th>
	<th>installed mem
	</th>
	<th>avail mem
	</th>
	<th>local discs
	</th>
	<th># nodes
	</th>
</tr>
<tr>
	<td style=\"vertical-align: top;\">Thinking
	</td>
	<td style=\"vertical-align: top;\">ivybridge
	</td>
	<td style=\"vertical-align: top;\">Xeon E5-2680v2
	</td>
	<td style=\"vertical-align: top;\">IB-QDR
	</td>
	<td style=\"vertical-align: top;\">20
	</td>
	<td style=\"vertical-align: top;\">64 GB
	</td>
	<td style=\"vertical-align: top;\">60 GB
	</td>
	<td style=\"vertical-align: top;\">250 GB
	</td>
	<td style=\"vertical-align: top;\">176
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">ThinKing
	</td>
	<td style=\"vertical-align: top;\">ivybridge
	</td>
	<td style=\"vertical-align: top;\">Xeon E5-2680v2
	</td>
	<td style=\"vertical-align: top;\">IB-QDR
	</td>
	<td style=\"vertical-align: top;\">20
	</td>
	<td style=\"vertical-align: top;\">128 GB
	</td>
	<td style=\"vertical-align: top;\">124 GB
	</td>
	<td style=\"vertical-align: top;\">250 GB
	</td>
	<td style=\"vertical-align: top;\">32
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">Thinking
	</td>
	<td style=\"vertical-align: top;\">haswell
	</td>
	<td style=\"vertical-align: top;\">Xeon E5-2680v3
	</td>
	<td style=\"vertical-align: top;\">IB-FDR
	</td>
	<td style=\"vertical-align: top;\">24
	</td>
	<td style=\"vertical-align: top;\">64 GB
	</td>
	<td style=\"vertical-align: top;\">60 GB
	</td>
	<td style=\"vertical-align: top;\">150 GB
	</td>
	<td style=\"vertical-align: top;\">48<br>
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">Thinking
	</td>
	<td style=\"vertical-align: top;\">haswell
	</td>
	<td style=\"vertical-align: top;\">Xeon E5-2680v3
	</td>
	<td style=\"vertical-align: top;\">IB-FDR
	</td>
	<td style=\"vertical-align: top;\">24
	</td>
	<td style=\"vertical-align: top;\">128 GB
	</td>
	<td style=\"vertical-align: top;\">124 GB
	</td>
	<td style=\"vertical-align: top;\">150 GB
	</td>
	<td style=\"vertical-align: top;\">96<br>
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">Genius<br>
	</td>
	<td style=\"vertical-align: top;\">skylake
	</td>
	<td style=\"vertical-align: top;\">Xeon 6140
	</td>
	<td style=\"vertical-align: top;\">IB-EDR
	</td>
	<td style=\"vertical-align: top;\">36
	</td>
	<td style=\"vertical-align: top;\">192 GB
	</td>
	<td style=\"vertical-align: top;\">188 GB
	</td>
	<td style=\"vertical-align: top;\">800 GB
	</td>
	<td style=\"vertical-align: top;\">86<br>
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">Genius
	</td>
	<td style=\"vertical-align: top;\">skylake-large memory
	</td>
	<td style=\"vertical-align: top;\">Xeon 6140
	</td>
	<td style=\"vertical-align: top;\">IB-EDR
	</td>
	<td style=\"vertical-align: top;\">36
	</td>
	<td style=\"vertical-align: top;\">768 GB
	</td>
	<td style=\"vertical-align: top;\">764 GB
	</td>
	<td style=\"vertical-align: top;\">800 GB
	</td>
	<td style=\"vertical-align: top;\">10<br>
	</td>
</tr>
<tr>
	<td style=\"vertical-align: top;\">Genius
	</td>
	<td style=\"vertical-align: top;\">skylake-GPU
	</td>
	<td style=\"vertical-align: top;\">Xeon 6140<br>4xP100 SXM2<br>
	</td>
	<td style=\"vertical-align: top;\">IB-EDR
	</td>
	<td style=\"vertical-align: top;\">36
	</td>
	<td style=\"vertical-align: top;\">192 GB
	</td>
	<td style=\"vertical-align: top;\">188 GB
	</td>
	<td style=\"vertical-align: top;\">800 GB
	</td>
	<td style=\"vertical-align: top;\">20<br>
	</td>
</tr>
</tbody>
</table>
<p>For using Cerebro, the shared memory section, we refer to the <a href=\"/assets/965\">Cerebro Quick Start Guide</a>.
</p>
<h2>Implementation of the VSC directory structure</h2>
<p>In the transition phase between Vic3 and ThinKing, the storage is mounted on both systems. When switching from Vic3 to ThinKing you will not need to migrate your data.
</p>
<p>The cluster uses the directory structure that is implemented on most VSC clusters. This implies that each user has two personal directories:
</p>
<ul>
	<li>A regular home directory which contains all files that a user might need to log on to the system, and small 'utility' scripts/programs/source code/.... The capacity that can be used is restricted by quota and this directory should not be used for I/O intensive programs. <br>
	For KU Leuven systems the full path is of the form /user/leuven/... , but this might be different on other VSC systems. However, on all systems, the environment variable VSC_HOME points to this directory (just as the standard HOME variable does).</li>
	<li>A data directory which can be used to store programs and their results. At the moment, there are no quota on this directory. For KU Leuven the path name is /data/leuven/... . On all VSC systems, the environment variable VSC_DATA points to this directory.</li>
</ul>
<p>There are three further environment variables that point to other directories that can be used:
</p>
<ul>
	<li>On each cluster you have access to a scratch directory that is shared by all nodes on the cluster. The variable VSC_SCRATCH_SITE will point to this directory. This directory is also accessible from the loginnodes, so it is accessible while your jobs run, and after they finish (for a limited time: files can be removed automatically after 14 days.)</li>
	<li>Similarly, on each cluster you have a VSC_SCRATCH_NODE directory, which is a scratch space local to each computenode. Thus, on each node, this directory point to a different physical location, and the connects are only accessible from that particular worknode, and (typically) only during the runtime of your job. But, if more than one job of you runs on the same node, they all see the same directory (and thus you have to make sure they do not overwrite each others data by creating subdirectories per job, or give proper filename, ...)</li>
</ul>
<h2></h2>
<h2>Access restrictions<a id=\"AccessRestrictions\" name=\"AccessRestrictions\"></a></h2>
<p>Access
 is available for faculty, students (under faculty supervision), and 
researchers of the KU  Leuven, UHasselt and their associations. This 
cluster is being integrated in the VSC network and as such becomes 
available to all VSC users.
</p>
<h2>History</h2>
<p>In September 2013 a new thin node cluster (HP) and a shared memory system (SGI) was bought. The thin node cluster was installed and configured in January/February 2014 and extended in september 2014. Installation and configuration of the SMP is done in April 2014. Financing of this systems was obtained from the Hercules foundation and the Flemish government.
</p>
<p>Do you want to see it ? Have a look at the movie
</p>
<p>
	<iframe allowfullscreen=\"\" src=\"//www.youtube.com/embed/O4jOzReDYnc?rel=0\" width=\"640\" height=\"360\" frameborder=\"0\">
	</iframe>
</p>"
309,"","<h2>Overview</h2><p>The tier-1 cluster <em>muk</em> is primarily aimed at large parallel computing jobs that require a high-bandwidth low-latency interconnect, but jobs that require a multitude of small independent tasks are also accepted.
</p><p>The main architectural features are:
</p><ul>
	<li>528 compute nodes with two Xeon E5-2670 processors (2,6GHz, 8 cores per processor, Sandy Bridge architecture) and 64GiB of memory, for a total memory capacity of 33 TiB and a peak performance of more than 175 TFlops (Linpack result 152,3 TFlops)</li>
	<li>FDR Infiniband interconnect with a fat tree topology (1:2 oversubscription)</li>
	<li>A storage system with a net capacity of approximately 400TB and a peak bandwidth of 9.5 GB/s.</li>
</ul><p>The cluster appeared for several years in the To<span style=\"line-height: 1.5em;\">p500 list of supercomputer sites:</span>
</p><table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width: 600px;\">
<tbody>
<tr>
	<td>
	</td>
	<td>June 2012
	</td>
	<td>Nov 2012
	</td>
	<td>June 2013
	</td>
	<td>Nov 2013
	</td>
	<td>June 2014
	</td>
</tr>
<tr>
	<td>Ranking
	</td>
	<td>118
	</td>
	<td>163
	</td>
	<td>239
	</td>
	<td>306
	</td>
	<td>430
	</td>
</tr>
</tbody>
</table><p>Compute time on <em>muk</em> is only available upon approval of a project. Information on requesting projects is available <a href=\"/nl/systemen-en-toegang/projecttoegang-tier1\">in Dutch</a> and <a href=\"/en/access-and-infrastructure/project-access-tier1\">in English</a>
</p><h2>Access restriction</h2><p>Once your project has been approved, your login on the tier-1 cluster will be enabled. You use the same vsc-account (vscXXXXX) as at your home institutions and you use the same $VSC_HOME and $VSC_DATA directories, though the tier-1 does have its own scratch directories.
</p><p>A direct login from your own computer through the public network to <em>muk</em> is not possible for security reasons. You have to enter via the VSC network, which is reachable from all Flemish university networks.
</p><pre>ssh login.hpc.uantwerpen.be
ssh login.hpc.ugent.be
ssh login.hpc.kuleuven.be or login2.hpc.kuleuven.be
</pre><p>Make sure that you have at least once connected to the login nodes of your institution, before attempting access to tier-1.
</p><p>Once on the VSC network, you can
</p><ul>
	<li>connect to <strong>login.muk.gent.vsc</strong> to work on the tier-1 cluster muk,</li>
	<li>connect to <strong>gligar01.gligar.gent.vsc</strong> or <strong>gligar02.gligar.gent.vsc</strong> for testing and debugging purposes (e.g., check if a code compiles). There you'll find the same software stack as on the tier-1. (On some machines gligar01.ugent.be and gligar02.ugent.be might also work.)</li>
</ul><p>There are two options to log on to these systems over the VSC network:
</p><ol>
	<li>You log on to your home cluster. At the command line, you start a ssh session to <em>login.muk.gent.vsc</em>.
<pre>ssh login.muk.gent.vsc</pre>
	</li>
	<li>You set up a so-called <em>ssh proxy</em> through your usual VSC login node <em>vsc.login.node</em> (the <em>proxy server</em> in this process) to <em>login.muk.gent.vsc</em> or <em>gligar01.ugent.be</em>.
	<ul>
		<li>To set up a ssh proxy using OpenSSH, the client for Linux and OS X or if you have Windows with the Cygwin emulation layer installed, follow the instructions <a href=\"/client/linux/openssh-proxy\">in the Linux client section</a>.</li>
		<li>To set up a ssh proxy on Windows using PuTTY, follow the instructions <a href=\"/client/windows/putty-proxy\">in the Windows client section</a>.</li>
	</ul>
	</li>
</ol><h2>Resource limits</h2><h3>Disk quota</h3><ul>
	<li>As you are using your $VSC_HOME and $VSC_DATA directories from your home institution, the quota policy from your home institution applies.</li>
	<li>On the shared (across nodes) scratch volume $VSC_SCRATCH the standard disk quota is 250GiB per user. If your project requires more disk space, you should request it in your project application as we have to make sure that the mix of allocated projects does not require more disk space than available.</li>
	<li>Currently, each institute has a maximal scratch quotum of 75TiB. So, please vacate as much as possible of the $VSC_SCRATCH at all times to enable large jobs.</li>
</ul><h3>Memory</h3><ul>
	<li>Each node has 64GiB of RAM. However, not all of that memory is available for user applications as some memory is needed for the operating system and file system buffers. In practise, roughly 60GiB is available to run your jobs. This also means that when using all cores, you should not request more than 3.75GiB of RAM per core (pmem resource attribute in qsub) or your job will be queued indefinitely since the resource manager will not be able to assign nodes to it.</li>
	<li>The maximum amount of total virtual memory per node ('vmem') you can request is 83GiB, see also the output of the <code>pbsmon</code> command. The job submit filter sets a default virtual memory limit if you don't specify something with your job using e.g.
	<pre>#PBS -l vmem=83gb</pre>
	</li>
</ul>"
311,"","<h2>Access</h2><pre>qsub  -l partition=gpu,nodes=1:K20Xm &lt;jobscript&gt;
</pre><p>or
</p><pre>qsub  -l partition=gpu,nodes=1:K40c &lt;jobscript&gt;
</pre><p>depending which GPU node you would like to use if you don't 'care' on which type of GPU node your job ends up you can just submit it like this:
</p><pre>qsub  -l partition=gpu &lt;jobscript&gt;
</pre><h3>Submit to a Phi node:</h3><pre>qsub -l partition=phi &lt;jobscript&gt;
</pre>"
313,"","<h2>Tier-1</h2><ul>
<li>Our <a href=\"/infrastructure/hardware/hardware-tier1-breniac\">current Tier-1 system is BrENIAC</a>, operated by KU Leuven. The system is aimed at large parallel computing jobs that require a high-bandwidth low-latency interconnect. Compute time is again only available upon approval of a project. See the <a href=\"https://www.vscentrum.be/en/access-and-infrastructure/project-access-tier1\">page on Tier-1 project access and links in that page</a>.</li>
	<li>
	Our <a href=\"/infrastructure/hardware/hardware-tier1-muk\">first Tier-1 system is muk</a>, was operated by UGent but is no longer in production.</li>
</ul><h2>Experimental setup</h2><ul>
	<li>
	<a href=\"/infrastructure/hardware/k20x-phi-hardware\">There is a small GPU and Xeon Phi test system</a> which is can be used by all VSC members on request (though a project approval is not required at the moment).
	<a href=\"/infrastructure/hardware/k20x-phi-hardware\">The documentation for this system is under development</a>.
	</li>
</ul><h2>Tier-2</h2><p>Four university-level cluster groups are also embedded in the VSC and partly funded from VSC budgets:
</p><ul>
	<li><a href=\"/infrastructure/hardware/hardware-ua\">The UAntwerpen clusters (hopper and leibniz)</a></li>
	<li><a href=\"/infrastructure/hardware/hardware-vub\">The VUB cluster (hydra)</a></li>
	<li><a href=\"/infrastructure/hardware/hardware-ugent\">The UGent local clusters</a></li>
	<li><a href=\"/infrastructure/hardware/hardware-kul\">The KU Leuven/UHasselt cluster (ThinKing and Cerebro)</a></li>
</ul>"
315,"","<h2>The icons</h2><table>
<tbody>
<tr>
	<td><img src=\"/assets/921\" alt=\"Windows\">
	</td>
	<td>Works on Windows, but may need additional pure Windows packages (free or commercial)
	</td>
</tr>
<tr>
	<td><img src=\"/assets/923\" alt=\"Windows+\">
	</td>
	<td>Works on Windows with a <a href=\"#UNIX\">UNIX compatibility layer</a> added, e.g., cygwin or the \"Windows Subsystem for Linux\" in Windows 10 build 1607 (anniversary edition) or later
	</td>
</tr>
</tbody>
</table><h2>Getting ready to request an account</h2><ul>
	<li>Before requesting an account, you need to generate a pair of ssh keys. One popular way to do this on Windows is<a href=\"/client/windows/keys-putty\"> using the freely available PuTTY client</a> which you can then also use to log on to the clusters.</li>
</ul><h2><a name=\"connecting\"></a>Connecting to the cluster</h2><ul>
	<li>Open a text-mode session using an ssh client
	<ul>
		<li><a href=\"/client/windows/console-putty\">PuTTY</a> is a simple-to-use and freely available GUI SSH client for Windows.</li>
		<li><a href=\"/client/windows/using-pageant\">pageant</a> can be used to manage active keys for PuTTY, WinSCP and FileZilla so that you don't need to enter the passphrase all the time.</li>
		<li><a href=\"/client/windows/putty-proxy\">Setting up a SSH proxy with PuTTY</a> to log on to a node protected by a firewall through another login node, e.g., to access the tier-1 system muk.</li>
		<li><a href=\"/client/windows/creating-an-ssh-tunnel\">Creating a SSH tunnel using PuTTY</a> to establish network communication between your local machine and the cluster otherwise blocked by firewalls.</li>
	</ul>
	</li>
	<li><a id=\"data-transfer\" name=\"data-transfer\"></a>Transfer data using Secure FTP (SFTP) clients:
	<ul>
		<li><a href=\"/client/windows/filezilla\">FileZilla</a></li>
		<li><a href=\"/client/windows/winscp\">WinSCP</a></li>
	</ul>
	</li>
	<li><a id=\"X-programs\" name=\"X-programs\"></a>Display graphical programs:
	<ul>
		<li>You can install a so-called X server: <a href=\"/client/windows/xming\">Xming</a>. X is the protocol that is used by most Linux applications to display graphics on a local or remote screen.</li>
		<li>On the KU Leuven/UHasselt clusters it is also possible to <a href=\"/client/multiplatform/nx-start-guide\">use the NX Client</a> to log on to the machine and run graphical programs. Instead of an X-server, another piece of client software is needed. That software is currently available for Windows, OS X, Linux, Android and iOS. </li>
		<li>The KU Leuven/UHasselt and UAntwerp clusters also offer support for visualization software through TurboVNC. VNC renders images on the cluster and transfers the resulting images to your client device. VNC clients are available for Windows, macOS, Linux, Android and iOS.
		<ul>
			<li>On the KU Leuven/UHasselt clusters, <a href=\"/client/multiplatform/turbovnc\">TurboVNC is supported on the visualization nodes</a>.</li>
			<li>On the UAntwerp clusters, TurboVNC is supported on all regular login nodes (without OpenGL support) and on the visualization node of Leibniz (with OpenGL support through VirtualGL). See the page \"<a href=\"/infrastructure/hardware/hardware-ua/visualization\">Remote visualization @ UAntwerp</a>\" for instructions.</li>
		</ul></li>
	</ul></li>
	<li>If you install the free <a href=\"http://www.cygwin.com/\" target=\"_blank\">UNIX emulation layer Cygwin</a> with the necessary packages, you can use the same OpenSSH client as on Linux systems and all pages about ssh and data transfer from <a href=\"/client/linux\">the Linux client pages</a> apply.</li>
</ul><h2>Programming tools</h2><ul>
	<li><a name=\"UNIX\"></a>By installing the <a href=\"https://www.cygwin.com/\" target=\"_blank\">UNIX emulation layer Cygwin</a> with the appropriate packages you can mimic very well the VSC cluster environment (at least with the foss toolchain). Cygwin supports the GNU compilers and also contains packages for OpenMPI (<a href=\"https://cygwin.com/cgi-bin2/package-grep.cgi?grep=openmpi&arch=x86_64\" target=\"_blank\">look for \"openmpi\"</a>) and some other popular libraries (FFTW, HDF5, ...). As such it can turn your Windows PC in a computer that can be used to develop software for the cluster if you don't rely on too many external libraries (which may be hard to install). This can come in handy if you sometimes need to work off-line. If you have a 64-bit Windows system (which most recent computers have), it is best to go for the 64-bit version of Cygwin. After all, the VSC-clusters are also running a 64-bit OS.</li>
	<li>If you're running Windows 10 build 1607 (Anniversary Edition) or later, you may consider running the \"<a href=\"https://www.google.be/webhp?q=windows%20subsystem%20for%20linux\">Windows Subsystem for Linux</a>\" that will give you a Ubuntu-like environment on Windows and allow you to install some Ubuntu packages. <em>In build 1607 this is still considered experimental technology and we offer no support.</em></li>
	<li><a href=\"/client/windows/microsoft-visual-studio\">Microsoft Visual Studio</a> can also be used to develop OpenMP or MPI programs. If you do not use any Microsoft-specific libraries but stick to plain C or C++, the programs can be recompiled on the VSC clusters. Microsoft is slow in implementing new standards though. In Visual Studio 2015, OpenMP support is still stuck at version 2.0 of the standard. An alternative is to get a license for the Intel compilers which plug into Visual Studio and give you the best of both worlds, the power of a full-blown IDE and compilers that support the latest technologies in the HPC world on Windows.</li>
	<li>Eclipse is a popular multi-platform Integrated Development Environment (IDE) very well suited for code development on clusters.
	<ul>
		<li>Read our <a href=\"/client/multiplatform/eclipse-intro\">Eclipse introduction</a> to find out why you should consider using Eclipse if you develop code and how to get it.</li>
		<li>You can use <a href=\"/client/multiplatform/eclipse-remote-editor\">Eclipse on the desktop as a remote editor for the cluster</a>.</li>
		<li>You can use <a href=\"/client/multiplatform/eclipse-vsc-subversion\">Eclipse on the desktop to access files in a subversion repository on the cluster</a>.</li>
		<li>You can combine the remote editor feature with version control from Eclipse, but some care is needed, and <a href=\"/client/multiplatform/eclipse-ptp-versioncontrol\">here's how to do it</a>.</li>
	</ul>
	On Windows Eclipse relies by default on the cygwin toolchain for its compilers and other utilities, so you need to install that too.</li>
	<li>Information on tools for version control (git and subversion) is available on the <a href=\"/cluster-doc/development/version-control\">\"Version control systems\" introduction page</a> on this web site.
	<ul>
	</ul>
	<ul>
		<li>Information about <a href=\"/client/windows/tortoisesvn\">using the TortoiseSVN Subversion client with the VSC systems</a>.</li>
	</ul></li>
</ul>"
317,"","<h2>Prerequisite: PuTTY and WinSCP</h2><p>You've <a href=\"/client/windows/keys-putty\">generated a public/private key pair with PuTTY</a> and have an approved account on the VSC clusters.
</p><h2>Connecting to the VSC clusters</h2><p>When you start the PuTTY executable 'putty.exe', a configuration screen pops up.  Follow the steps below to setup the connection to (one of) the VSC clusters.
</p><p>In the screenshots, we show the setup for user vsc98765 to the ThinKing cluster at K.U.Leuven via the loginnode login.hpc.kuleuven.be.
</p><p>You can find the names and ip-addresses of the loginnodes in the sections of <a href=\"/infrastructure/hardware\">the local VSC clusters</a>.
</p><p>Alternatively, you can follow a <a target=\"_blank\" href=\"https://www.vscentrum.be/assets/1191\">short video</a>
 explaining step-by-step the process of making connection to VSC login nodes (example based on KU Leuven cluster).<br></p><ol>
	<li>
	Within the category Session, in the field 'Host Name', type in &lt;vsc-loginnode&gt;, which is the name of the loginnode of the VSC cluster you want to connect to.<br>
	<img src=\"/assets/127\" alt=\"PuTTY Session\" width=\"465\"></li>
	<li>In the category Connection &gt; Data, in the field 'Auto-login username', put in &lt;vsc-account&gt;, which is your VSC username that you have received by mail after your request was approved.</li>
	<li>
	In the category Connection &gt; SSH &gt; Auth, click on 'Browse' and select the private key that you generated and saved above.<br>
	<img src=\"/assets/129\" alt=\"PuTTY SSH Auth\"><br>
	Here, the private key was previously saved in the folder C:\\Documents and Settings\\Me\\Keys.  In newer versions of Windows, \"C:\\Users\" is used instead \"C:\\Documents and Settings\".
	</li>
	<li>
	In the category Connection &gt; SSH &gt; X11, click the Enable X11 Forwarding checkbox:<br>
	<img src=\"/assets/131\" alt=\"PuTTY SSH Auth\"></li>
	<li>Now go back to Session, and fill in a name in the 'Saved Sessions' field and press 'Save' to store the session information.</li>
	<li>Now pressing 'Open' should start ask for you passphrase, and connect you to &lt;vsc-loginnode&gt;.</li>
</ol><p>The first time you make a connection to the loginnode, a Security Alert will appear and you will be asked to verify the authenticity of the loginnode.
</p><p><img src=\"/assets/133\" alt=\"PuTTY Security Alert\">
</p><p>For future sessions, just select your saved session from the list and press 'Open'.
</p>"
319,"","<h2>Getting started with Pageant</h2><p><a href=\"https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html\" target=\"_blank\">Pageant</a> is an SSH authentication agent that you can use for Putty and Filezilla. Before you run Pageant, you need to have a private key in PKK format (filename ends on <code>.pkk</code>). See <a href=\"/client/windows/keys-putty\">our page on generating keys with PuTTY</a> to find out how to generate and use one. When you run Pageant, it will put an icon of a computer wearing a hat into the System tray. It will then sit and do nothing, until you load a private key into it. If you click the Pageant icon with the right mouse button, you will see a menu. Select ‘View Keys’ from this menu. The Pageant main window will appear. (You can also bring this window up by double-clicking on the Pageant icon.) The Pageant window contains a list box. This shows the private keys Pageant is holding. When you start Pageant, it has no keys, so the list box will be empty. After you add one or more keys, they will show up in the list box.
</p><p>
	To add a key to Pageant, press the ‘Add Key’ button. Pageant will bring up a file dialog, labelled ‘Select Private Key File’. Find your private key file in this dialog, and press ‘Open’. Pageant will now load the private key. If the key is protected by a passphrase, Pageant will ask you to type the passphrase. When the key has been loaded, it will appear in the list in the Pageant window.
</p><p>
	Now start PuTTY (or Filezilla) and open an SSH session to a site that accepts your key. PuTTY (or Filezilla) will notice that Pageant is running, retrieve the key automatically from Pageant, and use it to authenticate. You can now open as many PuTTY sessions as you like without having to type your passphrase again.
</p><p>
	When you want to shut down Pageant, click the right button on the Pageant icon in the System tray, and select ‘Exit’ from the menu. Closing the Pageant main window does <em>not</em> shut down Pageant.
</p><p>
	You can find more info <a href=\"http://the.earth.li/~sgtatham/putty/0.63/htmldoc/Chapter9.html\" target=\"_blank\">in the on-line manual</a>.
</p><p><em>SSH authentication agents are very handy as you no longer need to type your passphrase every time that you try to log in to the cluster. It also implies that when someone gains access to your computer, he also automatically gains access to your account on the cluster. So be very careful and lock your screen when you're not with your computer! It is your responsibility to keep your computer safe and prevent easy intrusion of your VSC-account due to an obviously unprotected PC!</em><br>
</p>"
321,"","<h2>
Rationale</h2><p>
	ssh provides a safe way of connecting to a computer, encrypting traffic and avoiding passing passwords across public networks where your traffic might be intercepted by someone else. Yet making a server accessible from all over the world makes that server very vulnerable. Therefore servers are often put behind a <em>firewall</em>, another computer or device that filters traffic coming from the internet.
</p><p>
	In the VSC, all clusters are behind a firewall, but for the tier-1 cluster muk this firewall is a bit more restrictive than for other clusters. Muk can only be approached from certain other computers in the VSC network, and only via the internal VSC network and not from the public network. To avoid having to log on twice, first to another login node in the VSC network and then from there on to Muk, one can set up a so-called <em>ssh proxy</em>. You then connect through another computer (the <em>proxy server</em>) to the computer that you really want to connect to.
</p><p>
	This all sounds quite complicated, but once things are configured properly it is really simple to log on to the host.
</p><h2>
Setting up a proxy in PuTTY</h2><p>
	Setting up the connection in PuTTY is a bit more complicated than for a simple direct connection to a login node.
</p><ol class=\"list--ordered\">
	<li>
	First you need to start up pageant and load your private key into it. <a href=\"/client/windows/using-pageant\">See the instructions on our \"Using Pageant\" page</a>.</li>
	<li>
	In PuTTY, go first to the \"Proxy\" category (under \"Connection\"). In the Proxy tab sheet, you need to fill in the following information:<br>
	<table border=\"0\" cellpadding=\"1\" cellspacing=\"1\">
	<tbody>
	<tr>
		<td style=\"vertical-align: top; width: 30%;\">
			<a href=\"/assets/135\"><img src=\"/assets/135\" alt=\"\" title=\"PuttyProxy step1 tabProxy\" style=\"width: 100%;\"></a>
		</td>
		<td style=\"vertical-align: top;\">
			<ol>
				<li>
				Select the proxy type: \"Local\"</li>
				<li>
				Give the name of the \"proxy server\". This is <em>vsc.login.node</em>, your usual VSC login node, and not the computer on which you want to log on and work.</li>
				<li>
				Make sure that the \"Port\" number is 22.</li>
				<li>
				Enter your VSC-id in the \"Username\" field.</li>
				<li>
				In the \"Telnet command, or local proxy command\", enter the string<br>
				<pre>plink -agent -l %user %proxyhost -nc %host:%port
				</pre>
				(the easiest is to just copy-and-paste this text).<br>
				<em>\"plink\" (PuTTY Link) is a Windows program and comes with the full PuTTY suite of applications. It is the command line version of PuTTY. In case you've only installed the executables putty.exe and pageant.exe, you'll need to download plink.exe also from <a href=\"https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html\" target=\"_blank\">the PuTTY web site</a>. We strongly advise to simply install the whole PuTTY-suite of applications using the installer provided <a href=\"https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html\" target=\"_blank\">on that site</a>.</em></li>
			</ol>
		</td>
	</tr>
	</tbody>
	</table>
	</li>
	<li>
	Now go to the \"Data\" category in PuTTY, again under \"Connection\".<br>
	<table border=\"0\" cellpadding=\"1\" cellspacing=\"1\">
	<tbody>
	<tr>
		<td style=\"vertical-align: top; width: 30%;\">
			<a href=\"/assets/137\"><img src=\"/assets/137\" alt=\"\" title=\"PuttyProxy step2 tabSession\" style=\"width: 100%;\"></a>
		</td>
		<td style=\"vertical-align: top;\">
			<ol>
				<li>
				Fill in your VSC-id in the \"Auto-login username\" field.</li>
				<li>
				Leave the other values untouched (likely the values in the screen dump)</li>
			</ol>
		</td>
	</tr>
	</tbody>
	</table>
	</li>
	<li>
	Now go to the \"Session\" category<br>
	<table border=\"0\" cellpadding=\"1\" cellspacing=\"1\">
	<tbody>
	<tr>
		<td style=\"vertical-align: top; width: 30%\">
			<a href=\"/assets/139\"><img src=\"/assets/139\" alt=\"\" title=\"PuttyProxy step3 tabData loginname\" style=\"width: 100%;\"></a>
		</td>
		<td style=\"vertical-align: top;\">
			<ol>
				<li>
				Set the field \"Host Name (or IP address) to the computer you want to log on to. If you are setting up a proxy connection to access a computer on the VSC network, you will have to use its name on the internal VSC network. E.g., for the login nodes of the tier-1 cluster Muk at UGent, this is <strong>login.muk.gent.vsc</strong> and for the cluster on which you can test applications for the Muk, this is <strong>gligar.gligar.gent.vsc</strong>.</li>
				<li>
				Make sure that the \"Port\" number is 22.</li>
				<li>
				Finally give the configuration a name in the field \"Saved Sessions\" and press \"Save\". Then you won't have to enter all the above information again.</li>
				<li>
				And now you're all set up to go. Press the \"Open\" button on the \"Session\" tab to open a terminal window.</li>
			</ol>
		</td>
	</tr>
	</tbody>
	</table>
	</li>
</ol><h2>
For advanced users</h2><p>
	If you have an X-server on your Windows PC, you can also use X11 forwarding and run X11-applications on the host. All you need to do is click the box next to \"Enable X11 forwarding\" in the category \"Connection\" -&gt; \"SSH\"-&gt; \"X11\".
</p><p>
	What happens behind the scenes:
</p><ul>
	<li>		By specifying \"local\" as the proxy type, you tell PuTTY to not use one of its own build-in ways of setting up a proxy, but to use the command that you specify in the \"Telnet command\" of the \"Proxy\" category.</li>
	<li>		In the command<br>
	<pre>plink -agent -l %user %proxyhost -nc %host:%port
	</pre>	%user will be replaced by the userid you specify in the \"Proxy\" category screen, %proxyhost will be replaced by the host you specify in the \"Proxy\" category screen (<strong>vsc.login.node</strong> in the example), %host by the host you specified in the \"Session\" category (login.muk.gent.vsc in the example) and %port by the number you specified in the \"Port\" field of that screen (and this will typically be 22).</li>
	<li>		The plink command will then set up a connection to %proxyhost using the userid %user. The -agent option tells plink to use pageant for the credentials. And the -nc option tells plink to tell the SSH server on %proxyhost to further connect to %host:%port.</li>
</ul>"
323,"","<h2>Prerequisits</h2><p><a href=\"/client/windows/console-putty\">PuTTY</a> must be installed on your computer, and you should be able to <a href=\"/client/windows/console-putty\">connect via SSH to the cluster's login node</a>.
</p><h2>
Background</h2><p>
	Because of one or more firewalls between your desktop and the HPC clusters, it is generally impossible to communicate directly with a process on the cluster from your desktop except when the network managers have given you explicit permission (which for security reasons is not often done). One way to work around this limitation is SSH tunneling.
</p><p>
	There are several cases where this is usefull:
</p><ul>
	<li>
	Running X applications on the cluster: The X program cannot directly communicate with the X server on your local system. In this case, the tunneling is easy to set up as PuTTY will do it for you if you select the right options on the X11 settings page as explained on the <a href=\"/client/windows/console-putty\">page about text-mode access using PuTTY</a>.</li>
	<li>
	Running a server application on the cluster that a client on the desktop connects to. One example of this scenario is <a href=\"/cluster-doc/postprocessing/paraview-remote-visualization\">ParaView in remote visualization mode</a>, with the interactive client on the desktop and the data processing and image rendering on the cluster. How to set up the tunnel for that scenario is also <a href=\"/cluster-doc/postprocessing/paraview-remote-visualization\">explained on that page</a>.</li>
	<li>
	Running clients on the cluster and a server on your desktop. In this case, the source port is a port on the cluster and the destination port is on the desktop.</li>
</ul><h2>
Procedure: A tunnel from a local client to a server on the cluster</h2><ol>
	<li>
	Log in on the login node</li>
	<li>
	Start the server job, note the compute node's name the job is running on (e.g., 'r1i3n5'), as well as the port the server is listening on (e.g., '44444').</li>
	<li>
	Set up the tunnel:<br>
	<img src=\"/assets/141\" alt=\"PuTTY Reconfiguration dialog box\">
	<ol>
		<li>
		Right-click in PuTTY's title bar, and select 'Change Settings...'.</li>
		<li>
		In the 'Category' pane, expand 'Connection' -&gt; 'SSH', and select 'Tunnels' as show below:</li>
		<li>
		In the 'Source port' field, enter the local port to use (e.g., 11111).</li>
		<li>
		In the 'Destination' field, enter &lt;hostname&gt;:&lt;server-port&gt; (e.g., r1i3n5:44444 as in the example above).</li>
		<li>
		Click the 'Add' button.</li>
		<li>
		Click the 'Apply' button</li>
	</ol>
	</li>
</ol><p>
	<br>
	The tunnel is now ready to use.
</p>"
325,"","<p>FileZilla is an easy-to-use freely available ftp-style program to transfer files to and from your account on the clusters.
</p><p>You can also put FileZilla with your private key on a USB stick to access your files from any internet-connected PC.
</p><p>You can <a href=\"https://filezilla-project.org/download.php?type=client\" target=\"_blank\">download Filezilla</a> from the <a href=\"https://filezilla-project.org/\" target=\"_blank\">FileZilla project web page</a>.
</p><h2>Configuration of FileZilla to connect to a login node</h2><p>Note: Pageant should be running and your private key should be loaded first (more info on <a href=\"/client/windows/using-pageant\">our \"Using Pageant\" page</a>).
</p><ol>
	<li>Start FileZilla;</li>
	<li>Open the Site Manager using the 'File' menu;</li>
	<li>Create a new site by clicking the New Site button;</li>
	<li>In the tab marked General, enter the following values (all other fields remain blank):
	<ul>
		<li>Host: <em>vsc.login.node</em>, the name of the login node of your home institute VSC cluster</li>
		<li>Servertype: SFTP - SSH File Transfer Protocol</li>
		<li>Logontype: Normal</li>
		<li>User: <i>your own </i>VSC user ID, e.g., vsc98765;</li>
	</ul>
	</li>
	<li>Optionally, rename this setting to your liking by pressing the 'Rename' button;</li>
	<li>Press 'Connect' and enter your passphrase when requested.</li>
</ol><p style=\"text-align: center\"><img src=\"/assets/143\" alt=\"FileZilla configuration\" align=\"middle\" width=\"400\">
</p><p>Note that recent versions of FileZilla have a screen in the settings to manage private keys. The path to the private key must be provided in options (Edit Tab -&gt; options -&gt; connection -&gt; SFTP):</p><p style=\"text-align: center\"><img src=\"/assets/1195\" alt=\"FileZilla configuration\" align=\"middle\" width=\"578\">
</p><p>After that you should be able to connect after being asked for passphrase. As an alternative you can choose to use putty pageant.
</p>"
327,"","<h2>Prerequisite: WinSCP</h2><p>To transfer files to and from the cluster, we recommend the use of <a href=\"https://winscp.net/eng/docs/start\" target=\"_blank\">WinSCP</a>, which is a graphical ftp-style program (but than one that uses the ssh way of communicating with the cluster rather then the less secure ftp) that is also freely available. WinSCP can be downloaded both as an installation package and as a standalone portable executable. When using the portable version, you can copy WinSCP together with your private key on a USB stick to have access to your files from any internet-connected Windows PC.
</p><p>WinSCP also works together well with the PuTTY suite of applications. It uses the <a href=\"/client/windows/keys-putty\">keys generated with the PuTTY key generation program</a>, can <a href=\"/client/windows/console-putty\">launch terminal sessions in PuTTY</a> and <a href=\"/client/windows/using-pageant\">use ssh keys managed by pageant</a>.
</p><h2>Transferring your files to and from the VSC clusters</h2><p>The first time you make the connection, you will be asked to 'Continue connecting and add host key to the cache'; select 'Yes'.
</p><ol>
	<li>Start WinSCP and go the the \"Session\" category. Fill in the following information:
	<table border=\"0\" cellpadding=\"1\" cellspacing=\"1\">
	<tbody>
	<tr>
		<td style=\"vertical-align: top; width: 30%;\"><a href=\"/assets/145\"><img src=\"/assets/145\" alt=\"WinSCP Session parameters\" style=\"width: 100%;\"></a>
		</td>
		<td style=\"vertical-align: top;\">
			<ol>
				<li>Fill in the hostname of the VSC login node of your home institution. You can find this information in the <a href=\"/infrastructure/hardware\">overview of available hardware on this site</a>.</li>
				<li>Fill in your VSC username.</li>
				<li>If you are not using pageant to manage your ssh keys, you have to point WinSCP to the private key file (in PuTTY .ppk format) that should be used. When using pageant, you can leave this field blank.</li>
				<li>Double check that the port number is 22.</li>
			</ol>
		</td>
	</tr>
	</tbody>
	</table>
	</li>
	<li>
	If you want to store this data for later use, click the \"Save\" button at the bottom and enter a name for the session. Next time you'll start WinSCP, you'll get a screen with stored sessions that you can open by selecting them and clicking the \"Login\" button.
	</li>
	<li>
	Click the \"Login\" button to start the session that you just created. You'll be asked for your passphrase if pageant is not running with a valid key loaded.
    The first time you make the connection, you will be asked to \"Continue connecting and add host key to the cache\"; select \"Yes\".
	</li>
</ol><h2>Some remarks</h2><h3>Two interfaces</h3><p><a href=\"/assets/147\"><img src=\"/assets/147\" alt=\"\" style=\"float: right; width: 250px;\"></a>WinSCP has two modes for the graphical user interface:
</p><ul>
	<li>The \"commander mode\" where you get a window with two columns, with the local directory in the left column and the host directory (remote directory) in the right column. You can then transfer files by dragging them from one column to the other.</li>
	<li>The \"explorer mode\" where you only see the remote directory. You can transfer files by dragging them to and from other folder windows or the desktop.</li>
</ul><p>During the installation of WinSCP, you'll be prompted for a mode. But you can always change your mind afterwards and selct the interface mode by selecting the \"Preferences\" category after starting WinSCP.
</p><h3>Enable logging</h3><p>When you experience trouble transferring files using WinSCP, the support team may ask you to enable logging and mail the results.
</p><ol>
	<li>To enable logging:
	<table border=\"0\" cellpadding=\"1\" cellspacing=\"1\">
	<tbody>
	<tr>
		<td style=\"vertical-align: top; width:30%;\"><a href=\"/assets/149\"><img src=\"/assets/149\" alt=\"WinSCP logging\" style=\"width: 100%;\"></a>
		</td>
		<td style=\"vertical-align: top;\">
			<ol>
				<li>Check \"Advanced options\".</li>
				<li>Select the \"Logging\" category.</li>
				<li>Check the box next to \"Enable session logging on level\" and select the logging level requested by the user support team. Often normal loggin will be sufficient.</li>
				<li>
				Enter a name and directory for the log file. The default is \"%TEMP%\\!S.log\" which will expand to a name that is system-dependent and depends on the name of your WinSCP session. %TEMP% is a Windows environment variable pointing to a directory for temporary files which on most systems is well hidden. \"!S\" will expand to the name of your session (for a stored session the name you used there).
                    You can always change this to another directory and/or file name that is easier for you to work with.
				</li>
			</ol>
		</td>
	</tr>
	</tbody>
	</table>
	</li>
	<li>Now just run WinSCP as you would do without logging.</li>
	<li>To mail the result if you used the default log file name %TEMP%\\!S.log:
	<ol>
		<li>Start a new mail in your favourite mail program (it could even be a web mail service).</li>
		<li>Click whatever button or menu choice you need to add an attachment.</li>
		<li>
		Many mail programs will now show you a standard Windows dialog window to select the file. In many mail programs, the left top of the window will look like this (a screen capture from a Windows 7 computer):<br>
		<img src=\"/assets/151\" alt=\"WinSCP load logfile in Explorer\" style=\"width: 425px; \"><br>
		Click right of the text in the URL bar in the upper left of the window. The contents will now change to a regular Windows path name and will be selected. Just type %TEMP% and press enter and you will see that %TEMP% will expand to the name of the directory with the temporary files.
		<em>This trick may not work with all mail programs!</em>
		</li>
		<li>Finish the mail text and send the mail to user support.</li>
	</ol>
	</li>
</ol>"
329,"","<p>To display graphical applications from a Linux computer (such as the VSC clusters) on your Windows desktop, you need to install an X Window server.  Here we describe the installation of Xming, one such server and freely available.</p><h2>Installing Xming</h2><ol>
    <li>Download the Xming installer from <a href=\"http://www.straightrunning.com/XmingNotes/\" target=\"_blank\">the XMing web site</a>. </li>
    <li>Either install Xming from the <strong>Public Domain Releases</strong> (free) or from the <strong>Website Releases</strong> (after a donation) on the website.</li>
    <li>Run the Xming setup program on your Windows desktop.   Make sure to select 'XLaunch wizard' and 'Normal PuTTY Link SSH client'.<br>
    <img src=\"/assets/153\" alt=\"Xming-Setup.png\"></li>
</ol><h2><br>
Running Xming:</h2><ol>
    <li>To run Xming, select XLaunch from the Start Menu.</li>
    <li>Select 'Multiple Windows'. This will open each application in a separate window.<br>
    <img src=\"/assets/155\" alt=\"Xming-Display.png\"></li>
    <li>Select 'Start no client' to make XLaunch wait for other programs (such as PuTTY).<br>
    <img src=\"/assets/157\" alt=\"Xming-Start.png\"></li>
    <li>Select 'Clipboard' to share the clipboard.<br>
    <img src=\"/assets/159\" alt=\"Xming-Clipboard.png\"></li>
    <li>Finally save the configuration.<br>
    <img src=\"/assets/161\" alt=\"Xming-Finish.png\"></li>
    <li>Now Xming is running ... and you can launch a graphical application in your PuTTY terminal. Do not forget to enable X11 forwarding as explained on <a href=\"/client/windows/console-putty\">our PuTTY page</a>.<br>
    To test the connection, you can try to start a simple X program on the login nodes, e.g., xterm or xeyes. The latter will open a new window with a pair of eyes. The pupils of these eyes should follow your mouse pointer around. Close the program by typing \"ctrl+c\": the window should disappear.<br>
    If you get the error 'DISPLAY is not set', you did not correctly enable the X-Forwarding.</li>
</ol>"
331,"","<h2>
Prerequisites</h2><p>
	It is assumed that Microsoft Visual Studio Professional (at least the Microsoft Visual C++ component) is installed. Although Microsoft Visual C++ 2008 should be sufficient, this how-to assumes that Microsoft Visual C++ 2010 is used. Furthermore, one should be familiar with the basics of Visual Studio, i.e., how to create a new project, how to edit source code, how to compile and build an application.
</p><p>
	Note for KU Leuven and UHasselt users: Microsoft Visual Studio is covered by the campus license for Microsoft products of both KU Leuven and Hasselt University. Hence staff and students can <a href=\"https://www.dreamspark.com/\" target=\"_blank\">download</a> and use the software.
</p><p>
	Also note that although Microsoft offers a free evaluation version of its development tools, i.e., Visual Studio Express, this version does not support parallel programming.
</p><h2>
OpenMP</h2><p>
	Microsoft Visual C++ offers support for developing openMP C/C++ programs out of the box. However, as of this writing, support is still limited to the ancient OpenMP 2.0 standard. The project type best suited is a Windows Console Application. It is best to switch 'Precompiled headers' off.
</p><p>
	Once the project is created, simply write the code, and enable the openMP compiler option in the project's properties as shown below.
</p><p>
	<img src=\"/assets/163\" alt=\"OpenMP project settings\">
</p><p>
	Compiling, building and running your program can now be done in the familiar way.
</p><h2>
MPI</h2><p>
	In order to develop C/C++ programs that use MPI, a few extra things have to be installed, so this will be covered first.
</p><h3>
Installation</h3><ol class=\"list--ordered\">
	<li>
	The MPI libraries and infrastructure is part of Microsoft's <a href=\"https://msdn.microsoft.com/en-us/library/cc853440(v=vs.85).aspx\" target=\"_blank\">HPC Pack SDK</a>. Download the either the 32- or 64-bit version, whichever is appropriate for your desktop system (most probably the 32-bit version, denoted by 'x86'). Installing is merely a matter of double-clicking the downloaded installer.</li>
	<li>
	Although not strictly required, it is strongly recommended to install the <a href=\"https://marketplace.visualstudio.com/items?itemName=ClusterDebuggerLauncherCoreTeam.MPIProjectTemplate\" target=\"_blank\">MPI Project Template</a> as well. Again, one simply downloads and double-clicks the installer.</li>
</ol><h3>
Development</h3><p>
	To develop an MPI-based application, create an MPI project.
</p><p>
	<img src=\"/assets/165\" alt=\"New MPI project\">
</p><p>
	It is advisable not to use precompiled headers, so switch this setting off.
</p><p>
	Next, write your code. Once you are ready to debug or run your code, make the following adjustments to the project's properties in the 'Debugging' section.
</p><p>
	<img src=\"/assets/167\" alt=\"MPI project settings\">
</p><p>
	A few settings should be verified, and if necessary, modified:
</p><ol>
	<li>
	Make sure that the 'Debugger to lauch' is indeed the 'MPI Cluster Debugger'.</li>
	<li>
	The 'Run environment' is 'localhost/1' by default. Since this implies that only one MPI process will be started, it is not very exciting, so change it to, e.g., 'localhost/4' in order to have some parallel processes (4 in this example). Don not make this number too large, since the code will execute on your desktop machine.</li>
	<li>
	The 'MPIExec Command' should be pointed to 'mpiexec' that is found in the 'Bin' directory of the HPC Pack 2008 SDK installation directory.</li>
</ol><p>
	Debugging now proceeds as usual. One can switch between processes by selecting the main thread of the appropriate process by selecting the appropriate main thread in the Threads view.
</p><p>
	<img src=\"/assets/169\" alt=\"Switching MPI processes while debugging\">
</p><h2>
Useful links</h2><ul>
	<li>
	A <a href=\"https://www.codeproject.com/Articles/79508/Mastering-Debugging-in-Visual-Studio-A-Beginn\" target=\"_blank\">tutorial on debugging</a> in Microsoft Visual C++</li>
</ul>"
333,"","<h2>
Installation & setup</h2>
<ol>
	<li>
	<a href=\"https://tortoisesvn.net/downloads.html\" target=\"_blank\">Download</a> the approriate version for your system (32- or 64-bit) and install it. You may to reboot to complete the installation, do so if required.</li>
	<li>
	Optionally, but highly recommended: <a href=\"http://winmerge.org/downloads/\" target=\"_blank\">download</a> and install <a href=\"http://winmerge.org/\" target=\"_blank\">WinMerge</a>, a convenient GUI tool to compare and merge files.</li>
	<li>
	Start Pageant (the SSH agent that comes with PuTTY) and load your private key for authentication on the VSC cluster.</li>
</ol>
<h2>
Checking out a project from a VSC cluster repository</h2>
<pre>svn+ssh://userid@svn.login.node/data/leuven/300/vsc30000/svn-repo/simulation/trunk
</pre>
<ol>
	<li>
	Open Windows Explorer (by e.g., the Windows-E shortcut, or from the Start Menu) and navigate to the directory where you would like to check out your project that is in the VSC cluster repository.</li>
	<li>
	Right-click in this directory, you will notice 'SVN Checkout...' in the context menu, select it to open the 'Checkout' dialog.<br>
	<br>
	<img src=\"/assets/171\" alt=\"TortoiseSVN Checkout dialog\"></li>
	<li>
	In the 'URL of repository' field, type the following line, replacing userid by your VSC user ID, and '300' with '301', '302',... as required (e.g., for user ID 'vsc30257', replace '300' by '302').  For svn.login.node, substitute the appropriate login node for the cluster the repository is on.</li>
	<li>
	Check whether the suggested default location for the project suits you, i.e., the 'Checkout directory' field, if not, modify it.</li>
	<li>
	Click 'OK' to proceed with the check out.</li>
</ol>
<p>
	You now have a working copy of your project on your desktop and can continue to develop locally.
</p>
<h2>
Work cycle</h2>
<p>
	Suppose the file 'simulation.c' is added, and 'readme.txt' is added.  The 'simulation directory will now look as follows:<br>
	<br>
	<img src=\"/assets/173\" alt=\"TortoiseSVN directory view\">
</p>
<p>
	Files that were changed are marked with a red exclamation mark, while those marked in green were unchanged. Files without a mark such as 'readme.txt' have not been placed under version control yet.  The latter can be added to the repository by right-clicking on it, and choosing 'TortoiseSVN' and then 'Add...' from the context menu. Such files will be marked with a bleu '+' sign until the project is committed.
</p>
<p>
	By right-clicking in the project's directory, you will see context menu items 'SVN Update' and 'SVN Commit...'. These have exactly the same semantics as their command line counterparts introduced above. The 'TortoiseSVN' menu item expands into even more command that are familiar, with the notable exception of 'Check for modifications', which is in fact equivalent to 'svn status'.<br>
	<br>
	<img src=\"/assets/175\" alt=\"Tortoise status dialog\">
</p>
<p>
	Right-clicking in the directory and choosing 'SVN Commit...' will bring up a dialog to enter a comment and, if necessary, include or exclude files from the operation.<br>
	<br>
	<img src=\"/assets/177\" alt=\"TortoiseSVN commit dialog\">
</p>
<h2>
Merging</h2>
<p>
	When during an update a conflict that can not be resolved automatically is detected, TortoiseSVN behaves slightly different from the command line client. Rather than requiring you to resolve the conflict immediately, it creates a number of extra files. Suppose the repository was at revision 12, and a conflict was detected in 'simulation.c', then it will create:
</p>
<ul>
	<li>
	'simulation.c': this file is similar to the one subversion would open for you when you choose to edit a conflict via the command line client (this file is marked with a warning sign);</li>
	<li>
	'simulation.c.mine': this is the file in your working copy, i.e., the one that contains changes that were not committed yet;</li>
	<li>
	'simulation.c.r12': the last revision in the repository; and</li>
	<li>
	'simulation.c.r11': the previous revision in the repository.</li>
</ul>
<p>
	You have now two options to resolve the conflict.
</p>
<ol>
	<li>
	Edit 'simulation.c', keeping those modification of either version that you need.</li>
	<li>
	Use WinMerge to compare 'simulation.c.mine' and 'simulation.c.r12' and resolve the conflicts in the GUI, saving the result as 'simulation.c'. When two files are selected in Windows Explorer, they  can be compared using WinMerge by right-clicking on either, and choosing 'WinMerge' from the context menu.<br>
	<br>
	<img src=\"/assets/179\" alt=\"WinMerge file compare\"></li>
</ol>
<p>
	Once all conflicts have been resolved, commit your changes.
</p>
<h2>
Tagging</h2>
<p>
	Tagging can be done conveniently by right-clicking  in Windows Exploerer and selecting 'TortoiseSVN' and then 'Branch/tag...' from the context menu. After supplying the appropriate URL for the tag, e.g.,
</p>
<pre>svn+ssh://&lt;user-id&gt;@&lt;login-node&gt;/data/leuven/300/vsc30000/svn-repo/simulation/tag/nature-submission
</pre>
<p>
	you click 'OK'.
</p>
<h2>
Browsing the repository</h2>
<p>
	Sometimes it is convenient to browse a subversion repository. TortoiseSVN makes this easy, right-click in a directory in Windows Explorer, and select 'TortoiseSVN' and then 'Repo-browser' from the context menu.
</p>
<p>
	<br>
	<img src=\"/assets/181\" alt=\"TortoiseSVN Repo-browser\">
</p>
<h2>
Importing a local project into the VSC cluster repository</h2>
<p>
	As with the command line client, it is possible to import a local directory on your desktop system into your subversion repository on the VSC cluster  . Let us assume that this directory is called 'calculation'. Right-click on it in Windows Explorer, and choose 'Subversion' and then 'Import...' from the context menu. This will open the 'Import' dialog.<br>
	<br>
	<img src=\"/assets/183\" alt=\"TortoiseSVN Import dialog\">
</p>
<p>
	The repository's URL would be (modify the user ID and directory appropriately):
</p>
<pre>svn+ssh://&lt;user-id&gt;@&lt;login-node&gt;/data/leuven/300/vsc30000/svn-repo/calculation/trunk
</pre>
<p>
	TortoiseSVN will automatically create the 'calculation' and 'trunk' directory for you (it uses the '--parents' option).
</p>
<p>
	Creating directories such as 'branches' or 'tags' can be done using the repository browser. To invoke it, right-click in a directory in Windows Explorer and select 'TortoiseSVN' and then 'Repo-browser'. Navigate to the appropriate project directory and create a new directory by right-clicking in the parent directory's content view (right pane) and selecting 'Create folder...' from the context menu.
</p>"
335,"","<p>Since all VSC clusters use Linux as their main operating system, you will need to get acquainted with using the command-line interface and using the terminal.  To open a terminal in Linux when using KDE, choose Applications &gt; System &gt; Terminal &gt; Konsole.  When using GNOME, choose Applications &gt; <span class=\"guimenu\">Accessories &gt;</span> <span class=\"guimenuitem\">Terminal.</span>
</p><p>If you don't have any experience with using the command-line interface in Linux, we suggest you to read the <a href=\"/cluster-doc/using-linux/basic-linux-usage\">basic Linux usage</a> section first.
</p><h2>Getting ready to request an account</h2><ul>
	<li>Before requesting an account, you need to generate a pair of ssh keys. One popular way to do this on Linux is<a href=\"/client/linux/keys-openssh\"> using the freely available OpenSSH client</a> which you can then also use to log on to the clusters.</li>
</ul><h2><a name=\"connecting\"></a>Connecting to the cluster</h2><ul>
	<li>Open a text-mode session using an SSH client:
	<ul>
		<li><a href=\"/client/linux/login-openssh\">OpenSSH ssh command</a></li>
		<li><a href=\"/client/linux/ssh-agent\">Using ssh-agent</a> to avoid having to enter the passphrase all the time</li>
		<li><a href=\"/client/linux/openssh-proxy\">Setting up a SSH proxy</a> to long on to a node by a firewall through another login node, e.g., to access the tier-1 system muk</li>
		<li><a href=\"/client/linux/creating-an-ssh-tunnel\">Creating an SSH tunnel using OpenSSH</a> to establish network communication between your local machine and the cluster otherwise blocked by firewalls.</li>
	</ul>
	</li>
	<li><a id=\"data-transfer\" name=\"data-transfer\"></a>Transfer data using Secure FTP (SFTP) with the <a href=\"/client/linux/data-openssh\">OpenSSH sftp and scp commands</a>.</li>
	<li><a id=\"X-programs\" name=\"X-programs\"></a>Display programs that use graphics or have a GUI
	<ul>
		<li>No extra software is needed on a Linux client system, but you need to use the appropriate options with the ssh command as explained on <a href=\"/client/linux/login-openssh\">the page on OpenSSH</a>.</li>
		<li>On the KU Leuven/UHasselt clusters it is also possible to <a href=\"/client/multiplatform/nx-start-guide\">use the NX Client</a> to log on to the machine and run graphical programs. This requires additional client software that is currently available for Windows, OS X, Linux, Android and iOS. The advantage over displaying X programs directly on your Linux screen is that you can sleep your laptop, disconnect and move to another network without loosing your X-session. Performance may also be better with many programs over high-latency networks.</li>
		<li>The KU Leuven/UHasselt and UAntwerp clusters also offer support for visualization software through TurboVNC. VNC renders images on the cluster and transfers the resulting images to your client device. VNC clients are available for Windows, macOS, Linux, Android and iOS.
		<ul>
			<li>On the KU Leuven/UHasselt clusters, <a href=\"/client/multiplatform/turbovnc\">TurboVNC is supported on the visualization nodes</a>.</li>
			<li>On the UAntwerp clusters, TurboVNC is supported on all regular login nodes (without OpenGL support) and on the visualization node of Leibniz (with OpenGL support through VirtualGL). See the page \"<a href=\"/infrastructure/hardware/hardware-ua/visualization\">Remote visualization @ UAntwerp</a>\" for instructions.</li>
		</ul></li>
	</ul>
	</li>
</ul><h2>Software development</h2><ul>
	<li>Eclipse is a popular multi-platform Integrated Development Environment (IDE) very well suited for code development on clusters.
	<ul>
		<li>Read our <a href=\"/client/multiplatform/eclipse-intro\">Eclipse introduction</a> to find out why you should consider using Eclipse if you develop code and how to get it.</li>
		<li>You can use <a href=\"/client/multiplatform/eclipse-remote-editor\">Eclipse on the desktop as a remote editor for the cluster</a>.</li>
		<li>You can use <a href=\"/client/multiplatform/eclipse-vsc-subversion\">Eclipse on the desktop to access files in a subversion repository on the cluster</a>.</li>
		<li>You can combine the remote editor feature with version control from Eclipse, but some care is needed, and <a href=\"/client/multiplatform/eclipse-ptp-versioncontrol\">here's how to do it</a>.</li>
	</ul>
	</li>
	<li>Linux supports all popular version control systems. See <a href=\"https://www.vscentrum.be/cluster-doc/development/version-control\">our introduction to version control systems</a>.
	<ul>
		<li>Specific instructions to <a href=\"https://www.vscentrum.be/client/multiplatform/desktop-access-vsc-subversion\">access subversion repositories on the VSC clusters or other servers from your desktop with UNIX-style command line tools</a>.</li>
	</ul></li>
</ul>"
337,"","<h2>
Prerequisite: OpenSSH</h2><h3>
Linux</h3><p>
	On all popular Linux distributions, the OpenSSH software is readily available, and most often installed by default. You can check whether the OpenSSH software is installed by opening a terminal and typing:
</p><pre>$ ssh -V
OpenSSH_4.3p2, OpenSSL 0.9.8e-fips-rhel5 01 Jul 2008
</pre><p>
	To access the clusters and transfer your files, you will use the following commands:
</p><ul>
	<li>
	ssh: to generate the ssh keys and to open a shell on a remote machine,</li>
	<li>
	sftp: a secure equivalent of ftp,</li>
	<li>
	scp: a secure equivalent of the remote copy command rcp.</li>
</ul><h3>
Windows</h3><p>
	You can use OpenSSH on Windows also if you install the <a href=\"http://www.cygwin.com/\" target=\"_blank\">free UNIX emulation layer Cygwin</a> with the package \"openssh\".
</p><h3>macOS/OS X</h3><p>macOS/OS X comes with its own implementation of OpenSSH, so you don't need to install any third-party software to use it.  Just open a Terminal window and jump in!
</p><h2>
Generating a public/private key pair</h2><p>
	Usually you already have the software needed and a key pair might already be present in the default location inside your home directory:
</p><pre>$ ls ~/.ssh
authorized_keys2    id_rsa            id_rsa.pub         known_hosts
</pre><p>
	You can recognize a public/private key pair when a pair of files has the same name except for the extension \".pub\" added to one of them. In this particular case, the private key is \"id_rsa\" and public key is \"id_rsa.pub\".  You may have multiple keys (not necessarily in the directory \"~/.ssh\") if you or your operating system requires this. A popular alternative key type, instead of rsa, is dsa. However, we recommend to use rsa keys.
</p><p>
	You will need to generate a new key pair, when:
</p><ul>
	<li>
	you don't have a key pair yet,</li>
	<li>
	you forgot the passphrase protecting your private key,</li>
	<li>
	or your private key was compromised.</li>
</ul><p>
	To generate a new public/private pair, use the following command:
</p><pre>$ ssh-keygen
Generating public/private rsa key pair. 
Enter file in which to save the key (/home/user/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/user/.ssh/id_rsa.
Your public key has been saved in /home/user/.ssh/id_rsa.pub.
</pre><p>
	This will ask you for a file name to store the private and public key, and a passphrase to protect your private key. It needs to be emphasized that you really should choose the passphrase wisely! The system will ask you for it every time you want to use the private key, that is every time you want to access the cluster or transfer your files.
</p><p>
	<span class=\"visualHighlight\">Keys are required in the OpenSSH format.</span>
</p><p>
	If you have a public key \"id_rsa_2048_ssh.pub\" in the SSH2 format, you can use OpenSSH's ssh-keygen to convert it to the OpenSSH format in the following way:
</p><pre>$ ssh-keygen -i -f ~/.ssh/id_rsa_2048_ssh.pub &gt; ~/.ssh/id_rsa_2048_openssh.pub
</pre>"
339,"","<h2>Prerequisite: OpenSSH</h2><p>See <a href=\"/client/linux/keys-openssh\">the page on generating keys</a>.
</p><h2>Connecting to the VSC clusters</h2><h3>Text mode</h3><p>In many cases, a text mode connection to one of the VSC clusters is sufficient. To make such a connection, the ssh command is used:
</p><pre>$ ssh &lt;vsc-account&gt;@&lt;vsc-loginnode&gt;
</pre><p>Here,
</p><ul>
	<li>&lt;vsc-account&gt; is your VSC username that you have received by mail after your request was approved,</li>
	<li>&lt;vsc-loginnode&gt; is the name of the loginnode of the VSC cluster you want to connect to.</li>
</ul><p><span class=\"visualHighlight\">You can find the names and ip-addresses of the loginnodes in the sections on the <a href=\"/infrastructure/hardware\">available hardware</a>.</span>
</p><p>The first time you make a connection to the loginnode, you will be asked to verify the authenticity of the loginnode, e.g.,
</p><pre>$ ssh vsc98765@login.hpc.kuleuven.be
The authenticity of host 'login.hpc.kuleuven.be (134.58.8.192)' can't be established.
RSA key fingerprint is b7:66:42:23:5c:d9:43:e8:b8:48:6f:2c:70:de:02:eb.
Are you sure you want to continue connecting (yes/no)?
</pre><p>Here, user vsc98765 wants to make a connection to the ThinKing cluster at KU Leuven via the loginnode login.hpc.kuleuven.be.
</p><p>If your private key is not stored in a default file (~/.ssh/id_rsa) you need to provide the path to it while making the connection:</p><pre>$ ssh -i &lt;path-to-your-private-key-file&gt; &lt;vsc-account&gt;@&lt;vsc-loginnode&gt;</pre><h3>Connection with support for graphics</h3><p>On most clusters, we support a number of programs that have a GUI mode or display graphics otherwise through the X system. To be able to display the output of such a program on the screen of your Linux machine, you need to tell ssh to forward X traffic from the cluster to your Linux desktop/laptop by specifying the -X option. There is also an option -x to disable such traffic, depending on the default options on your system as specified in /etc/ssh/ssh_config, or ~/.ssh/config.<br>
	Example:
</p><pre>ssh -X vsc123456@login.hpc.kuleuven.be
</pre><p>To test the connection, you can try to start a simple X program on the login nodes, e.g., <code>xterm</code> or <code>xeyes</code>. The latter will open a new window with a pair of eyes. The pupils of these eyes should follow your mouse pointer around. Close the program by typing \"ctrl+c\": the window should disappear.
</p><p>If you get the error 'DISPLAY is not set', you did not correctly enable the X-Forwarding.
</p><h2>Links</h2><ul>
	<li><a href=\"http://man.openbsd.org/ssh\" target=\"_blank\">ssh manual page</a> (external)</li>
</ul>"
341,"","<p>
	The OpenSSH program ssh-agent is a program to hold private keys used for public key authentication (RSA, DSA). The idea is that you store your private key in the ssh authentication agent and can then log in or use sftp as often as you need without having to enter your passphrase again. This is particularly useful when setting up a ssh proxy connection (e.g., for the tier-1 system muk) as these connections are more difficult to set up when your key is not loaded into an ssh-agent.
</p><p>
	This all sounds very easy. The reality is more difficult though. The problem is that subsequent commands, e.g., the command to add a key to the agent or the ssh or sftp commands, must be able to find the ssh authentication agent. Therefore some information needs to be passed from ssh-agent to subsequent commands, and this is done through two <em>environment variables</em>: <code>SSH_AUTH_SOCK</code> and <code>SSH_AGENT_PID</code>. The problem is to make sure that these variables are defined with the correct values in the shell where you start the other ssh commands.
</p><h2>
Starting ssh-agent: Basic scenarios</h2><p>
	There are a number of basic scenarios
</p><ol>
	<li>
	You're lucky and your system manager has set up everything so that ssh-agent is started automatically when the GUI starts after logging in and the environment variables are hence correctly defined in all subsequent shells.
            You can check for that easily: type
	<pre>$ ssh-add -l</pre>
	If the command returns with the message
	<pre>Could not open a connection to your authentication agent.</pre>
	then ssh-agent is not running or not configured properly, and you'll need to follow one of the following scenarios.
	</li>
	<li>
	Start an xterm (or whatever your favourite terminal client is) and continue to work in that xterm window or other terminal windows started from that one:
	<pre>$ ssh-agent xterm &
	</pre>
	The shell in that xterm is then configured correctly, and when that xterm is killed, the ssh-agent will also be killed.
	</li>
	<li>
	ssh-agent can also output the commands that are needed to configure the shell. These can then be used to configure the current shell or any further shell. E.g., if you're a bash user, an easy way to start a ssh-agent and configure it in the current shell, is to type
	<pre>$ eval `ssh-agent -s`
	</pre>
	at the command prompt. If you start a new shell (e.g., by starting an xterm) from that shell, it should also be correctly configured to contact the ssh authentication agent.
            A better idea though is to store the commands in a file and excute them in any shell where you need access to the authentication agent. E.g., for bash users:
	<pre>$ ssh-agent -s &gt;~/.ssh-agent-environment
. ~/.ssh-agent-environment
	</pre>
	and you can then configure any shell that needs access to the authentication agent by executing
	<pre>$ . ~/.ssh-agent-environment<br>
	</pre>
	Note that this will not necessarily shut down the ssh-agent when you log out of the system. It is not a bad idea to explicitly kill the ssh-agent before you log out:
	<pre>$ ssh-agent -k
	</pre>
	</li>
</ol><h2>
<a id=\"ssh-add\" name=\"ssh-add\"></a>Managing keys</h2><p>
	Once you have an ssh-agent up and running, it is very easy to add your key to it. If your key has the default name(id_rsa), all you need to do is to type
</p><pre>$ ssh-add
</pre><p>
	at the command prompt. You will then be asked to enter your passphrase. If your key has a different name, e.g., id_rsa_cluster, you can specify that name as an additional argument to ssh-add:
</p><pre>$ ssh-add ~/.ssh/id_rsa_cluster
</pre><p>
	To list the keys that ssh-agent is managing, type
</p><pre>$ ssh-add -l
</pre><p>
	You can now use the OpenSSH commands <a href=\"/client/linux/login-openssh\">ssh</a>, <a href=\"/client/linux/data-openssh\">sftp</a> and <a href=\"/client/linux/data-openssh\">scp</a> without having to enter your passphrase again.
</p><h2>
Starting ssh-agent: Advanced options</h2><p>
	In case ssh-agent is not started by default when you log in to your computer, there's a number of things you can do to automate the startup of ssh-agent and to configure subsequent shells.
</p><h3>
Ask your local system administrator</h3><p>
	If you're not managing your system yourself, you can always ask your system manager if he can make sure that ssh-agent is started when you log on and in such a way that subsequent shells opened from the desktop have the environmental variables SSH_AUTH_SOCK and SSH_AGENT_PID set (with the first one being the most important one).
</p><p>
	And if you're managing your own system, you can dig into the manuals to figure out if there is a way to do so. Since there are so many desktop systems avaiable for Linux systems (gnome, KDE, Ubuntu unity, ...) we cannot offer help here.
</p><h3>
A semi-automatic solution in bash</h3><p>
	<span style=\"color:#b22222;\"><em>This solution requires some modifications to .bash_profile and .bashrc. Be careful when making these modifications as errors may lead to trouble to log on to your machine. So test by executing these files with <code>source ~/.bash_profile</code> and <code>source ~/.bashrc</code>.</em></span>
</p><p>
	This simple solution is based on option 3 given above to start ssh-agent.
</p><ol>
	<li>
	You can define a new shell command by using the <a href=\"https://www.gnu.org/software/bash/manual/bash.html#Aliases\" target=\"_blank\">bash alias mechanism</a>. Add the following line to the file .bashrc in your home directory:
	<pre>alias start-ssh-agent='/usr/bin/ssh-agent -s &gt;~/.ssh-agent-environment; . ~/.ssh-agent-environment'
	</pre>
	The new command start-ssh-agent will now start a new ssh-agent, store the commands to set the environment variables in the file .ssh-agent-environment in your home directory and then \"source\" that file to execute the commands in the current shell (which then sets SSH_AUTH_SOCK and SSH_AGENT_PID to appropriate values).
	</li>
	<li>
	Also put the line
	<pre>[[ -s ~/.ssh-agent-environment ]] && . ~/.ssh-agent-environment &&gt;/dev/null
	</pre>
	in your .bashrc file. This line will check if the file ssh-agent-environment exists in your home directory and \"source\" it to set the appropriate environment variables.
	</li>
	<li>
	As explained in the <a href=\"https://www.gnu.org/software/bash/manual/bash.html#Bash-Startup-Files\" target=\"_blank\">GNU bash manual</a>, .bashrc is only read when starting so-called interactive non-login shells. Interactive login shells will not read this file by default. Therefore it is <a href=\"https://www.gnu.org/software/bash/manual/bash.html#Bash-Startup-Files\" target=\"_blank\">advised in the GNU bash manual</a> to add the line
	<pre>[[ -s ~/.bashrc ]] && . ~/.bashrc
	</pre>
	to your .bash_profile. This will execute .bashrc if it exists whenever .bash_profile is called.
	</li>
</ol><p>
	You can now start a SSH authentication agent by issuing the command <code>start-ssh-agent</code> and add your key <a href=\"#ssh-add\">as indicated above</a> with <code>ssh-add</code>.
</p><h3>
An automatic and safer solution in bash</h3><p>
	One disadvantage of the previous solution is that a new ssh-agent will be started every time you execute the command start-ssh-agent, and all subsequent shells will then connect to that one.
</p><p>
	The following solution is much more complex, but a lot safer as it will first do an effort to see if there is already a ssh-agent running that can be contacted:
</p><ol>
	<li>
	It will first check if the environment variable SSH_AUTH_SOCK is defined, and try to contact that agent. This makes sure that no new agent will be started if you log on onto a system that automatically starts an ssh-agent.</li>
	<li>
	Then it will check for a file .ssh-agent-environment, source that file and try to connect to the ssh-agent. This will make sure that no new agent is started if another agent can be found through that file.</li>
	<li>
	And only if those two tests fail will a new ssh-agent be started.</li>
</ol><p>
	This solution uses a Bash function.
</p><ol><li>        Add the following block of text to your .bashrc file:<pre>start-ssh-agent() {
#
# Start an ssh agent if none is running already.
# * First we try to connect to one via SSH_AUTH_SOCK
# * If that doesn't work out, we try via the file ssh-agent-environment
# * And if that doesn't work out either, we just start a fresh one and write
#   the information about it to ssh-agent-environment for future use.
#
# We don't really test for a correct value of SSH_AGENT_PID as the only 
# consequence of not having it set seems to be that one cannot kill
# the ssh-agent with ssh-agent -k. But starting another one wouldn't 
# help to clean up the old one anyway.
#
# Note: ssh-add return codes: 
#   0 = success,
#   1 = specified command fails (e.g., no keys with ssh-add -l)
#   2 = unable to contact the authentication agent
#
sshfile=~/.ssh-agent-environment
#
# First effort: Via SSH_AUTH_SOCK/SSH_AGENT_PID
#
if [ -n \"$SSH_AUTH_SOCK\" ]; then
  # SSH_AUTH_SOCK is defined, so try to connect to the authentication agent
  # it should point to. If it succeeds, reset newsshagent.
  ssh-add -l &&gt;/dev/null 
  if [[ $? != 2 ]]; then 
    echo \"SSH agent already running.\"
    unset sshfile
    return 0
  else
    echo \"Could not contact the ssh-agent pointed at by SSH_AUTH_SOCK, trying more...\"
  fi
fi
#
# Second effort: If we're still looking for an ssh-agent, try via $sshfile
#
if [ -e \"$sshfile\" ]; then
  # Load the environment given in $sshfile
  . $sshfile &&gt;/dev/null
  # Try to contact the ssh-agent
  ssh-add -l &&gt;/dev/null 
  if [[ $? != 2 ]]; then 
    echo \"SSH agent already running; reconfigured the environment.\"
    unset sshfile
    return 0
  else
    echo \"Could not contact the ssh-agent pointed at by $sshfile.\"
  fi
fi
#
# And if we haven't found a working one, start a new one...
#
#Create a new ssh-agent
echo \"Creating new SSH agent.\"
ssh-agent -s &gt; $sshfile && . $sshfile    
unset sshfile
}
	</pre>            A shorter version without all the comments and that does not generate output is<pre>start-ssh-agent() {
sshfile=~/.ssh-agent-environment
#
if [ -n \"$SSH_AUTH_SOCK\" ]; then
  ssh-add -l &&gt;/dev/null 
  [[ $? != 2 ]] && unset sshfile && return 0
fi
#
if [ -e \"$sshfile\" ]; then
  . $sshfile &&gt;/dev/null
  ssh-add -l &&gt;/dev/null 
  [[ $? != 2 ]] && unset sshfile && return 0
fi
#
ssh-agent -s &gt; $sshfile && . $sshfile &&gt;/dev/null
unset sshfile
}
	</pre>            This defines the command <code>start-ssh-agent</code>.</li><li>            Since start-ssh-agent will now first check for a usable running agent, it doesn't harm to simply execute this command in your .bashrc file to start a SSH authentication agent. So add the line<pre>start-ssh-agent &&gt;/dev/null
	</pre> after the above function definition. All output is sent to /dev/null (and hence not shown) as a precaution, since <code>scp</code> or <code>sftp</code> sessions fail when output is generated in <code>.bashrc</code> on many systems (typically with error messages such as \"Received message too long\" or \"Received too large sftp packet\").            You can also use the newly defined command start-ssh-agent at the command prompt. It will then check your environment, reset the environment variables SSH_AUTH_SOCK and SSH_AGENT_PID or start a new ssh-agent.</li><li>            As explained in the <a href=\"https://www.gnu.org/software/bash/manual/bash.html#Bash-Startup-Files\" target=\"_blank\">GNU bash manual</a>, .bashrc is only read when starting so-called interactive non-login shells. Interactive login shells will not read this file by default. Therefore it is <a href=\"https://www.gnu.org/software/bash/manual/bash.html#Bash-Startup-Files\" target=\"_blank\">advised in the GNU bash manual</a> to add the line<pre>[[ -s ~/.bashrc ]] && . ~/.bashrc
	</pre> to your .bash_profile. This will execute .bashrc if it exists whenever .bash_profile is called.</li></ol><p>
	You can now simply add your key <a href=\"#ssh-add\">as indicated above</a> with <code>ssh-add</code> and it will become available in all shells.
</p><p>
	The only remaining problem is that the ssh-agent process that you started may not get killed when you log out, and if it fails to contact again to the ssh-agent when you log on again, the result may be a built-up of ssh-agent processes. You can always kill it by hand before logging out with <code>ssh-agent -k</code>.
</p><h2>
Links</h2><ul>
	<li>
	<a href=\"http://man.openbsd.org/ssh-agent\" target=\"_blank\">ssh-agent manual page</a> (external)</li>
	<li>
	<a href=\"http://man.openbsd.org/ssh-add\" target=\"_blank\">ssh-add manual page</a> (external)</li>
</ul>"
343,"","<h2>
    Rationale</h2><p>
    ssh provides a safe way of connecting to a computer, encrypting traffic and avoiding passing passwords across public networks where your traffic might be intercepted by someone else. Yet making a server accessible from all over the world makes that server very vulnerable. Therefore servers are often put behind a <em>firewall</em>, another computer or device that filters traffic coming from the internet. </p><p>
    In the VSC, all clusters are behind a firewall, but for the tier-1 cluster muk this firewall is a bit more restrictive than for other clusters. Muk can only be approached from certain other computers in the VSC network, and only via the internal VSC network and not from the public network. To avoid having to log on twice, first to another login node in the VSC network and then from there on to Muk, one can set up a so-called <em>ssh proxy</em>. You then connect through another computer (the <em>proxy server</em>) to the computer that you really want to connect to.</p><p>
    This all sounds quite complicated, but once things are configure properly it is really simple to log on to the host.</p><h2>
    Setting up a proxy in OpenSSH</h2><p>
    Setting up a proxy is done by adding a few lines to the file <code>$HOME/.ssh/config</code> on the machine from which you want to log on to another machine.</p><p>
    The basic structure is as follows:</p><pre>Host &lt;my_connectionname&gt;
    ProxyCommand ssh -q %r@&lt;proxy server&gt; 'exec nc &lt;target host&gt; %p'
    User &lt;userid&gt;</pre><p>
    where:</p><ul>
    <li>
        <code>&lt;my_connectionname&gt;</code>: the name you want to use for this proxy connection. You can then log on to the <code>&lt;target host&gt;</code> using this proxy configuration using ssh <code>&lt;my_connectionname&gt;</code></li>
    <li>
        <code>&lt;proxy server&gt;</code>: The name of the proxy server for the connection</li>
    <li>
        <code>&lt;target host&gt;</code>: The host to which you want to log on.</li>
    <li>
        <code>&lt;userid&gt;</code>: Your userid on <code>&lt;target host&gt;</code>.</li>
</ul><p>
    <em><strong>Caveat:</strong> Access via the proxy will only work if you have logged in to the proxy server itself at least once from the client you're using.</em></p><h2>
    Some examples</h2><h3>
    A regular proxy without X forwarding</h3><p>
    In Linux or macOS, SSH proxies are configured as follows:</p><p>
    In your <code>$HOME/.ssh/config</code> file, add the following lines:</p><pre>Host tier1
    ProxyCommand ssh -q %r@vsc.login.node 'exec nc login.muk.gent.vsc %p'
    User vscXXXXX
</pre><p>
    where you replace <em>vsc.login.node</em> with the name of the login node of your home tier-2 cluster (see also the <a href=\"/infrastructure/hardware\">overview of available hardware</a>).</p><p>
    Replace <code>vscXXXXX</code> your own VSC account name (e.g., <code>vsc40000</code>).</p><p>
    The name 'tier1' in the 'Host' field is arbitrary. Any name will do, and this is the name you need to use when logging in:</p><pre>$ ssh tier1
</pre><h3>
    A proxy with X forwarding</h3><p>
    This requires a minor modification to the lines above that need to be added to <code>$HOME/.ssh/config</code>:</p><pre>Host tier1X
    ProxyCommand ssh -X -q %r@vsc.login.node 'exec nc login.muk.gent.vsc %p'
    ForwardX11 yes
    User vscXXXXX
</pre><p>
    I.e., you need to add the -X option to the ssh command to enable X forwarding and need to add the line '<code>ForwardX11 yes</code>'.</p><pre>$ ssh tier1X</pre><p>
    will then log you on to login.muk.gent.vsc with X forwarding enabled provided that the $DISPLAY variable was correctly set on the client on which you executed the ssh command. Note that simply executing</p><pre>$ ssh -X tier1</pre><p>
    has the same effect. It is not necessary to specify the X forwarding in the config file, it can be done just when running ssh.</p><h3>
    The proxy for testing/debugging on muk</h3><p>
    For testing/debugging, you can login to the UGent login node gengar1.gengar.gent.vsc over the VSC network. The following <code>$HOME/.ssh/config</code> can be used:</p><pre>Host tier1debuglogin
    ProxyCommand ssh -q %r@vsc.login.node 'exec nc gengar1.gengar.gent.vsc %p'
    User vscXXXXX
</pre><p>
    Change <code>vscXXXXX</code> to your VSC username and connect with</p><pre>$ ssh tier1debuglogin</pre><h2>
    For advanced users</h2><p>
    You can define many more properties for a ssh connection in the config file, e.g., setting up ssh tunneling. On most Linux machines, you can get more information about all the possibilities by issuing</p><pre>$ man 5 ssh_config</pre><p>
    Alternatively, you can also google on this line and find <a href=\"http://www.manpagez.com/man/5/ssh_config/\" target=\"_blank\">copies of the manual page on the internet</a>.</p>"
345,"","<h2>
Prerequisits</h2><ul><li>A ssh key pair, properly installed in your VSC account, see the  <a href=\"/client/linux/keys-openssh\">page on generating keys</a>.</li><li>Additionally, you should be able to <a href=\"/client/linux/login-openssh\">connect to the cluster's login node</a> using ssh.</li></ul><h2>
Background</h2><p>
	Because of one or more firewalls between your desktop and the HPC clusters, it is generally impossible to communicate directly with a process on the cluster from your desktop except when the network managers have given you explicit permission (which for security reasons is not often done). One way to work around this limitation is SSH tuneling. There are serveral cases where this is usefull:
</p><ul>
	<li>
	Running X applications on the cluster: The X program cannot directly communicate with the X server on your local system. In this case, the tunneling is easy to set up as OpenSSH will do it for you if you specify the -X-option on the command line when you log on to the cluster in text mode:
	<pre>$ ssh -X &lt;vsc-account&gt;@&lt;vsc-loginnode&gt;
	</pre>
	where &lt;vsc-account&gt; is your VSC-number and &lt;vsc-loginnode&gt; is the hostname of the cluster's login node you are using.</li>
	<li>
	Running a server application on the cluster that a client on the desktop connects to. One example of this scenario is <a href=\"/cluster-doc/postprocessing/paraview-remote-visualization\">ParaView in remote visualization mode</a>, with the interactive client on the desktop and the data processing and image rendering on the cluster. Setting up a tunnel for this scenario is also <a href=\"/cluster-doc/postprocessing/paraview-remote-visualization\">explained on that page</a>.</li>
	<li>
	Running clients on the cluster and a server on your desktop. In this case, the source port is a port on the cluster and the destination port is on the desktop.</li>
</ul><h2>
Procedure</h2><p>
	In a terminal window on your client machine, issue the following command:
</p><pre>ssh  -L11111:r1i3n5:44444  -N  &lt;vsc-account&gt;@&lt;vsc-loginnode&gt;
</pre><p>
	where &lt;vsc-account&gt; is your VSC-number and &lt;vsc-loginnode&gt; is the hostname of the cluster's login node you are using.  The local port is given first (e.g., 11111, followed by the remote host (e.g., 'r1i3n5') and the server port (e.g., 44444).
</p><ol>
	<li>
	Log in on the login node</li>
	<li>
	Start the server job, note the compute node's name the job is running on (e.g., 'r1i3n5'), as well as the port the server is listening on (e.g., '44444').</li>
</ol>"
347,"","<h2>
Prerequisite: OpenSSH</h2><p>
	See the page on <a href=\"/client/linux/keys-openssh\">generating keys</a>.
</p><ul>
</ul><h2>
Using scp</h2><p>
	Files can be transferred with scp, which is more or less a cp equivalent, but then to or from a remote machine.
</p><p>
	For example, to copy the (local) file localfile.txt to your home directory on the cluster (where &lt;vsc-loginnode&gt; is a loginnode), use:
</p><pre>scp localfile.txt &lt;vsc-account&gt;@&lt;vsc-loginnode&gt;:
</pre><p>
	Likewise, to copy the remote file remotefile.txt from your home directory on the cluster to your local computer, use:
</p><pre>scp &lt;vsc-account&gt;@&lt;vsc-loginnode&gt;:localfile.txt .
</pre><p>
	<span class=\"visualHighlight\">The colon is required!</span>
</p><h2>
Using sftp</h2><p>
	The sftp is an equivalent of the ftp command, but it uses the secure ssh protocol to connect to the clusters.
</p><p>
	One easy way of starting a sftp session is
</p><pre>sftp &lt;vsc-account&gt;@&lt;vsc-loginnode&gt;
</pre><h2>
Links</h2><ul>
	<li>
	<a href=\"http://man.openbsd.org/scp\" target=\"_blank\">scp manual page</a> (external)</li>
	<li>
	<a href=\"http://man.openbsd.org/sftp\" target=\"_blank\">sftp manual page</a> (external)</li>
</ul>"
349,"","<p>Since all VSC clusters use Linux as their main operating system, you will need to get acquainted with using the command-line interface and using the Terminal.  To open a Terminal window in macOS (formerly OS X), choose Applications &gt; Utilities &gt; Terminal in the Finder.
</p><p>If you don't have any experience with using the Terminal, we suggest you to read the <a href=\"/cluster-doc/using-linux/basic-linux-usage\">basic Linux usage</a> section first (which also applies to macOS).
</p><h2>Getting ready to request an account</h2><ul>
	<li>Before requesting an account, you need to generate a pair of ssh keys. One popular way to do this on macOS is<a href=\"/client/macosx/keys-openssh\"> using the OpenSSH client</a> included with macOS which you can then also use to log on to the clusters.</li>
</ul><h2><a name=\"connecting\"></a>Connecting to the machine</h2><ul>
	<li>Open a text-mode session using an SSH client: <a href=\"/client/macosx/login-openssh\">OpenSSH ssh command or JellyfiSSH</a>.</li>
	<li><a id=\"data-transfer\" name=\"data-transfer\"></a>Transfer data using Secure FTP (SFTP) with the <a href=\"/client/macosx/data-cyberduck\">OpenSSH sftp and scp commands, Cyberduck or FileZilla</a>.</li>
	<li><a id=\"X-programs\" name=\"X-programs\"></a>Running GUI programs or other programs that use graphics.
	<ul>
		<li>Linux programs use the X protocol to display graphics on local or remote screens. To use your Mac as a remote screen, you need to install a X server. <a href=\"https://www.xquartz.org/\" target=\"_blank\">XQuartz</a> is one that is freely available. Once the X server is up and running, you can simply open a terminal window and connect to the cluster using the command line SSH client in the same way as you would on Linux. </li>
		<li>On the KU Leuven/UHasselt clusters it is possible to <a href=\"/client/multiplatform/nx-start-guide\">use the NX Client</a> to log on to the machine and run graphical programs. Instead of an X-server, another piece of client software is needed. That software is currently available for Windows, macOS, Linux, Android and iOS.</li>
		<li>The KU Leuven/UHasselt and UAntwerp clusters also offer support for visualization software through TurboVNC. VNC renders images on the cluster and transfers the resulting images to your client device. VNC clients are available for Windows, macOS, Linux, Android and iOS.
		<ul>
			<li>On the KU Leuven/UHasselt clusters, <a href=\"/client/multiplatform/turbovnc\">TurboVNC is supported on the visualization nodes</a>.</li>
			<li>On the UAntwerp clusters, TurboVNC is supported on all regular login nodes (without OpenGL support) and on the visualization node of Leibniz (with OpenGL support through VirtualGL). See the page \"<a href=\"/infrastructure/hardware/hardware-ua/visualization\">Remote visualization @ UAntwerp</a>\" for instructions.</li>
		</ul></li>
	</ul>
	</li>
</ul><h2>Advanced topics</h2><ul>
	<li>Eclipse is a popular multi-platform Integrated Development Environment (IDE) very well suited for code development on clusters.
	<ul>
		<li>Read our <a href=\"/client/multiplatform/eclipse-intro\">Eclipse introduction</a> to find out why you should consider using Eclipse if you develop code and how to get it. To get the full functionality of the Parallel Tools Platform and Fortran support on macOS, you need <a href=\"/client/macosx/eclipse-on-osx\">to install some additional software and start Eclipse in a special way as we explain here</a>.</li>
		<li>You can use <a href=\"/client/multiplatform/eclipse-remote-editor\">Eclipse on the desktop as a remote editor for the cluster</a>.</li>
		<li>You can use <a href=\"/client/multiplatform/eclipse-vsc-subversion\">Eclipse on the desktop to access files in a subversion repository on the cluster</a>.</li>
		<li>You can combine the remote editor feature with version control from Eclipse, but some care is needed, and <a href=\"/client/multiplatform/eclipse-ptp-versioncontrol\">here's how to do it</a>.</li>
	</ul>
	</li>
	<li>Most popular version control systems, including Subversion and git, are supported on macOS. See <a href=\"https://www.vscentrum.be/cluster-doc/development/version-control\">our introduction to version control systems</a>.
	<ul>
		<li>Specific instructions to <a href=\"/client/multiplatform/desktop-access-vsc-subversion\">access subversion repositories on the VSC clusters or other servers from your desktop with UNIX-style command line tools</a>. </li>
	</ul></li>
</ul>"
351,"","<h2>
Prerequisite: OpenSSH</h2><p>
	Every macOS install comes with its own implementation of OpenSSH, so you don't need to install any third-party software to use it.  Just open a Terminal window and jump in!  Because of this, you can use the same commands as specified in the <a href=\"/client/linux\">Linux client</a> section to access the cluster and transfer files.
</p><h2>
Generating a public/private key pair</h2><p>
	Generating a public/private key pair is identical to what is described in the <a href=\"/client/linux/keys-openssh\">Linux client</a> section, that is, by using the ssh-keygen command in a Terminal window.
</p>"
353,"","<h2>Prerequisites</h2><ul>
	<li>macOS comes with its own implementation of OpenSSH, so you don't need to install any third-party software to use it.  Just open a Terminal window and jump in!  Because of this, you can use the same commands as specified in the <a href=\"/client/linux\">Linux client section</a> to access the cluster and transfer files (<a href=\"/client/linux/keys-openssh\">ssh-keygen</a> to generate the keys, <a href=\"/client/linux/login-openssh\">ssh</a> to log on to the cluster and <a href=\"/client/linux/data-openssh\">scp and sftp</a> for file transfer).</li>
	<li>Optional: You can use <a href=\"http://www.m-works.co.nz/jellyfissh.php\" target=\"_blank\">JellyfiSSH</a> to store your ssh session settings.  The most recent version is available for a small fee from the Mac App Store, but if you <a href=\"https://www.google.be/webhp?ion=1&ie=UTF-8#q=JellyfiSSH+4.5.2\" target=\"_blank\">google for JellyfiSSH 4.5.2</a>, the version used for the screenshots in this page, you can still find some free downloads for that version. Installation is easy: just drag the program's icon to the Application folder in the Finder, and you're done.</li>
</ul><h2>Connecting using OpenSSH</h2><p>Like in the Linux client section, the ssh command is used to make a connection to (one of) the VSC clusters. In a Terminal window, execute:
</p><pre>$ ssh &lt;vsc-account&gt;@&lt;vsc-loginnode&gt;
</pre><p>where
</p><ul>
	<li>&lt;vsc-account&gt; is your VSC username that you have received by mail after your request was approved,</li>
	<li>&lt;vsc-loginnode&gt; is the name of the loginnode of the VSC cluster you want to connect to.</li>
</ul><p>You can find the names and ip-addresses of the loginnodes in the <a href=\"/infrastructure/hardware\">sections of the local VSC clusters</a>.
</p><p>SSH will ask you to enter your passphrase.
</p><p>On sufficiently recent macOS/OS X versions (Leopard and newer) you can use the Keychain Access service to automatically provide your passphrase to ssh. All you need to do is to add the key using
</p><pre>$ ssh-add ~/.ssh/id_rsa
</pre><p>(assuming that your private key that <a href=\"/client/macosx/keys-openssh\">you generated before</a> is called id_rsa).
</p><h2>Using JellyfiSSH for bookmarking ssh connection settings</h2><p>You can use JellyfiSSH to create a user-friendly bookmark for your ssh connection settings. To do this, follow these steps:
</p><ol>
	<li>Start JellyfiSSH and select 'New'. This will open a window where you can specify the connection settings.</li>
	<li>
	In the 'Host or IP' field, type in &lt;vsc-loginnode&gt;. In the 'Login name' field, type in your &lt;vsc-account&gt;.<br>
	In the screenshot below we have filled in the fields for a connection to ThinKing cluster at KU Leuven as user vsc98765.<br>
	<img src=\"/assets/185\" alt=\"JellyfiSSH Connection Settings\" width=\"400\"></li>
	<li>You might also want to change the Terminal window settings, which can be done by clicking on the icon in the lower left corner of the JellyfiSSH window.</li>
	<li>When done, provide a name for the bookmark in the 'Bookmark Title' field and press 'Add' to create the bookmark.</li>
	<li>To make a connection, select the bookmark in the 'Bookmark' field and click on 'Connect'.  Optionally, you can make the bookmark the default by selecting it as the 'Startup Bookmark' in the JellyfiSSH &gt; Preferences menu entry.</li>
</ol>"
355,"","<h2>Prerequisite: OpenSSH, Cyberduck or FileZilla</h2><ul>
	<li>OS X comes with its own implementation of OpenSSH, so you don't need to install any third-party software to use it. Just open a Terminal window and jump in! Because of this, you can use the same <a href=\"/client/linux/data-openssh\">scp and sftp commands as in Linux</a> to access the cluster and transfer files.</li>
	<li>We recommend <a href=\"https://cyberduck.io\" target=\"_blank\">Cyberduck</a> as a graphical alternative to the scp command. This program is freely available (with a voluntary donation) from <a href=\"https://cyberduck.io\" target=\"_blank\">the Cyberduck web site</a> and easy to use. Installation is easy: just drag the program's icon to the Application folder in the Finder, and you're done.<br>
	The program can also be found in the App Store but at a price.</li>
	<li>An alternative SFTP GUI is <a href=\"https://filezilla-project.org/\" target=\"_blank\">FileZilla</a>. FileZilla for macOS is very similar to FileZilla for Windows (see also our <a href=\"/client/windows/filezilla\">page about FileZilla in the Windows section</a>). It can be downloaded from the <a href=\"https://filezilla-project.org/download.php?show_all=1\" target=\"_blank\">FileZilla download page</a>.</li>
</ul><h2>Transferring files with Cyberduck</h2><p>Files can be easily transferred with Cyberduck.  Setup is easy:
</p><ol>
	<li>After starting Cyberduck, the Bookmark tab will show up. To add a new bookmark, click on the '+' sign on the bottom left of the window.  A new window will open.</li>
	<li>In the 'Server' field, type in &lt;vsc-loginnode&gt;. In the 'Username' field, type in your &lt;vsc-account&gt;.</li>
	<li>Click on 'More Options', select 'Use Public Key Authentication' and point it to your private key (the filename will be shown underneath). Please keep in mind that Cybeduck works only with passphrase-protected private keys.</li>
	<li>
	Finally, type in a name for the bookmark in the 'Nickname' field and close the window by pressing on the red circle in the top left corner of the window.<br><br>
	<img src=\"/assets/187\" alt=\"Cyberduck Bookmark\" width=\"318\"></li>
	<li>To open the scp connection, click on the 'Bookmarks' icon (which resembles an open book) and double click on the bookmark you just created.</li>
</ol><h2>Transferring files with FileZilla</h2><p>To install FileZilla, follow these steps:
</p><ol>
	<li>Download the appropriate file from the <a href=\"https://filezilla-project.org/download.php?show_all=1\" target=\"_blank\">FileZilla download page</a>. </li>
	<li>The file you just downloaded is a compressed UNIX-style archive (with a name ending on .tar.bz2). Doubleclick on this file in Finder (most likely in the Downloads folder) and drag the FileZilla icon that appears to the Applications folder.</li>
	<li>Depending on the settings of your machine, you may get notification that Filezilla.app cannot be opened because it is from an unidentified developer when you try to start it. Check out the macOS Gatekeeper on <a href=\"https://support.apple.com/en-gb/HT202491\" target=\"_blank\">this Apple support page</a>.</li>
</ol><p>FileZilla for macOS works in pretty much the same way as FileZilla for Windows:
</p><ol>
	<li>start FileZilla;</li>
	<li>open the 'Site Manager' using the 'File' menu;</li>
	<li>create a new site by clicking the New Site button;</li>
	<li>in the tab marked General, enter the following values (all other fields remain blank):
	<ul>
		<li>Host: <i>l</i><em>ogin.node.vsc </em>(replace with the name of the login node of your home cluster)</li>
		<li>Servertype: SFTP - SSH File Transfer Protocol</li>
		<li>Logon Type: Normal</li>
		<li>User: <i>your own </i>VSC user ID, e.g., vsc98765;</li>
	</ul>
	</li>
	<li>optionally, rename this setting to your liking by pressing the 'Rename' button;</li>
	<li>press 'Connect'. Enter your passphrase when requested. FileZilla will try to use the information in your macOS Keychain. See the page on '<a href=\"/client/macosx/login-openssh\">Text-mode access using OpenSSH</a>' to find out how to add your key to the keychain using <code>ssh-add</code>. </li>
</ol><p style=\"text-align: center;\"><a href=\"/assets/189\"><img src=\"/assets/189\" alt=\"FileZilla connection screen\" width=\"560\"></a>
</p><p>Note that recent versions of FileZilla have a screen in the settings to 
manage private keys. The path to the private key must be provided in 
options (Edit Tab -&gt; options -&gt; connection -&gt; SFTP):
</p><p style=\"text-align: center\"><img src=\"/assets/1197\" alt=\"FileZilla configuration\" align=\"middle\" width=\"440\">
</p><p>After that you should be able to connect after being asked for 
passphrase. As an alternative you can choose to use the built-in macOS keychain system.
</p>"
357,"","<h2>Installation</h2><p>Eclipse doesn't come with its own compilers. By default, it relies on the Apple gcc toolchain. You can install this toolchain by installing the Xcode package from the App Store. This package is free, but since it takes quite some disk space and few users need it, it is not installed by default on OS X (though it used to be). After installing Xcode, you can install Eclipse according to the instructions on the Eclipse web site. Eclipse will then use the gcc command from the Xcode distribution. The Apple version of gcc is really just the gcc front-end layered on top of a different compiler, LLVM, and might behave differently from gcc on the cluster.
</p><p>If you want a regular gcc or need Fortran or MPI or mathematical libraries equivalent to those in the <a href=\"/cluster-doc/development/toolchain-foss\">foss toolchain</a> on the cluster, you'll need to install additional software. We recommend <a href=\"https://www.macports.org/\">using MacPorts</a> for this as it contains ports to macOS of most tools that we include in our toolchains. Using MacPorts requires some familiarity with the bash shell, so you may have a look at <a href=\"/cluster-doc/using-linux\">our \"Using Linux\"  section</a> or search the web for a good bash tutorial (one in a Linux tutorial will mostly do). E.g., you'll have to add the directory where MacPort installs the applications to your PATH enviroment variable. For a typical MacPorts installation, this directory is /opt/local/bin.
</p><p>After installing MacPorts, the following commands will install a libraries and tools that are very close to those of the foss2016b toolchain (tested September 2016):<span></span></p><pre>sudo port install gcc5
sudo port select --set gcc mp-gcc5
sudo port install openmpi-gcc5 +threads
sudo port select --set mpi openmpi-gcc5-fortran
sudo port install OpenBLAS +gcc5 +lapack
sudo port install scalapack +gcc5 +openmpi
sudo port install fftw-3 +gcc5 +openmpi</pre><p>Some components may be slightly newer versions than provided in the foss2015a toolchain, while the MPI library is an older version (at least when tested in September 2016).</p><p><span></span>If you also want a newer version of subversion that can integrate with the \"Native JavaHL connector\" in Eclipse, the following commands will install the appropriate packages:
</p><pre>sudo port install subversion
sudo port install subversion-javahlbindings
</pre><p>At the time of writing, this installed version 1,9,4 of subversion which has a compatielbe \"Native JavaHL connector\" in Eclipse.
</p><h2>Configurating Eclipse for other compilers</h2><p>Eclipse uses the PATH environment variable to find other software it uses, such as compilers but also some commands that give information on where certain libraries are stored or how they are configured. In a regular UNIX/Linux system, you'd set the variable in your shell configuration files (e.g., <code>.bash_profile</code> if you use the bash shell). This mechanism also works on OS X, but not for applications that are not started from the shell but from the Dock or by clicking on their icon in the Finder.
</p><p>Because of security concerns, Apple has made it increasingly difficult to define the path for GUI applications that are not started through a shell script.
</p><ul>
	<li>In 10.7 and earlier, one could define environment variables for GUI applications in <code>~/.MacOSX/environment.plist</code>. </li>
	<li>In 10.8 and 10.9 one had to modify the <code>Info.plist</code> file in the so-called application bundle.</li>
</ul><p>Both tricks are explained in the <a href=\"https://wiki.eclipse.org/PTP/photran/documentation\">Photran installation instructions on the Eclispe wiki</a>. However, in OS X 10.10 (Yosemite) neither mechanism works for setting the path.
</p><p>Our advise is to:
</p><ul>
	<li>Configure your bash shell so that you can find the gfortran executable and the corresponding gcc executable. (E.g., try <code>gfortran --version</code> and &lt;code&gt;gcc --version and check the output of these commands).</li>
	<li>Then start Eclipse also from a terminal window.
	<ul>
		<li>Use the full path, for the default install procedure this is very likely <code>/Application/eclipse/eclipse</code>,</li>
		<li>or add the path to Eclipse to the PATH environment variable (so you likely have to add <code>/Application/eclipse</code> to the path),</li>
		<li>or define an alias to start Eclipse, e.g., by adding the line<br>
		<code>alias start-eclipse='/Applications/eclipse/eclipse &gt;&/dev/null &'</code><br>
		to your <code>.bashrc</code> file. This line defines a new command <code>start-eclipse</code>.</li>
	</ul>
	This should work for all OS X versions.</li>
</ul>"
359,"","<ul>
	<li>Eclipse is a popular multi-platform Integrated Development Environment (IDE) very well suited for code development on clusters.
	<ul>
		<li>Read our <a href=\"/client/multiplatform/eclipse-intro\">Eclipse introduction</a> to find out why you should consider using Eclipse if you develop code and how to get it.</li>
		<li>You can use <a href=\"/client/multiplatform/eclipse-remote-editor\">Eclipse on the desktop as a remote editor for the cluster</a>.</li>
		<li>You can use <a href=\"/client/multiplatform/eclipse-vsc-subversion\">Eclipse on the desktop to access files in a subversion repository on the cluster</a>.</li>
		<li>You can combine the remote editor feature with version control from Eclipse, but some care is needed, and <a href=\"/client/multiplatform/eclipse-ptp-versioncontrol\">here's how to do it</a>.</li>
	</ul>
	</li>
	<li><a href=\"/client/multiplatform/desktop-access-vsc-subversion\">Subversion repositories on the VSC-servers (and other servers of course) can be accessed from the desktop.</a></li>
	<li>NX client technology is a computer program that handles remote X Window System connections. NX software is currently available for Windows, Mac OS X, Linux, Android and iOS.
	<ul>
		<li>You can use freeware <a href=\"/client/multiplatform/nx-start-guide\">NoMachine NX Client</a> to connect to HPC cluster in the GUI mode.</li>
	</ul>
	</li>
	<li>TurboVNC provides access to remote visualization applications. The software is currently available for Windows, Mac OS X and Linux.
	<ul>
		<li>You can use freeware <a href=\"/client/multiplatform/turbovnc\"> TurboVNC Client</a> to connect to the visualization nodes of HPC cluster.</li>
	</ul>
	</li>
</ul>"
361,"","<h2>Software development on clusters</h2><p><a href=\"https://www.eclipse.org/\">Eclipse</a> is an extensible IDE for program development. The basic IDE is written in Java for the development of Java programs, but can be extended through packages. The IDE was originally developed by IBM, but open-sourced and has become very popular. There are some interesting history tidbits on <a href=\"https://en.wikipedia.org/wiki/Eclipse_(software)\" target=\"_blank\">the WikiPedia entry for Eclipse</a>.
</p><h3>Some attractive features</h3><ul>
	<li>Multi-platform: Available for Windows, OS X and Linux, and works mostly the same on all these platforms.</li>
	<li>Support for C/C++ (via de CDT plugin) and Fortran (via the Photran plugin) development. This goes far beyond syntax coloring and includes things like code refactoring, build process management, etc.</li>
	<li>Support for the development of parallel applications on a cluster, including automatic synchronisation of the source files on your laptop with one or more cluster accounts. So you can easily do code development while off-line. Eclipse is heavily promoted (and actively developed) within the XSEDE collaboration of supercomputer centres in the USA.<br>
	If you have suitable compilers and libraries on your local machine, you may even be able to do part of the testing and debugging on your local machine, avoiding delays caused by the job queueing system. Another advantage of running Eclipse locally rather than on the cluster is that the GUI has all of the responsiveness of a local program, not influenced by network delays.</li>
	<li>It integrates with most popular version control system (offering a GUI to them also).</li>
</ul><h3>Caveat</h3><p>The documentation of the Parallel Tools Platform also tells you how to launch and debug programs on the cluster from the Eclipse IDE. However, this is for very specific cluster configurations and we cannot support this on our clusters at the moment. You can use features such as syncrhonised projects (where Eclipse puts a copy of the project files from your desktop on the cluster, and even synchronises back if you change them on the cluster) or opening a SSH shell from the IDE to directly enter commands on the cluster.
</p><h3>Release policy</h3><p>The eclipse project works with a \"synchronised release policy\". Major new versions of the IDE and a wide range of packages (including the C/C++ development package (CDT), Parallel Tools Platform (PTP) and the Fortran development package (Photran) which is now integrated in the PTP) occur simultaneously in June of each year which guarantees that there are no compatibility problems between packages if you upgrade your whole installation at once. Bug fixes are of course released in between version updates. Each version has its own code name and the code name has become more popular than the actual version number (as version numbers for the packages differ). E.g., the whole June 2013 release (base IDE and packages) is known as the \"Kepler\" release (version number 4.3), the June 2014 release as the \"Luna\" release (version number 4.4), the June 2015 as the \" Mars\"  release (version number 4.5) and the June 2016 release as \"Neon\".
</p><h2>Getting eclipse</h2><p>The best place to get Eclipse is the <a href=\"http://www.eclipse.org/downloads\" target=\"_blank\">the official Eclipse download page</a>. That site contains various pre-packaged versions with a number of extension packages already installed. The most interesting one for C/C++ or Fortran development on clusters is \"Eclipse for Parallel Application Developers\". The installation instructions depend on the machine you're installing on, but typically it is not more than unpacking some archive in the right location. You'll need a sufficiently recent Java IDE on your machine though. Instructions are available on the <a href=\"http://wiki.eclipse.org/Eclipse/Installation\" target=\"_blank\">Eclipse Wiki</a>.
</p><p>The CDT, Photran and PTP plugins integrate with compilers and libraries on your system. For Linux, it uses the gcc compiler on your system. On OS X it integrates with gcc and on Windows, you need to install <a href=\"https://www.cygwin.com/\" target=\"_blank\">Cygwin</a> and its gcc toolchain (it may also work with the <a href=\"http://www.mingw.org/\" target=\"_blank\">MinGW</a> and <a href=\"http://mingw-w64.org/doku.php\" target=\"_blank\">Mingw-64</a> gcc versions but we haven't verified this).
</p><p>The Eclipse documentation is <a href=\"https://help.eclipse.org/\">also available on-line</a>.
</p><ul>
	<li><a href=\"/client/macosx/eclipse-on-osx\">Check out some OS X specific issues here</a>.</li>
</ul><h2>Basic concepts</h2><ul>
	<li>
	A workspace is a place where eclipse stores a set of projects. It corresponds to a folder on file. The actual files of project can but do not need to be in that folder. However, all internal data that eclipse maintains will be. A user can have more than one workspace. Eclipse will ask at the start which workspace to use for the current session.
    Workspaces are not easily portable between computers. They are simply a way to organise your projects on your local computer.
	</li>
	<li>
	Each workspace can contain one or more projects. Each project is a collection of resources, e.g., C files or Fortran files, and typically has a releasable component that can be build from those resources, e.g., an executable. It is a good idea to use workspaces to group a number of related projects. A project also corresponds to a folder in the file system. That folder does not have to be contained in the workspace folder.
    Projects can be transported easily from one workstation to another.
	</li>
	<li>
	A perspective defines the (initial) layout of views and editors for a particular task. E.g., the C/C++ perspective shows an editor to edit C/C++-files and views to quickly navigate in the code, check definitions, etc. The Debug perspective is used to debug an application. The PTP also has a system monitoring perspective to monitor jobs.
	</li>
</ul><h2>Interesting bits in the documentation</h2><ul>
	<li>
	<a href=\"http://help.eclipse.org/neon/topic/org.eclipse.platform.doc.user/gettingStarted/intro/overview.htm?cp=0_0\">Basic Eclipse user guide with a getting started section</a>
	</li>
	<li>Parallel Tools Platform:
	<ul>
		<li>
		<a href=\"http://help.eclipse.org/neon/topic/org.eclipse.ptp.doc.user/html/localVsRemote.html?cp=62_4_1#sync\" target=\"_blank\">Parallel Development User Guide - Introduction to PTP Project Types - Synchronized</a> explains the advantages and disadvantages of the \" Synchronized project type\".
		</li>
		<li>
		The PTP also supports modules to configure the remote shell before actually building the application: <a href=\"http://help.eclipse.org/neon/topic/org.eclipse.ptp.doc.user/html/modules.html?cp=62_8\" target=\"_blank\">Parallel Development User Guide - Configuring Environment Modules</a>.
		</li>
		<li>
		<a href=\"http://wiki.eclipse.org/PTP\" target=\"_blank\">The PTP Wiki</a>
		</li>
	</ul>
	</li>
</ul>"
363,"","<h2>Prerequisites</h2><ul>
	<li>The user should be familiar with the basic use of the Eclipse IDE.</li>
	<li><a href=\"http://www.eclipse.org/\" target=\"_blank\">Eclipse IDE</a> has been installed on the user's desktop or laptop.<br>
	<em>We advise to install the bundle 'Eclipse for Parallel Application Developers' of a recent Eclipse release, e.g., the <a href=\"https://www.eclipse.org/neon/\">4.6/Neon (2016)</a> or the  <a href=\"https://www.eclipse.org/mars/\" target=\"_blank\">4.5/Mars (2015)</a> release, as they contain a lot of other useful tools, including the 'Remote System Explorer' used here. On older releases or other bundles you may have to install the 'Remote System Explorer End-User Runtime' and 'Remote System Explorer User Actions' components. This page was tested with the 4.6/Neon (2016) release and Helios (2010) release.</em></li>
	<li>The user should have a VSC account and be able to access it.</li>
</ul><h2>Installing additional components</h2><p>In order to use Eclipse as a remote editor, you may have to install two extra components: the \"Remote System Explorer End-User Runtime\" and the \"Remote System Explorer User Actions\". Here is how to do this:
</p><ol>
	<li>From Eclipse's 'Help' menu, select 'Install New Software...', the following dialog will appear:<img src=\"/assets/191\" alt=\"Eclipse Install New Software dialog\"></li>
	<li>From the 'Work with:' drop down menu, select 'Neon - http://download.eclipse.org/releases/neon' (or replace \"Neon\" with the name of the release that you are using). The list of available components is now automatically populated.</li>
	<li>From the category 'General Purpose Tools', select 'Remote System Explorer End-User Runtime' and 'Remote System Explorer User Actions'.</li>
	<li>Click the 'Next &gt;' button to get the installation details.</li>
	<li>Click the 'Next &gt;' button again to review the licenses.</li>
	<li>Select the 'I accept the terms of the license agreement' radio button.</li>
	<li>Click the 'Finish' button to start the download and installation process.</li>
	<li>As soon as the installation is complete, you will be prompted to restart Eclipse, do so by clicking the 'Restart Now' button.</li>
</ol><p>After restarting, the installation process of the necessary extra components is finished, and they are ready to be configured.
</p><h2>Configuration</h2><p>Before the new components can be used, some configuration needs to be done.
</p><p>Microsoft Windows users who use the PuTTY SSH client software should first prepare a private key for use with Eclipse's authentication system.  Users using the OpenSSH client on Microsoft Windows, Linux or MacOS X can skip this preparatory step.
</p><h3>Microsoft Windows PuTTY users only</h3><p>Eclipse's SSH components can not handle private keys generated with PuTTY, only OpenSSH compliant private keys. However, PuTTY's key generator 'PuTTYgen' (that was used to generate the public/private key pair in the first place) can be used to convert the PuTTY private key to one that can be used by Eclipse. See '<a href=\"/client/windows/keys-putty#PuTTY_to_OpenSSH\">How to convert a PuTTY key to OpenSSH format?</a>'
</p><p>Microsoft Windows PuTTY users should now proceed with the instructions for all users, below.
</p><h3>All users</h3><ol>
	<li>From the 'Window' menu ('Eclipse' menu on OS X), select 'Preferences'.</li>
	<li>In the category 'General', expand the subcategory 'Network Connections' and select 'SSH2'.</li>
	<li>Point Eclipse to the directory where the OpenSSH private key is stored that is used for authentication on the VSC cluster.  If that key is not called 'id_rsa', select it by clicking the 'Add Private Key...' button.</li>
	<li>Close the 'Preferences' dialog by clicking 'OK'.</li>
</ol><h2>Creating a remote connection</h2><p>In order to work on a remote system, a connection should be created first.
</p><ol>
	<li>From the 'Window' menu, select 'Open Perspective' and then 'Other...', a dialog like the one below will open (the exact contents depends on the components installed in Eclipse).<br>
	<img src=\"/assets/193\" alt=\"Eclipse Open Perspective dialog\"></li>
	<li>Select 'Remote System Explorer' from the list, and press 'OK', now the 'Remote Systems' view appears (at the left by default).</li>
	<li>In that view, right-click and select 'New' and then 'Connection' from the context menu, the 'New Connection' dialog should now appear.</li>
	<li>From the 'System type' list, select 'SSH Only' and press 'Next &gt;'.</li>
	<li>In the 'Host name' field, enter <em>vsc.login.node</em>, in the 'Connection Name' field, the same host name will appear automatically. The latter can be changed if desired. Optionally, a description can be added as well. Click 'Next &gt;' to continue.</li>
	<li>In the dialog 'Sftp Files' nothing needs to be changed, so just click 'Next &gt;'.</li>
	<li>In the dialog 'Ssh Shells' nothing needs to be changed either, so again just click 'Next &gt;'.</li>
	<li>In the dialog 'Ssh Terminals' (newer versions of Eclipse) nothing needs to be changed either, click 'Finish'.</li>
</ol><p>The new connection has now been created successfully. It can now be used.
</p><h2>Browsing the remote file system</h2><p>One of the features of Eclipse 'Remote systems' component is browsing a remote file system.
</p><ol>
	<li>In the 'Remote Systems' view, expand the 'Sftp Files' item under the newly created connection, 'My Home' and 'Root' will appear.</li>
	<li>Expand 'My Home', a dialog to enter your password will appear.</li>
	<li>First enter your user ID in the 'User ID' field, by default this will be your user name on your local desktop or laptop.  Change it to your VSC user ID.</li>
	<li>Mark the 'Save user ID' checkbox so that Eclipse will remember your user ID for this connection.</li>
	<li>Click 'OK' to proceed, leaving the 'Password' field blank.</li>
	<li>If the login node is not in your known_hosts file, you will be prompted about the authenticity of vsc.login.node, confirm that you want to continue connecting by clicking 'Yes'.</li>
	<li>If no know_hosts exists, Eclipse will prompt you to create one, confirm this by clicking 'Yes'.</li>
	<li>You will now be prompted to enter the passphrase for your private key, do so and click 'OK'. 'My Home' will now expand and show the contents of your home directory on the VSC cluster.</li>
</ol><p>Any file on the remote file system can now be viewed or edited using Eclipse as if it were a local file.
</p><p>It may be convenient to also display the content of your data directory (i.e., '$VSC_DATA'). This can be accomplished easily by creating a new filter.
</p><ol>
	<li>Right-click on the 'Sftp Files' item in your VSC connection ('Remote Systems' view), and select 'New' and then 'Filter...' from the context menu.</li>
	<li>In the 'Folder' field, type the path to your data directory (or use 'Browse...').  If you don't know where your data directory is located, type 'echo $VSC_DATA' on the login's command line to see its value. Leave all other fields and checkboxes to their default values and press 'Next &gt;'.</li>
	<li>In the field 'Filter name', type any name you find convenient, e.g., 'My Data'. leave the checkbox to its default value and click 'Finish'.</li>
</ol><p>A new item called 'My Data' now appeared under VSC's 'Sftp Files' and can be expanded to see the files in '$VSC_DATA'. Obviously, the same can be done for your scratch directory.
</p><h2>Using an Eclipse terminal</h2><p>The 'Remote Systems' view also allows to open a terminal to the remote connection. This can be used as an alternative to the PuTTY or OpenSSH client and may be convenient for software development (compiling, building and running programs) without leaving the Eclipse IDE.
</p><p>A new terminal can be launched from the 'Remote Systems' view by right-clicking the VSC connection's 'Ssh Shells' item and selecting 'Launch Terminal' or 'Launch...' (depending on the version of Eclipse). The 'Terminals' view will open (bottom of the screen by default).
</p><h2>Connecting/Disconnecting</h2><p>Once a connection has been created, it is trivial to connect to it again. To connect to a remote host, right-click on the VSC cluster connection in the 'Remote Systems' view, and select 'Connect' from the context menu.  You may be prompted to provide your private key's passphrase.
</p><p>For security reasons, it may be useful to disconnect from the VSC cluster when Eclipse is no longer used to browse or edit files. Although this happens automatically when you exit the Eclipse IDE, you may want to disconnect without leaving the applicaiton.
</p><p>To disconnect from a remote host, right-click on the VSC cluster connection in the 'Remote Systems' view, and select 'Disconnect' from the context menu.
</p><h2>Further information</h2><p>More information on Eclipse's capabilities to interact with remote systems can be found in the Eclipse help files that were automatically installed with the respective components. The information can be accessed by selecting 'Help Contents' from the 'Help' menu, and is available under 'RSE User Guide' heading.
</p>"
365,"","<h2>Prerequisites</h2><p>It is assumed that a recent version of the Eclipse IDE is installed on the desktop, and that the user is familiar with Eclipse as a development environment. The installation instructions were tested with the Helios (2010), 4.4/Luna (2014) and the 4.6/Neon (2016) release of Eclipse but may be slightly different for other versions.
</p><h2>Installation & setup</h2><p>In order to interact with subversion repositories, some extra plugins have to be installed in Eclipse.
</p><ol>
	<li>When you start Eclipse, note the code name of the version in the startup screen.</li>
	<li>From the 'Help' menu, select 'Install New Software...'.</li>
	<li>From the 'Work with' drop down menu, select 'Neon - http://download.eclipse.org/releases/neon' (where Neon is the name of the release, see the first step). This will populate the components list.</li>
	<li>Expand 'Collaboration' and check the box for 'Subversive SVN Team Provider' and click the 'Next &gt;' button.</li>
	<li>Click 'Next &gt;' in the 'Install Details' dialog.</li>
	<li>Indicate that you accept the license agreement by selecting the appropriate radio button and click 'Finish'.</li>
	<li>When Eclipse prompts you to restart it, do so by clicking 'Restart Now'</li>
	<li>An additional
     component is needed (an SVN Team Provider), however, To trigger the
     install, open the Eclipse \"Preferences\" menu (under the
     \"File\" menu, or under \"Eclipse\" on OS X) and go to
     \"Team\" and then \"SVN\"
	</li>
	<li><span></span>Select the tab \"SVN
     connector\"
	</li>
	<li><span></span>Then click on \"Get
     Connectors\" to open the 'Subversive Connectors Discovery'
     dialog.
	<br><em>You will not see this button if there is already a connector
      installed. If you need a different one, you can still install one via \"Install new
      software\" in the \"Help\" menu. Search for
      \"SVNKit\" for connectors that don't need any additional software on the system (our preference), or \"JavaHL\" for another family that connects to the original implementation. Proceed in a similar way as below (step 13).
	</em></li>
	<li>The easiest choice is to use one of the \"SVN Kit\" connectors as they do not require the installation of other software on your computer, but you have to chose the appropriate version. The subversion project tries to maintain compatibility between server and client from different versions as much as possible, so the version shouldn't matter too much. However, if on your desktop/laptop you'd like to mix between using svn through Eclipse and through another tool, you have to be careful that the SVN connector is compatible with the other SVN tools on your system. SVN Kit 1.8.12 should work with other SVN tools that support version 1.7-1.9 according to the documentation (we cannot test all combinations ourselves). <ol><li>In case you prefer to use the \"Native JavaHL\" connector instead, make sure that you have subversion binaries including the Java bindings installed on your system, and pick the matching version of the connector. Also see the <a href=\"http://subclipse.tigris.org/wiki/JavaHL\">JavaHL subclipse Wiki page</a> of the <a href=\"http://www.tigris.org/\">tigris.org community</a>.</li></ol></li>
	<li>
	Mark the checkbox next to the appropriate version of 'SVN Kit' and click 'Next &gt;'.</li>
	<li>The 'Install' dialog opens, offering to install two components, click 'Next &gt;'.</li>
	<li>The 'Install Details' dialog opens, click 'Next &gt;'.</li>
	<li>Accept the license agreement terms by checking the appropriate radio button in the 'Review Licenses' dialog and click 'Finish'.</li>
	<li>You may receive a warning that unsigned code is about to be installed, click 'OK' to continue the installation.</li>
	<li>Eclipse prompts you to restart to finish the installation, do so by clicking 'Restart Now'.</li>
</ol><p>Eclipse is now ready to interact with subversion repositories.
</p><h3><strong>Microsoft Windows PuTTY users only</strong></h3><p>Eclipse's SSH components can not handle private keys generated with PuTTY, only OpenSSH compliant private keys. However, PuTTY's key generator 'PuTTYgen' (that was used to generate the public/private key pair in the first place) can be used to convert the PuTTY private key to one that can be used by Eclipse.  See the section <a href=\"/client/windows/keys-putty#PuTTY_to_OpenSSH\">converting PuTTY keys to OpenSSH format</a> in the <a href=\"/client/windows/keys-putty\">page on generating keys with PyTTY</a> for details if necessary.
</p><h2>Checking out a project from a VSC cluster repository</h2><p>To check out a project from a VSC cluster repository, one uses the Eclipse 'Import' feature (don't ask...).
</p><pre>svn+ssh://userid@vsc.login.node/data/leuven/300/vsc30000/svn-repo
</pre><p><img src=\"/assets/199\" alt=\"Eclipse Checkout from SVN Repository Location\"><br>
	In the 'User' field, enter your VSC user ID.
</p><ul>
	<li>Switch to the 'SSH' tab of this dialog, and select 'Private key' for authentication. Use the 'Browse' button to find the appropriate private key file to authenticate on the VSC cluster. Note that this should be a private key in OpenSSH format. Also enter the passphrase for your private key. If you wish, you can store your passphrase here at this point, but this may pose a security risk.<br>
	<img src=\"/assets/201\" alt=\"Eclipse Checkout from SVN Repository Location SSH\"></li>
	<li>You will be prompted to select a resource to be checked out, click the 'Browse' button and select the project you want to check out. Remember that if you use the recommended repository layout, you will probably want to check out the project's 'trunk'. Click 'Finish'.<br>
	<img src=\"/assets/203\" alt=\"Eclipse Checkout from SVN Repository Location Select Resource\"></li>
	<li>The 'Check Out As' dialog offers several options, select the 'Checkout as a project with the name specified' and click 'Finish' and click 'Finish' to proceed with the check out.<br>
	<img src=\"/assets/205\" alt=\"Eclipse Checkout from SVN as...\"></li>
</ul><p>Note that Eclipse remembers repository URLs, hence checking out another project from the same repository will skip quite a number of the steps outlined above.
</p><h2>Work cycle</h2><p>The development cycle from the point of view of version control is exactly the same as that for a command line subversion client. Once a project has been checked out or placed under version control, all actions can be performed by right clicking on the project or specific files in the 'Project Explorer' view and choosing the appropriate action from the 'Team' entry in the context menu. The menu items are fairly self-explanatory, but you may want to read the section on <a href=\"/client/windows/tortoisesvn\">TortoiseSVN</a> since Eclipse's version control interface is very akin to the former.
</p><p>Note that files and directories displayed in the 'Project Explorer' view are now decorated to indicate version control status. A '&gt;' preceeding a file or directory's name indicate that it has been modified since the last update. A new file not yet under version control has a '?' embedded in its icon.
</p><p>When a project is committed, subversive opens a dialog to enter an appropriate comment, and offers to automatically add new files to the repository. Note that Eclipse also offers to commit its project settings, e.g., the '.project' file. Whether or not you wish to store these settings in the repository  depends on your setup, but probably you don't.
</p>"
367,"","<p><em>If you're not familiar with Eclipse, read <a href=\"/client/multiplatform/eclipse-intro\">our introduction page</a> first.</em></p><p>Eclipse also supports several version control systems out of the box or through optional plug-ins.</p><p>The PTP (Parallel Tools Platform) strongly encourages a model where you run eclipse locally on your workstation and let Eclipse synchronise the project files with your cluster account. If you want to use version control in this scenario, the PTP manual advises to put your <em>local</em> files under version control (which can be done through Eclipse also) and synchronise that with some remote repository (e.g., one of the hosting providers), and to not put the automatically synchronised version of the code that you use for compiling and running on the cluster also under version control. In other words,</p><ul>
    <li>
    The version control system is used to version manage your files on your local workstation,
    </li>
    <li>
    And Eclipse PTP is then used to manage the files on the cluster.
    </li>
</ul><p>If you still want to use the cluster file space as a remote repository, we strongly recommend that you do this in a different directory from where you let Eclipse synchronise the files, and don't touch the files in that repository directly.</p><h2>For experts</h2><p>The synchronised projects feature in Eclipse internally uses the Git version control system to take care of the synchronisation. That's also the reason why the Parallel Software Development bundle of Eclipse comes with the EGit plug-in included. It does this however in a way that does not interfere with regular git operations. In both your local and remote project directory, you'll find a hidden .ptp-sync directory which in fact is a regular git repository, but stored in a different subdirectory rather than the standard .git subdirectory. So you can still have a standard Git repository besides it and they will not interfere if you follow the guidelines on this page.</p>"
369,"","<h2>
Prerequisites</h2>
<ul>
	<li>
	You should be familiar with subversion, either by reading the online subversion book, or, for the impatient, the <a href=\"/cluster-doc/development/subversion\">documentation page on creating and using a subversion repository on the VSC clusters</a>.</li>
	<li>
	You should have (access to) a subversion repository on a VSC cluster, or any other repository (you should however be able to commit changes).</li>
	<li>
	A command line client for subversion, which is available all operating systems. For Windows, you can use the <a href=\"https://www.cygwin.com/\" target=\"_blank\">Cygwin unix emulation layer</a> and the subversion packages included in it.</li>
</ul>
<h2>Environment & general use</h2>
<p>
	All operations introduced in the <a href=\"/cluster-doc/development/subversion\">documentation page on using subversion repositories on the VSC clusters</a> work as illustrated therein.  The repository's URI can be conveniently assigned to an environment variable
</p>
<pre>$ export SVN=\"svn+ssh://userid@vsc.login.node/data/leuven/300/vsc30000/svn-repo\"
</pre>
<p>
	where userid should be replaced by your own VSC user ID, and vsc.login.node to the appropriate login node for the cluster the repository is on.  In the above, it is assumed that the SVN repository you are going to use is in your VSC data directory (here shown for user vsc30000) and is called svn-repo.  This should be changed appropriately.
</p>
<h2>
Checking out a project from a VSC cluster repository</h2>
<p>
	To check out the simulation project to a directory 'simulation' on your desktop, simply type:
</p>
<pre>$ svn checkout  ${SVN}/simulation/trunk  simulation
</pre>
<p>
	The passphrase for your private key used to authenticate on the VSC cluster will be requested.
</p>
<p>
	Once the project is checked out, you can start editing or adding files and directories, committing your changes when done.
</p>
<h2>
Importing a local project into the VSC cluster repository</h2>
<p>
	Importing a project directory that is currently on your desktop and not on the VSC cluster is also possible, again by simply modifying the URLs in the previous section appropriately. Suppose the directory on your desktop is 'calculation', the steps to take are the following:
</p>
<pre>$ svn mkdir -m 'calculation: creating dirs' --parents   \\
            $SVN/calculation/trunk    \\
            $SVN/calculation/branches \\
            $SVN/calculation/tags
$ svn import -m 'calculation: import' \\
             calculation              \\
             $SVN/calculation/trunk
</pre>
<p>
	Note that each time you access the repository, you need to authenticate, which gets tedious pretty soon. Using ssh-agent may be considered to simplify life, see, e.g., a <a href=\"http://novosial.org/openssh/publickey-auth/index.html\" target=\"_blank\">short tutorial</a> on a possible setup.
</p>
<h2>
Links</h2>
<ul>
	<li>
	<a href=\"https://subversion.apache.org\" target=\"_blank\">Apache Subversion</a>, with documentation, source and binary packages for various operating systems.</li>
	<li>
	<a href=\"https://www.cygwin.com\" target=\"_blank\">Cygwin</a>, a UNIX emulation layer for Windows. Search for subversion in the list of packages when running the setup program.</li>
</ul>"
371,"","<h2>Installing NX NoMachine client</h2>
<ul>
	<li>
	Download the enterprise version of the client from  <a href=\"https://www.nomachine.com/download-enterprise#NoMachine-Enterprise-Client\" target=\"_blank\">https://www.nomachine.com/download-enterprise#NoMachine-Enterprise-Client</a>
	</li>
	<li>
	Continue with Configuration of your NoMachine NX client.
	</li>
</ul>
<h2>NoMachine NX Client Configuration Guide</h2>
<ol>
	<li>NoMachine NX requires keys in OpenSSH format, therefore the existing key needs to be <a href=\"/client/windows/keys-putty#PuTTY_to_OpenSSH\">converted into OpenSSH format</a> if you're working on Windows and using PuTTY.</li>
	<li>Start the NoMachine client and press twice continue to see the screen with connection. Press <strong>New</strong> to create a new connection.</li>
	<li>Change the Protocol to <strong>SSH</strong>.</li>
	<li>Choose the hostname:<strong></strong><strong></strong><strong> </strong></li>
	<ul>
		<li><strong>for ThinKing (Tier-2): nx.hpc.kuleuven.be</strong> and port <strong>22</strong>. </li>
		<li><strong>for BrENIAC (Tier-1): login2-tier1.hpc.kuleuven.be</strong> and port <strong>22</strong>.<strong></strong></li>
		<li>If you experience problems with connection please switch to Protocol NX and port 4000.</li>
	</ul>
	<li>Choose the authentication <strong>Use the system login.</strong></li>
	<li>Choose the authentication method <strong>Private key.</strong></li>
	<li>Browse your private key. This should be in OpenSSH format (not .ppk).
	<ul>
		<li>For Android users it is easy to transfer your key and save it in the chosen location with the Box (KU Leuven) or Dropbox (UHasselt) apps.</li>
		<li>For iOS users (iPad running iOS 5 or later) it is possible to transfer the key with iTunes. Connect your device through iTunes, go to the connected device, choose the \"apps\" tab, scroll down to \"file sharing\". Select the NoMachine client and add files to NoMachine Documents. Remember to Sync your device.</li>
		<li>Browse your file on a mobile device from the given location.</li>
	</ul></li>
	<li>Choose the option <span b=\"\">Don’t use proxy for the network connection.</span></li>
	<li>Give the name to your connection, e.g. <em>Connection to nx.hpc.kuleuven.be</em>. You can optionally create the link to that connection on your desktop. Click the \"Done\" button to finish configuration.</li>
	<li>Choose the just created connection and press \"Connect\".</li>
	<li>Enter your username (<strong>vsc-account</strong>) and <strong>passphrase</strong> for your private key and press \"ok\".</li>
	<li>If you are creating for the first time choose <strong>New desktop</strong>. Otherwise please go to step 16 for instructions how to reconnect to your session.</li>
	<li>Choose <strong>Create a new virtual desktop</strong> and continue. Each user is allowed to have a maximum 5 desktops open.</li>
	<li>Read the useful information regarding your session displayed on several screens. This step is very important in case of mobile devices – once you miss the instructions it is not so easy to figure out how to operate NoMachine on your device. You can optionally choose not to show these messages again.</li>
	<li>Once connected you will see the virtual Linux desktop.</li>
	<li>When<strong> </strong><strong>reconnecting</strong> choose your desktop from all the listed ones. If there are too many you can use the option <strong>find a user or a desktop</strong> and type your username (vsc-account). Once you found your desktop press <strong>connect</strong>.</li>
	<li>You will be prompted about the screen resolution (<strong>Change the server resolution to match the client when I connect</strong>) which can be changed to match the client when you connect. It is a recommended setup as your session will correspond to your actual device resolution. While reconnection from a different device (e.g. mobile device) it is highly recommended to change the resolution.</li>
</ol>
<p>For more detailed information about the configuration process please refer to the <a target=\"_blank\" href=\"https://www.vscentrum.be/assets/1187\">short video</a> (ThinKing configuration) showing the installation and configuration procedure step-by-step or to the <a href=\"/assets/1293\">document containing graphical instructions</a>.
</p>
<h2>How to start using NX on ThinKing?</h2>
<ol>
	<li>Once your desktop is open, you can use all available GUI designed software that is listed within the Applications menu. Software is divided into several groups:
	<ul>
		<li>Accessories (e.g. Calculator, Character Map, Emacs, Gedit, GVim),</li>
		<li>Graphics (e.g. gThumb Image Viewer, Xpdf PDF Viewer),</li>
		<li>Internet (e.g. Firefox with pdf support, Filezilla),</li>
		<li><strong>HPC</strong> (modules related to HPC use: <strong>Computation</strong> sub-menu with Matlab, RStudio and SAS, <strong>Visualisation</strong> sub-menu with Paraview and VisIt),</li>
		<li>Programming (e.g. Meld Diff Viewer),</li>
		<li>System tools (e.g. File Browser, Terminal).</li>
	</ul></li>
	<li>Running the applications in the text mode requires having a terminal open. To launch the terminal please go to Applications -&gt; System tools -&gt; Terminal. From Terminal all the commands available on regular login node can be used.</li>
	<li>Some more information can be found on <a href=\"/assets/197\">slides from our lunchbox session</a>. In the slides you can find the information how to<strong> connect the local HDD</strong> to the NX session for easier transfer of data between the cluster and your local computer.</li>
</ol>
<h2>Attached documents</h2>
<ul>
	<li><a href=\"/assets/1293\">Instructions with screenshots</a></li>
	<li><a href=\"/assets/197\">Slides from the lunchbox session</a></li>
</ul>"
373,"","<p>There are two possibilities
</p><ol>
	<li>
	You can copy your private key from the machine where you generated the key to the other computers you want to use to access the VSC clusters.
    If you want to use both PuTTY on Windows and the tradinional OpenSSH client on OS X or Linux (or Windows with Cygwin) and chose for this scenario, you should generate the key using PuTTY and then export it in OpenSSH format as explained on 
	<a href=\"/client/windows/keys-putty\">the PuTTY pages</a>.
	</li>
	<li>Alternatively, you can generate another keypair for the second machine following the instructions for your respective client (<a href=\"/client/windows/keys-putty\">Windows</a>, <a href=\"/client/macosx/keys-openssh\">macOS/OS X</a>, <a href=\"/client/linux/keys-openssh\">Linux</a>) and then upload the new public key to your account:
	<ol>
		<li>Go to <a href=\"https://account.vscentrum.be/\">the account management web site account.vscentrum.be</a></li>
		<li>Choose \"Edit account\"</li>
		<li>And then add the public key via that page. It can take half an hour before you can use the key.</li>
	</ol>
	</li>
</ol><p>We prefer the second scenario, in particular if you want to access the clusters from a laptop or tablet, as these are easily stolen. In this way, all you need to do if your computer is stolen or your key may be compromised in another way, is to delete that key on the account website (via \"Edit account\"). You can continue to work on your other devices.
</p>"
375,"","<p>Data on the VSC clusters can be stored in several locations, depending on the size and usage of these data. Following locations are available:
</p><ul>
	<li><a href=\"#Home\">Home directory</a>
	<ul>
		<li>Location available as $VSC_HOME</li>
		<li>The data stored here should be relatively small, and not generating very intense I/O during jobs. <br>
		Its main purpose is to stora all kinds of configuration files are stored, e.g., ssh-keys, .bashrc, or Matlab, and Eclipse configuration, ...</li><li>Performance is tuned for the intended load: reading configuration files etc.</li><li>Readable and writable on all VSC sites.</li><li>As best practice, the permissions on your home directory should be only for yourself, i.e., 700. To share data with others, use the data directory.</li>
	</ul>
	</li>
	<li><a href=\"#Data\">Data directory</a>
	<ul>
		<li>Location available as $VSC_DATA</li>
		<li>A bigger 'workspace', for program code, datasets or results that must be stored for a longer period of time.</li><li>There is no performance guarantee; depending on the cluster, performance may not be very high.</li><li>Readable and writable on all VSC sites.</li>
	</ul>
	</li>
	<li><a href=\"#Scratch\">Scratch directories</a>
	<ul>
		<li>Several types exist, available in $VSC_SCRATCH_XXX variables</li>
		<li>For temporary or transient data; there is typically no backup for these filesystems, and 'old' data may be removed automatically.</li>
		<li>Currently, $VSC_SCRATCH_NODE, $VSC_SCRATCH_SITE and $VSC_SCRATCH_GLOBAL are defined, for space that is available per node, per site, or globally on all nodes of the VSC (currently, there is no real 'global' scratch filesystem yet).</li><li>These file systems are not exported to other VSC sites.</li>
	</ul>
	</li>
</ul><p>Since these directories are not necessarily mounted on the same locations over all sites, you should always (try to) use the environment variables that have been created.
</p><p>Quota is enabled on the three directories, which means the amount of data you can store here is limited by the operating system, and not just by the capacity of the disk system, to prevent that the disk system fills up accidentally. You can see your current usage and the current limits with the appropriate quota command as explained on the <a href=\"/cluster-doc/account-management/managing-disk-usage\">page on managing disk space</a>. The actual disk capacity, shared by <i>all</i> users, can be found on the  <a href=\"/infrastructure/hardware\">Available hardware</a> page.
</p><p>You will only receive a warning when you reach the soft limit of either quota. You will only start losing data when you reach the hard limit. Data loss occurs when you try to save new files: this will not work because you have no space left, and thus you will lose these new files. You will however not be warned when data loss occurs, so keep an eye open for the general quota warnings! The same holds for running jobs that need to write files: when you reach your hard quota, jobs will crash.
</p><h2><a name=\"Home\"></a>Home directory</h2><p>This directory is where you arrive by default when you login to the cluster. Your shell refers to it as \"~\" (tilde), or via the environment variable $VSC_HOME.
</p><p>The data stored here should be relatively small (e.g., no files or directories larger than a gigabyte, although this is not imposed automatically), and usually used frequently. The typical use is storing configuration files, e.g., by Matlab, Eclipse, ...
</p><p>The operating system also creates a few files and folders here to manage your account. Examples are:
</p><table class=\"prettytable\">
<tbody>
<tr>
	<td>.ssh/
	</td>
	<td>This directory contains some files necessary for you to login to the cluster and to submit jobs on the cluster. Do not remove them, and do not alter anything if you don't know what you're doing!
	</td>
</tr>
<tr>
	<td>.profile<br>.bash_profile
	</td>
	<td>This script defines some general settings about your sessions,
	</td>
</tr>
<tr>
	<td>.bashrc
	</td>
	<td>This script is executed every time you start a session on the cluster: when you login to the cluster and when a job starts. You could edit this file to define variables and aliases.  However, note that loading modules is strongly discouraged.
	</td>
</tr>
<tr>
	<td>.bash_history
	</td>
	<td>This file contains the commands you typed at your shell prompt, in case you need them again.
	</td>
</tr>
</tbody>
</table><h2><a name=\"Data\"></a>Data directory</h2><p>In this directory you can store all other data that you need for longer terms. The environment variable pointing to it is $VSC_DATA. There are no guarantees about the speed you'll achieve on this volume. I/O-intensive programs should not run directly from this volume (and if you're not sure, whether your program is I/O-intensive, don't run from this volume).</p><p>This directory is also a good location to share subdirectories with other users working on the same research projects.</p><h2><a name=\"Scratch\"></a>Scratch space</h2><p>To enable quick writing from your job, a few extra file systems are available on the work nodes. These extra file systems are called scratch folders, and can be used for storage of temporary and/or transient data (temporary results, anything you just need during your job, or your batch of jobs).
</p><p>You should remove any data from these systems after your processing them has finished. There are no guarantees about the time your data will be stored on this system, and we plan to clean these automatically on a regular base. The maximum allowed age of files on these scratch file systems depends on the type of scratch, and can be anywhere between a day and a few weeks. We don't guarantee that these policies remain forever, and may change them if this seems necessary for the healthy operation of the cluster.
</p><p>Each type of scratch has his own use:
</p><ul>
	<li><strong>Shared scratch ($VSC_SCRATCH)</strong><br>To allow a job running on multiple nodes (or multiple jobs running on separate nodes) to share data as files, every node of the cluster (including the login nodes) has access to this shared scratch directory. Just like the home and data directories, every user has its own scratch directory. Because this scratch is also available from the login nodes, you could manually copy results to your data directory after your job has ended. Different clusters on the same site may or may not share the scratch space pointed to by $VSC_SCRATCH.<br>This scratch space is provided by a central file server that contains tens or hundreds of disks. Even though it is shared, it is usually very fast as it is very rare that all nodes would do I/O simultaneously. It also implements a parallel file system that allows a job to do parallel file I/O from multiple processes to the same file simultaneously, e.g., through MPI parallel I/O. <br>For most jobs, this is the best scratch system to use.</li><li><strong>Site scratch ($VSC_SITE_SCRATCH)</strong><br>A variant of the previous one, may or may not be the same. On clusters that have access to both a cluster-local scratch and site-wide scratch file system, this variable will point to the site-wide available scratch volume. On other sites it will just point to the same volume as $VSC_SCRATCH.</li><li><strong>Node scratch ($VSC_SCRATCH_NODE)</strong><br>
	Every node has its own scratch space, which is completely separated from the other nodes. On many cluster nodes, this space is provided by a local hard drive or SSD. Every job automatically gets its own temporary directory on this node scratch, available through the environment variable $TMPDIR. $TMPDIR is guaranteed to be unique for each job.<br>
	Note however that when your job requests multiple cores and these cores happen to be in the same node, this $TMPDIR is shared among the cores! Also, you cannot access this space once your job has ended. And on a supercomputer, a local hard disk may not be faster than a remote file system which often has tens or hundreds of drives working together to provide disk capacity.</li>
	<li><strong>Global scratch ($VSC_SCRATCH_GLOBAL)<br></strong>We may or may not implement a VSC-wide scratch volume in the future, and the environment variable VSC_SCRATCH_GLOBAL is reserved to point to that scratch volume.  Currently is just points to the same volume as $VSC_SCRATCH or $VSC_SITE_SCRATCH.</li>
</ul>"
377,"","<p>BEgrid has its <a href=\"http://www.begrid.be/\" target=\"_blank\">own documentation web site as it is a project at the federal level</a>. Some useful links are:
</p><ul>
	<li><a href=\"http://wiki.begrid.be/\" target=\"_blank\">BEgrid Wiki</a></li>
	<li><a href=\"https://edms.cern.ch/ui/file/722398/1.2/gLite-3-UserGuide.pdf\">gLite 3.1 User Guide (PDF, op CERN)</a><br>gLite is the grid middleware used on BEgrid.</li>
	<li><a href=\"http://www.begrid.be/index.php?module=webpage&id=16\" target=\"_blank\">Other related links on the BEgrid web site.</a></li>
</ul>"
381,"","<p>This is just some random text. Don't be worried if the remainder of this paragraph sounds like Latin to you cause it is Latin. Cras mattis consectetur purus sit amet fermentum. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Sed posuere consectetur est at lobortis. Morbi leo risus, porta ac consectetur ac, vestibulum at eros. Cras mattis consectetur purus sit amet fermentum. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Sed posuere consectetur est at lobortis. Morbi leo risus, porta ac consectetur ac, vestibulum at eros.
</p>"
385,"","<p>What I tried to do with the \"Asset\" box in the right column:</p><ul><li>I included two pictures from our asset toolbox. What is shown are square thumbnails of the pictures.</li><li>I also included two PDFs that have no picture attached to them. They simply don't show up.</li></ul><p><br></p>"
387,"","<h2>Inline code with &lt;code&gt;...&lt;/code&gt;</h2><p>We used inline code on the old vscentrum.be to clearly mark system commands etc. in text.
</p><ul>
	<li>For this we used the &lt;code&gt; tag.</li>
	<li>There was support in the editor to set this tag</li>
	<li>It doesn't seem to work properly in the current editor. If the fragment of code contains a slash (/), the closing tag gets omitted.</li>
</ul><p>Example: At UAntwerpen you'll have to use <code>module avail MATLAB</code> and 
	<code>module load MATLAB/2014a</code> respectively.
</p><p>However, If you enter both &lt;code&gt;-blocks on the same line in a HTML file, the editor doesn't process them well: <code>module avail MATLAB</code> and &lt;code&gt;module load MATLAB.
</p><p>En dit is <code>code inline</code> als test.
</p><p>En dit dit wordt een nieuw pre-blok:
</p><pre>#!/bin/bash
echo \"Hello, world!\"
</pre><h2>Code in &lt;pre&gt;...&lt;/pre&gt;</h2><p>This was used a lot on the old vscentrum.be site to display fragments of code or display output in a console windows.
</p><ul>
	<li>Readability of fragments is definitely better if a fixed width font is used as this is necessary to get a correct alignment.</li>
	<li>Formatting is important: Line breaks should be respected. The problem with the CMS seems to be that the editor respects the line breaks, the database also stores them as I can edit the code again, but the CMS removes them when generating the final HTML-page as I don't see the line breaks again in the resulting HTML-code that is loaded into the browser.</li>
</ul><pre>#!/bin/bash -l
#PBS -l nodes=1:nehalem
#PBS -l mem=4gb
module load matlab
cd $PBS_O_WORKDIR
...
</pre><h2>The &lt;code&gt; style in the editor</h2><p>In fact, the Code style of the editor works on a paragraph basis and all it does is put the paragraph between &lt;pre&gt; and &lt;/pre&gt;-tags, so the problem mentioned above remains. The next text was edited in WYSIWIG mode:
</p><pre>#!/bin/bash -l
#PBS -l nodes=4:ivybridge
...
</pre><p>Another editor bug is that it isn't possible to switch back to regular text mode at the end of a code fragment if that is at the end of the text widget: The whole block is converted back to regular text instead and the formatting is no longer shown.
</p><p>Een Workaround is misschien meerdere &lt;pre&gt;-blokken gebruiken?
</p><pre>#!/bin/bash -l
</pre><pre>#PBS -l nodes=4:ivybridge
</pre><pre>...
</pre><p>Neen, want dan krijg je meerdere grijze vakken...
</p><p>En met &lt;br&gt; en de &lt;code&gt;-tag?
</p><p><code>#! /bin/bash -l<br>#PBS -l nodes=4:ivybridge<br>...
	</code>
</p><p>Ook dit is niet ideaal, want alles staat niet aaneenin een kader, maar het is beter dan niets...
</p>"
395,"Tier-1 infrastructure","<p>Our first Tier-1 cluster, Muk, was installed in the spring of 2012 and became operationa a few months later. This system is primarily optimised for the processing of large parallel computing tasks that need to have a high-speed interconnect.<o:p></o:p>
</p>"
399,"","<p>The list below gives an indication of which (scientific) software, libraries and compilers are available on TIER1 on 1 December 2014. For each package, the available version(s) is shown as well as (if applicable) the compilers/libraries/options with which the software was compiled. Note that some software packages are only available when the end-user demonstrates to have valid licenses to use this software on the TIER1 infrastructure of Ghent University.</p><ul>
	<li>ABAQUS/6.12.1-linux-x86_64</li>
	<li>ALADIN/36t1_op2bf1-ictce-4.1.13</li>
	<li>ALADIN/36t1_op2bf1-ictce-4.1.13-strict</li>
	<li>Allinea/4.1-32834-Redhat-6.0-x86_64</li>
	<li>ANTLR/2.7.7-ictce-4.1.13</li>
	<li>APR/0.9.18-ictce-4.1.13</li>
	<li>APR/1.5.0-ictce-4.1.13</li>
	<li>APR/1.5.0-ictce-5.5.0</li>
	<li>APR-util/1.3.9-ictce-4.1.13</li>
	<li>APR-util/1.5.3-ictce-4.1.13</li>
	<li>APR-util/1.5.3-ictce-5.5.0</li>
	<li>ASE/3.6.0.2515-ictce-4.1.13-Python-2.7.3</li>
	<li>Autoconf/2.69-ictce-4.1.13</li>
	<li>BEAGLE/20130408-ictce-4.0.6</li>
	<li>beagle-lib/20120124-ictce-4.1.13</li>
	<li>BEDTools/2.17.0-ictce-4.1.13</li>
	<li>BEDTools/v2.17.0-ictce-4.1.13</li>
	<li>Bison/2.5-ictce-4.1.13</li>
	<li>Bison/2.6.5-ictce-4.1.13</li>
	<li>Bison/2.7.1-ictce-5.5.0</li>
	<li>Bison/2.7-ictce-4.1.13</li>
	<li>Bison/2.7-ictce-5.5.0</li>
	<li>Bison/3.0.2-intel-2014b</li>
	<li>BLACS/1.1-gompi-1.1.0-no-OFED</li>
	<li>Boost/1.51.0-ictce-4.1.13-Python-2.7.3</li>
	<li>Boost/1.55.0-ictce-5.5.0-Python-2.7.6</li>
	<li>Bowtie/1.0.0-ictce-4.1.13</li>
	<li>Bowtie2/2.0.2-ictce-4.1.13</li>
	<li>Bowtie2/2.1.0-ictce-5.5.0</li>
	<li>BWA/0.6.2-ictce-4.1.13</li>
	<li>bzip2/1.0.6-ictce-4.1.13</li>
	<li>bzip2/1.0.6-ictce-5.5.0</li>
	<li>bzip2/1.0.6-iomkl-4.6.13</li>
	<li>CDO/1.6.2-ictce-5.5.0</li>
	<li>CDO/1.6.3-ictce-5.5.0</li>
	<li>Circos/0.64-ictce-5.5.0-Perl-5.18.2</li>
	<li>CMake/2.8.10.2-ictce-4.0.6</li>
	<li>CMake/2.8.10.2-ictce-4.1.13</li>
	<li>CMake/2.8.12-ictce-5.5.0</li>
	<li>CMake/2.8.4-ictce-4.1.13</li>
	<li>CP2K/20130228-ictce-4.1.13</li>
	<li>CP2K/20131211-ictce-5.5.0</li>
	<li>CP2K/2.5.1-intel-2014b-psmp</li>
	<li>Cufflinks/2.1.1-ictce-4.1.13</li>
	<li>Cufflinks/2.1.1-ictce-5.5.0</li>
	<li>cURL/7.28.1-ictce-4.1.13</li>
	<li>cURL/7.28.1-ictce-5.5.0</li>
	<li>cURL/7.33.0-ictce-4.1.13</li>
	<li>cURL/7.34.0-ictce-5.5.0</li>
	<li>cutadapt/1.3-ictce-4.1.13-Python-2.7.3</li>
	<li>Cython/0.17.4-ictce-4.1.13-Python-2.7.3</li>
	<li>Cython/0.19.2-ictce-5.5.0-Python-2.7.6</li>
	<li>DB/4.7.25-ictce-4.1.13</li>
	<li>DBD-mysql/4.023-ictce-4.1.13-Perl-5.16.3</li>
	<li>Doxygen/1.8.1.1-ictce-4.1.13</li>
	<li>Doxygen/1.8.2-ictce-4.1.13</li>
	<li>Doxygen/1.8.3.1-ictce-4.1.13</li>
	<li>Doxygen/1.8.3.1-ictce-5.5.0</li>
	<li>Doxygen/1.8.6-ictce-5.5.0</li>
	<li>e2fsprogs/1.42.7-ictce-4.1.13</li>
	<li>EasyBuild/1.10.0(default)</li>
	<li>EasyBuild/1.7.0</li>
	<li>EasyBuild/1.8.2</li>
	<li>EasyBuild/1.9.0</li>
	<li>ed/1.9-ictce-4.1.13</li>
	<li>Eigen/3.1.1-ictce-4.1.13</li>
	<li>Eigen/3.2.0-ictce-5.5.0</li>
	<li>ESMF/6.1.1-ictce-4.1.13</li>
	<li>ESMF/6.1.1-ictce-5.5.0</li>
	<li>expat/2.1.0-ictce-4.1.13</li>
	<li>expat/2.1.0-ictce-5.5.0</li>
	<li>fastahack/20110215-ictce-4.1.13</li>
	<li>FFTW/3.3.1-gompi-1.1.0-no-OFED</li>
	<li>FFTW/3.3.3-ictce-4.1.13</li>
	<li>FFTW/3.3.3-ictce-4.1.13-single</li>
	<li>FFTW/3.3.3-ictce-4.1.14</li>
	<li>FFTW/3.3.3-ictce-4.1.14-single</li>
	<li>FFTW/3.3.3-iomkl-4.6.13-single</li>
	<li>FFTW/3.3.4-intel-2014b</li>
	<li>flex/2.5.35-ictce-4.1.13</li>
	<li>flex/2.5.37-ictce-4.1.13</li>
	<li>flex/2.5.37-ictce-5.5.0</li>
	<li>flex/2.5.37-intel-2014b</li>
	<li>flex/2.5.39-intel-2014b</li>
	<li>FLTK/1.3.2-ictce-4.1.13</li>
	<li>FLUENT/14.5</li>
	<li>FLUENT/15.0.7</li>
	<li>fontconfig/2.11.1-ictce-5.5.0</li>
	<li>freetype/2.4.11-ictce-4.1.13</li>
	<li>freetype/2.4.11-ictce-5.5.0</li>
	<li>g2clib/1.4.0-ictce-4.1.13</li>
	<li>g2clib/1.4.0-ictce-5.5.0</li>
	<li>g2lib/1.4.0-ictce-4.1.13</li>
	<li>g2lib/1.4.0-ictce-5.5.0</li>
	<li>Gaussian/g09_B.01-ictce-4.1.13-amd64-gpfs-I12</li>
	<li>Gaussian/g09_D.01-ictce-5.5.0-amd64-gpfs</li>
	<li>GCC/4.6.3</li>
	<li>GCC/4.8.3</li>
	<li>GD/2.52-ictce-5.5.0-Perl-5.18.2</li>
	<li>GDAL/1.9.2-ictce-4.1.13</li>
	<li>GDAL/1.9.2-ictce-5.5.0</li>
	<li>GLib/2.34.3-ictce-4.1.13</li>
	<li>glproto/1.4.16-ictce-4.1.13</li>
	<li>GMAP/2013-11-27-ictce-5.5.0</li>
	<li>gnuplot/4.4.4-ictce-4.1.13</li>
	<li>gompi/1.1.0-no-OFED</li>
	<li>Greenlet/0.4.0-ictce-4.1.13-Python-2.7.3</li>
	<li>grib_api/1.9.18-ictce-4.1.13</li>
	<li>GROMACS/4.6.5-ictce-5.5.0-hybrid</li>
	<li>GROMACS/4.6.5-ictce-5.5.0-mpi</li>
	<li>GSL/1.16-ictce-4.1.13</li>
	<li>GSL/1.16-ictce-5.5.0</li>
	<li>gzip/1.4</li>
	<li>h5py/2.1.0-ictce-4.1.13-Python-2.7.3</li>
	<li>Hadoop/0.9.9-rdma</li>
	<li>Hadoop/2.0.0-cdh4.4.0</li>
	<li>Hadoop/2.0.0-cdh4.5.0</li>
	<li>Hadoop/2.3.0-cdh5.0.0</li>
	<li>Hadoop/2.x-0.9.1-rdma</li>
	<li>hanythingondemand/2.1.1-ictce-5.5.0-Python-2.7.6</li>
	<li>hanythingondemand/2.1.4-ictce-5.5.0-Python-2.7.6</li>
	<li>HDF/4.2.8-ictce-4.1.13</li>
	<li>HDF/4.2.8-ictce-5.5.0</li>
	<li>HDF5/1.8.10-ictce-4.1.13-gpfs-mt</li>
	<li>HDF5/1.8.10-ictce-4.1.13-parallel-gpfs</li>
	<li>HDF5/1.8.10-ictce-5.5.0-gpfs</li>
	<li>HDF5/1.8.10-ictce-5.5.0-gpfs-mt</li>
	<li>HDF5/1.8.12-ictce-5.5.0</li>
	<li>HDF5/1.8.9-ictce-4.1.13</li>
	<li>hwloc/1.6-iccifort-2011.13.367</li>
	<li>hwloc/1.9-GCC-4.8.3</li>
	<li>icc/11.1.069</li>
	<li>icc/11.1.073</li>
	<li>icc/11.1.075</li>
	<li>icc/2011.13.367</li>
	<li>icc/2011.6.233</li>
	<li>icc/2013.5.192</li>
	<li>icc/2013.5.192-GCC-4.8.3</li>
	<li>icc/2013_sp1.2.144</li>
	<li>iccifort/2011.13.367</li>
	<li>iccifort/2013.5.192-GCC-4.8.3</li>
	<li>ictce/3.2.1.015.u4</li>
	<li>ictce/3.2.2.u3</li>
	<li>ictce/4.0.6</li>
	<li>ictce/4.1.13</li>
	<li>ictce/4.1.14</li>
	<li>ictce/5.5.0</li>
	<li>ictce/6.2.5</li>
	<li>ifort/11.1.069</li>
	<li>ifort/11.1.073</li>
	<li>ifort/11.1.075</li>
	<li>ifort/2011.13.367</li>
	<li>ifort/2011.6.233</li>
	<li>ifort/2013.5.192</li>
	<li>ifort/2013.5.192-GCC-4.8.3</li>
	<li>ifort/2013_sp1.2.144</li>
	<li>iimpi/5.5.3-GCC-4.8.3</li>
	<li>imkl/10.2.4.032</li>
	<li>imkl/10.2.6.038</li>
	<li>imkl/10.3.12.361</li>
	<li>imkl/10.3.12.361-impi-4.1.0.030</li>
	<li>imkl/10.3.12.361-MVAPICH2-1.9</li>
	<li>imkl/10.3.12.361-OpenMPI-1.6.3</li>
	<li>imkl/10.3.6.233</li>
	<li>imkl/11.0.5.192</li>
	<li>imkl/11.1.2.144</li>
	<li>imkl/11.1.2.144-iimpi-5.5.3-GCC-4.8.3</li>
	<li>impi/3.2.2.006</li>
	<li>impi/4.0.0.028</li>
	<li>impi/4.0.2.003</li>
	<li>impi/4.1.0.027</li>
	<li>impi/4.1.0.030</li>
	<li>impi/4.1.1.036</li>
	<li>impi/4.1.3.049</li>
	<li>impi/4.1.3.049-GCC-4.8.3</li>
	<li>impi/4.1.3.049-iccifort-2013.5.192-GCC-4.8.3</li>
	<li>intel/2014b</li>
	<li>iomkl/4.6.13</li>
	<li>IPython/0.13.1-ictce-4.1.13-Python-2.7.3</li>
	<li>JasPer/1.900.1-ictce-4.1.13</li>
	<li>JasPer/1.900.1-ictce-5.5.0</li>
	<li>Java/1.7.0_10</li>
	<li>Java/1.7.0_15</li>
	<li>Java/1.7.0_17</li>
	<li>Java/1.7.0_40</li>
	<li>Java/1.7.0_60</li>
	<li>Java/1.8.0_20</li>
	<li>LAPACK/3.4.0-gompi-1.1.0-no-OFED</li>
	<li>libdrm/2.4.27-ictce-4.1.13</li>
	<li>libffi/3.0.13-ictce-4.1.13</li>
	<li>libffi/3.0.13-ictce-5.5.0</li>
	<li>libgd/2.1.0-ictce-5.5.0</li>
	<li>Libint/1.1.4-ictce-4.1.13</li>
	<li>Libint/1.1.4-ictce-5.5.0</li>
	<li>libint2/2.0.3-intel-2014b</li>
	<li>libjpeg-turbo/1.3.0-ictce-4.1.13</li>
	<li>libjpeg-turbo/1.3.0-ictce-5.5.0</li>
	<li>libpciaccess/0.13.1-ictce-4.1.13</li>
	<li>libpng/1.6.10-ictce-5.5.0</li>
	<li>libpng/1.6.3-ictce-4.1.13</li>
	<li>libpng/1.6.6-ictce-4.1.13</li>
	<li>libpng/1.6.6-ictce-5.5.0</li>
	<li>libpthread-stubs/0.3-ictce-4.1.13</li>
	<li>libreadline/6.2-ictce-4.1.13</li>
	<li>libreadline/6.2-ictce-5.5.0</li>
	<li>libreadline/6.2-intel-2014b</li>
	<li>libreadline/6.2-iomkl-4.6.13</li>
	<li>libxc/2.0.1-ictce-5.5.0</li>
	<li>libxc/2.2.0-intel-2014b</li>
	<li>libxml2/2.8.0-ictce-4.1.13-Python-2.7.3</li>
	<li>libxml2/2.9.0-ictce-4.1.13</li>
	<li>libxml2/2.9.1-ictce-4.1.13</li>
	<li>libxml2/2.9.1-ictce-5.5.0</li>
	<li>libXp/1.0.1</li>
	<li>libXp/1.0.1-ictce-4.1.13</li>
	<li>M4/1.4.16-ictce-3.2.2.u3</li>
	<li>M4/1.4.16-ictce-4.1.13</li>
	<li>M4/1.4.16-ictce-5.5.0</li>
	<li>M4/1.4.17-ictce-5.5.0</li>
	<li>M4/1.4.17-intel-2014b</li>
	<li>makedepend/1.0.4-ictce-4.1.13</li>
	<li>makedepend/1.0.4-ictce-5.5.0</li>
	<li>MariaDB/5.5.29-ictce-4.1.13</li>
	<li>MATLAB/2010b</li>
	<li>MATLAB/2012b</li>
	<li>Mesa/7.11.2-ictce-4.1.13-Python-2.7.3</li>
	<li>mpi4py/1.3-ictce-4.1.13-Python-2.7.3</li>
	<li>MrBayes/3.2.0-ictce-4.1.13</li>
	<li>MVAPICH2/1.9-iccifort-2011.13.367</li>
	<li>NASM/2.07-ictce-4.1.13</li>
	<li>NASM/2.07-ictce-5.5.0</li>
	<li>NCL/6.1.2-ictce-4.1.13</li>
	<li>NCL/6.1.2-ictce-5.5.0</li>
	<li>NCO/4.4.4-ictce-4.1.13</li>
	<li>ncurses/5.9-ictce-4.1.13</li>
	<li>ncurses/5.9-ictce-5.5.0</li>
	<li>ncurses/5.9-intel-2014b</li>
	<li>ncurses/5.9-iomkl-4.6.13</li>
	<li>ncview/2.1.2-ictce-4.1.13</li>
	<li>neon/0.30.0-ictce-4.1.13</li>
	<li>netaddr/0.7.10-ictce-5.5.0-Python-2.7.6</li>
	<li>netCDF/4.1.3-ictce-4.1.13</li>
	<li>netCDF/4.2.1.1-ictce-4.1.13</li>
	<li>netCDF/4.2.1.1-ictce-4.1.13-mt</li>
	<li>netCDF/4.2.1.1-ictce-5.5.0</li>
	<li>netCDF/4.2.1.1-ictce-5.5.0-mt</li>
	<li>netCDF/4.3.0-ictce-5.5.0</li>
	<li>netcdf4-python/1.0.7-ictce-5.5.0-Python-2.7.6</li>
	<li>netCDF-C++/4.2-ictce-4.1.13</li>
	<li>netCDF-C++/4.2-ictce-4.1.13-mt</li>
	<li>netCDF-C++/4.2-ictce-5.5.0-mt</li>
	<li>netCDF-Fortran/4.2-ictce-4.1.13</li>
	<li>netCDF-Fortran/4.2-ictce-4.1.13-mt</li>
	<li>netCDF-Fortran/4.2-ictce-5.5.0</li>
	<li>netCDF-Fortran/4.2-ictce-5.5.0-mt</li>
	<li>netifaces/0.8-ictce-5.5.0-Python-2.7.6</li>
	<li>NEURON/7.2-ictce-4.1.13</li>
	<li>numactl/2.0.9-GCC-4.8.3</li>
	<li>numexpr/2.0.1-ictce-4.1.13-Python-2.7.3</li>
	<li>numexpr/2.2.2-ictce-5.5.0-Python-2.7.6</li>
	<li>NWChem/6.1.1-ictce-4.1.13-2012-06-27-Python-2.7.3</li>
	<li>OpenBLAS/0.2.9-GCC-4.8.3-LAPACK-3.5.0</li>
	<li>OpenFOAM/2.1.1-ictce-4.1.13</li>
	<li>OpenFOAM/2.2.0-ictce-4.1.13</li>
	<li>OpenFOAM/2.3.0-intel-2014b</li>
	<li>OpenMPI/1.4.5-GCC-4.6.3-no-OFED</li>
	<li>OpenMPI/1.6.3-iccifort-2011.13.367</li>
	<li>OpenPGM/5.2.122-ictce-4.1.13</li>
	<li>OpenPGM/5.2.122-ictce-5.5.0</li>
	<li>PAML/4.7-ictce-4.1.13</li>
	<li>pandas/0.11.0-ictce-4.1.13-Python-2.7.3</li>
	<li>pandas/0.12.0-ictce-5.5.0-Python-2.7.6</li>
	<li>pandas/0.13.1-ictce-5.5.0-Python-2.7.6</li>
	<li>Paraview/4.1.0-ictce-4.1.13</li>
	<li>paycheck/1.0.2</li>
	<li>paycheck/1.0.2-ictce-4.1.13-Python-2.7.3</li>
	<li>paycheck/1.0.2-iomkl-4.6.13-Python-2.7.3</li>
	<li>pbs_python/4.3.5-ictce-5.5.0-Python-2.7.6</li>
	<li>Perl/5.16.3-ictce-4.1.13</li>
	<li>Perl/5.18.2-ictce-5.5.0</li>
	<li>picard/1.100-ictce-4.1.13</li>
	<li>Primer3/2.3.0-ictce-4.1.13</li>
	<li>printproto/1.0.5</li>
	<li>printproto/1.0.5-ictce-4.1.13</li>
	<li>PROJ.4/4.8.0-ictce-5.5.0</li>
	<li>pyproj/1.9.3-ictce-5.5.0-Python-2.7.6</li>
	<li>pyTables/2.4.0-ictce-4.1.13-Python-2.7.3</li>
	<li>pyTables/3.0.0-ictce-5.5.0-Python-2.7.6</li>
	<li>Python/2.5.6-ictce-4.1.13-bare</li>
	<li>Python/2.7.3-ictce-4.1.13(default)</li>
	<li>Python/2.7.3-iomkl-4.6.13</li>
	<li>Python/2.7.6-ictce-5.5.0</li>
	<li>PyZMQ/14.0.1-ictce-5.5.0-Python-2.7.6</li>
	<li>PyZMQ/2.2.0.1-ictce-4.1.13-Python-2.7.3</li>
	<li>Qt/4.8.5-ictce-4.1.13</li>
	<li>QuantumESPRESSO/5.0.2-ictce-5.5.0-hybrid</li>
	<li>QuantumESPRESSO/5.0.3-ictce-5.5.0-hybrid</li>
	<li>R/3.0.2-ictce-4.1.13</li>
	<li>R/3.0.2-ictce-5.5.0</li>
	<li>SAMtools/0.1.18-ictce-4.1.13</li>
	<li>SAMtools/0.1.19-ictce-5.5.0</li>
	<li>Schrodinger/2014-2_Linux-x86_64</li>
	<li>SCOOP/0.6.0.final-ictce-4.1.13-Python-2.7.3</li>
	<li>SCOTCH/6.0.0_esmumps-intel-2014b</li>
	<li>scripts/3.0.0</li>
	<li>scripts/4.0.0</li>
	<li>setuptools/1.4.2</li>
	<li>Spark/1.0.0</li>
	<li>SQLite/3.8.1-ictce-4.1.13</li>
	<li>SQLite/3.8.4.1-ictce-4.1.13</li>
	<li>SQLite/3.8.4.1-ictce-5.5.0</li>
	<li>subversion/1.6.11-ictce-4.1.13</li>
	<li>subversion/1.6.23-ictce-4.1.13</li>
	<li>subversion/1.8.8-ictce-4.1.13</li>
	<li>SURF/1.0-ictce-4.1.13-LINUXAMD64</li>
	<li>Szip/2.1-ictce-4.1.13</li>
	<li>Szip/2.1-ictce-5.5.0</li>
	<li>Tachyon/0.5.0</li>
	<li>Tcl/8.5.12-ictce-4.1.13</li>
	<li>Tcl/8.6.1-ictce-4.1.13</li>
	<li>Tcl/8.6.1-ictce-5.5.0</li>
	<li>tcsh/6.18.01-ictce-4.1.13</li>
	<li>tcsh/6.18.01-ictce-5.5.0</li>
	<li>Tk/8.5.12-ictce-4.1.13</li>
	<li>TopHat/2.0.10-ictce-5.5.0</li>
	<li>TopHat/2.0.8-ictce-4.1.13</li>
	<li>UDUNITS/2.1.24-ictce-4.1.13</li>
	<li>UDUNITS/2.1.24-ictce-5.5.0</li>
	<li>UNAFold/3.8-ictce-4.1.13</li>
	<li>util-linux/2.24-ictce-5.5.0</li>
	<li>uuid/1.6.2-ictce-4.1.13</li>
	<li>Valgrind/3.8.1</li>
	<li>VarScan/v2.3.6-ictce-4.1.13</li>
	<li>VASP/5.2.11-ictce-4.1.13-mt</li>
	<li>VASP/5.3.2-ictce-4.1.13-vtst-3.0b-20121111-mt</li>
	<li>VASP/5.3.3-ictce-3.2.1.015.u4-mt</li>
	<li>VASP/5.3.3-ictce-4.1.13-mt</li>
	<li>VASP/5.3.3-ictce-4.1.13-mt-dftd3</li>
	<li>VASP/5.3.3-ictce-4.1.13-mt-no-DNGXhalf</li>
	<li>VASP/5.3.3-ictce-4.1.13-vtst-3.0b-20121111-mt</li>
	<li>VASP/5.3.3-ictce-4.1.13-vtst-3.0c-20130327-mt</li>
	<li>VASP/5.3.3-ictce-5.5.0-mt</li>
	<li>VASP/5.3.3-ictce-6.2.5-mt</li>
	<li>VASP/5.3.5-intel-2014b-vtst-3.1-20140328-mt-vaspsol2.01</li>
	<li>VASP/5.3.5-intel-2014b-vtst-3.1-20140328-mt-vaspsol2.01-gamma</li>
	<li>VMD/1.9.1-ictce-4.1.13</li>
	<li>vsc-base/1.7.3</li>
	<li>vsc-base/1.9.1</li>
	<li>vsc-mympirun/3.2.3</li>
	<li>vsc-mympirun/3.3.0</li>
	<li>vsc-mympirun/3.4.2</li>
	<li>VSC-tools/0.1.2-ictce-4.1.13-Python-2.7.3</li>
	<li>VSC-tools/0.1.5</li>
	<li>VSC-tools/0.1.5-ictce-4.1.13-scoop</li>
	<li>VSC-tools/1.7.1</li>
	<li>VTK/6.0.0-ictce-4.1.13-Python-2.7.3</li>
	<li>WIEN2k/14.1-intel-2014b</li>
	<li>WPS/3.5.1-ictce-4.1.13-dmpar</li>
	<li>WRF/3.4-ictce-5.5.0-dmpar</li>
	<li>WRF/3.5.1-ictce-4.1.13-dmpar</li>
	<li>XML-LibXML/2.0018-ictce-4.1.13-Perl-5.16.3</li>
	<li>XML-Simple/2.20-ictce-4.1.13-Perl-5.16.3</li>
	<li>xorg-macros/1.17</li>
	<li>xorg-macros/1.17-ictce-4.1.13</li>
	<li>YAXT/0.2.1-ictce-5.5.0</li>
	<li>ZeroMQ/2.2.0-ictce-4.1.13</li>
	<li>ZeroMQ/4.0.3-ictce-5.5.0</li>
	<li>zlib/1.2.7-ictce-4.1.13</li>
	<li>zlib/1.2.7-ictce-5.5.0</li>
	<li>zlib/1.2.7-iomkl-4.6.13</li>
	<li>zlib/1.2.8-ictce-5.5.0</li>
</ul>"
403,"VSC Echo newsletter","<p>VSC Echo is e-mailed three times a year to all subscribers. The newsletter contains updates about our infrastructure, training programs and other events and highlights some of the results obtained by users of our clusters.</p>"
407,"Mission & vision","<p>Upon the establishment of the VSC, the Flemish government assigned us a number of tasks.</p>"
409,"The VSC in Flanders","<p>The VSC is a partnership of five Flemish university associations. The infrastructure is spread over four locations: Antwerp, Brussels, Ghent and Louvain.</p>"
411,"Our history","<p>Since its establishment in 2007, the VSC has evolved and grown considerably.
</p>"
413,"Publications","<p>In this section you’ll find all previous editions of our newsletter and various other publications issued by the VSC.
</p>"
415,"Organisation structure","<p>In this section you can find more information about the structure of our organisation and the various advisory committees.
</p>"
417,"Press material","<p>Would you like to write about our services? On this page you will find useful material such as our logo or recent press releases.
</p>"
451,"","<p>Op 25 oktober 2012 organiseerde het VSC de plechtige ingebruikname van de eerste Vlaamse tier 1 cluster aan de Universiteit Gent, waar de cluster ook geplaatst werd.</p>"
455,"","<p>On 25 October 2012 the VSC inaugurated the first Flemish tier 1 compute cluster. The cluster is housed in the data centre of Ghent University.</p>"
459,"","<p>Programma / Programme
</p><ul>
	<li>
		<a href=\"/assets/83\">Toespraak door professor Paul Van Cauwenberge, rector van de Universiteit Gent</a></li>
	<li>
		<a href=\"https://videolab.avnet.kuleuven.be/video/?id=d1d1ff47a891dd732b56a4b4e4c39be8&amp;height=388&amp;width=640&amp;autostart=true\">Film over onderzoek op de tier 1</a></li>
	<li>
		<a href=\"/assets/85\">Toespraak door professor Peter Marynen, voorzitter stuurgroep VSC</a></li>
	<li>
		Een kort woordje door de heer Eric Van Bael, managing director HP België</li>
	<li>
		<a href=\"/assets/87\">Toespraak door dr. ir. Kurt Lust, VSC-coördinator</a> (illustraties in <a href=\"/assets/275\">PDF</a>)</li>
	<li>
		<a href=\"https://videolab.avnet.kuleuven.be/video/?id=75270088b4163e233ce3adc66ad22f45&amp;height=388&amp;width=640&amp;autostart=true\">Videoboodschap van minister Ingrid Lieten, viceminister-president van de Vlaamse regering en Vlaams minister van Innovatie, Overheidsinvesteringen, Media en Armoedebestreiding</a></li>
</ul><p>Het programma werd gevolgd door de officiële ingebruikname van de cluster in het datacentrum en een receptie.<br></p>"
461,"Links","<ul>
	<li><a href=\"/events/tier1-launch-2012/invitation\">The invitation</a> (in Dutch)</li>
	<li><a href=\"/events/tier1-launch-2012/media\">In the media</a></li>
	<li><a href=\"/events/tier1-launch-2012/photo-album\">Photo album</a></li>
</ul>"
465,"","<p>We organize regular trainings on many HPC-related topics. The level ranges fro introductory to advanced. We also actively promote some courses organised elsewhere. The courses are open to participants at the university associations. Many are also open to external users (the limitations often caused by software licenses of the packages used during hand-ons). For further info, you can contact the <a href=\"/en/about-vsc/contact\">course coordinator Geert Jan Bex</a>.</p>"
467,"Previous events and training sessions","<p>We keep links to our previous events and training sessions. Materials used during the course can also be found on those pages.</p>"
469,"","<p>More questions? <a href=\"/en/about-vsc/contact\">Contact the course coordinator or one of the other coordinators</a>.</p>"
471,"","<p>On you application form, you will be asked to indicate the scientific domain of your application according to the NWO classification. Below we present the list of domains and subdomains. You only need to give the domain in your application, but the subdomains may make it easier to determine the most suitable domain for your application.
</p><ul>
	<li>Archaeology
	<ul>
		<li>Prehistory</li>
		<li>Antiquity and late antiquity</li>
		<li>Oriental archaeology</li>
		<li>Mediaeval archaeology</li>
		<li>Industrial archaeology</li>
		<li>Preservation and restoration, museums</li>
		<li>Methods and techniques</li>
		<li>Archeology, other</li>
	</ul></li>
	<li>Area studies
	<ul>
		<li>Asian languages and literature</li>
		<li>Asian religions and philosophies</li>
		<li>Jewish studies</li>
		<li>Islamic studies</li>
		<li>Iranian and Armenian studies</li>
		<li>Central Asian studies</li>
		<li>Indian studies</li>
		<li>South-east Asian studies</li>
		<li>Sinology</li>
		<li>Japanese studies</li>
		<li>Area studies, other</li>
	</ul></li>
	<li>Art and architecture
	<ul>
		<li>Pre-historic and pre-classical art</li>
		<li>Antiquity and late antiquity art</li>
		<li>Mediaeval art</li>
		<li>Renaissance and Baroque art</li>
		<li>Modern and contemporary art</li>
		<li>Oriental art and architecture</li>
		<li>Iconography</li>
		<li>History of architecture</li>
		<li>Urban studies</li>
		<li>Preservation and restoration of cultural heritage</li>
		<li>Museums and collections</li>
		<li>Art and architecture, other</li>
	</ul></li>
	<li>Astronomy, astrophysics
	<ul>
		<li>Planetary science</li>
		<li>Astronomy, astrophysics, other</li>
	</ul></li>
	<li>Biology
	<ul>
		<li>Microbiology</li>
		<li>Biogeography, taxonomy</li>
		<li>Animal ethology, animal psychology</li>
		<li>Ecology</li>
		<li>Botany</li>
		<li>Zoology</li>
		<li>Toxicology (plants, invertebrates)</li>
		<li>Biotechnology</li>
		<li>Biology, other</li>
	</ul></li>
	<li>Business administration
	<ul>
		<li>Business administration</li>
	</ul></li>
	<li>Chemistry
	<ul>
		<li>Analytical chemistry</li>
		<li>Macromolecular chemistry, polymer chemistry</li>
		<li>Organic chemistry</li>
		<li>Inorganic chemistry</li>
		<li>Physical chemistry</li>
		<li>Catalysis</li>
		<li>Theoretical chemistry, quantum chemistry</li>
		<li>Chemistry, other</li>
	</ul></li>
	<li>Communication science
	<ul>
		<li>Communication science</li>
	</ul></li>
	<li>Computer science
	<ul>
		<li>Computer systems, architectures, networks</li>
		<li>Software, algorithms, control systems</li>
		<li>Theoretical computer science</li>
		<li>Information systems, databases</li>
		<li>User interfaces, multimedia</li>
		<li>Artificial intelligence, expert systems</li>
		<li>Computer graphics</li>
		<li>Computer simulation, virtual reality</li>
		<li>Computer science, other</li>
		<li>Bioinformatics/biostatistics, biomathematics, biomechanics</li>
	</ul></li>
	<li>Computers and the humanities
	<ul>
		<li>Software for humanities</li>
		<li>Textual and content analysis</li>
		<li>Textual and linguistic corpora</li>
		<li>Databases for humanities</li>
		<li>Hypertexts and multimedia</li>
		<li>Computers and the humanities, other</li>
	</ul></li>
	<li>Cultural anthropology
	<ul>
		<li>Cultural anthropology</li>
	</ul></li>
	<li>Demography
	<ul>
		<li>Demography</li>
	</ul></li>
	<li>Development studies
	<ul>
		<li>Development studies</li>
	</ul></li>
	<li>Earth sciences
	<ul>
		<li>Geochemistry, geophysics</li>
		<li>Paleontology, stratigraphy</li>
		<li>Geodynamics, sedimentation, tectonics, geomorphology</li>
		<li>Petrology, mineralogy, sedimentology</li>
		<li>Atmosphere sciences</li>
		<li>Hydrosphere sciences</li>
		<li>Geodesy, physical geography</li>
		<li>Earth sciences, other</li>
	</ul></li>
	<li>Economy
	<ul>
		<li>Microeconomics</li>
		<li>Macroeconomics</li>
		<li>Econometrics</li>
	</ul></li>
	<li>Environmental science
	<ul>
		<li>Environmental science</li>
	</ul></li>
	<li>Gender studies
	<ul>
		<li>Gender studies</li>
	</ul></li>
	<li>Geography / planning
	<ul>
		<li>Geography</li>
		<li>Planning</li>
	</ul></li>
	<li>History
	<ul>
		<li>Pre-classical civilizations</li>
		<li>Antiquity and late antiquity history</li>
		<li>Mediaeval history</li>
		<li>Modern and contemporary history</li>
		<li>Social and economic history</li>
		<li>Cultural history</li>
		<li>Comparative political history</li>
		<li>Librarianschip, archive studies</li>
		<li>History, other</li>
		<li>History and philosophy of science and technology</li>
		<li>History of ancient science</li>
		<li>History of mediaeval science</li>
		<li>History of modern science</li>
		<li>History of contemporary science</li>
		<li>History of technology</li>
		<li>History of Science, other</li>
		<li>History of religions</li>
		<li>History of Christianity</li>
		<li>Theology and history of theology</li>
	</ul></li>
	<li>History of science
	<ul>
		<li>History of ancient science</li>
		<li>History of mediaeval science</li>
		<li>History of modern science</li>
		<li>History of contemporary science</li>
		<li>History of technology</li>
		<li>Science museums and collections</li>
		<li>History of science, other</li>
	</ul></li>
	<li>Language and literature
	<ul>
		<li>Pre-classical philology and literature</li>
		<li>Greek and Latin philology and literature</li>
		<li>Mediaeval and Neo-Latin languages and literature</li>
		<li>Mediaeval European languages and literature</li>
		<li>Modern European languages and literature</li>
		<li>Anglo-American literature</li>
		<li>Hispanic and Brazilian literature</li>
		<li>African languages and literature</li>
		<li>Comparative literature</li>
		<li>Language and literature, other</li>
	</ul></li>
	<li>Law
	<ul>
		<li>Private law</li>
		<li>Constitutional and Administrative law</li>
		<li>International and European law</li>
		<li>Criminal law and Criminology</li>
	</ul></li>
	<li>Life sciences
	<ul>
		<li>Bioinformatics/biostatistics, biomathematics, biomechanics</li>
		<li>Biophysics, clinical physics</li>
		<li>Biochemistry</li>
		<li>Genetics</li>
		<li>Histology, cell biology</li>
		<li>Anatomy, morphology</li>
		<li>Physiology</li>
		<li>Immunology, serology</li>
		<li>Life sciences, other</li>
	</ul></li>
	<li>Life sciences and medicine
	<ul>
		<li>History and philosophy of the life sciences, ethics and
evolution biology
		</li>
	</ul></li>
	<li>Linguistics
	<ul>
		<li>Phonetics and phonology</li>
		<li>Morphology, grammar and syntax</li>
		<li>Semantics and philosophy of language</li>
		<li>Linguistic typology and comparative linguistics</li>
		<li>Dialectology, linguistic geography, sociolinguistic</li>
		<li>Lexicon and lexicography</li>
		<li>Psycholinguistics and neurolinguistics</li>
		<li>Computational linguistics and philology</li>
		<li>Linguistic statistics</li>
		<li>Language teaching and acquisition</li>
		<li>Translation studies</li>
		<li>Linguistics, other</li>
	</ul></li>
	<li>Medicine
	<ul>
		<li>Pathology, pathological anatomy</li>
		<li>Organs and organ systems</li>
		<li>Medical specialisms</li>
		<li>Health sciences</li>
		<li>Kinesiology</li>
		<li>Gerontology</li>
		<li>Nutrition</li>
		<li>Epidemiology</li>
		<li>Health Services Research</li>
		<li>Health law</li>
		<li>Health economics</li>
		<li>Medical sociology</li>
		<li>Medicine, other</li>
	</ul></li>
	<li>Mathematics
	<ul>
		<li>Logic, set theory and arithmetic</li>
		<li>Algebra, group theory</li>
		<li>Functions, differential equations</li>
		<li>Fourier analysis, functional analysis</li>
		<li>Geometry, topology</li>
		<li>Probability theory, statistics</li>
		<li>Operations research</li>
		<li>Numerical analysis</li>
		<li>Mathematics, other</li>
	</ul></li>
	<li>Music, theatre, performing arts and media
        <ul>
	         <li>Ethnomusicology</li>
	<li>History of music and musical iconography</li>
	<li>Musicology</li>
	<li>Opera and dance</li>
	<li>Theatre studies and iconography</li>
	<li>Film, photography and audio-visual media</li>
	<li>Journalism and mass communications</li>
	<li>Media studies</li>
	<li>Music, theatre, performing arts and media, other</li>
</ul></li>
<li>Pedagogics
<ul>
	<li>Pedagogics</li>
</ul></li>
<li>Philosophy
<ul>
	<li>Metaphysics, theoretical philosophy</li>
	<li>Ethics, moral philosophy</li>
	<li>Logic and history of logic</li>
	<li>Epistemology, philosophy of science</li>
	<li>Aesthetics, philosophy of art</li>
	<li>Philosophy of language, semiotics</li>
	<li>History of ideas and intellectual history</li>
	<li>History of ancient and mediaeval philosophy</li>
	<li>History of modern and contemporary philosophy</li>
	<li>History of political and economic theory</li>
	<li>Philosophy, other</li>
	<li>History and philosophy of science and technology</li>
</ul></li>
<li>Physics
<ul>
	<li>Subatomic physics</li>
	<li>Nanophysics/technology</li>
	<li>Condensed matter and optical physics</li>
	<li>Processes in living systems</li>
	<li>Fusion physics</li>
	<li>Phenomenological physics</li>
	<li>Other physics</li>
	<li>Theoretical physics</li>
</ul></li>
<li>Psychology
<ul>
	<li>Clinical Psychology</li>
	<li>Biological and Medical Psychology</li>
	<li>Developmental Psychology</li>
	<li>Psychonomics and Cognitive Psychology</li>
	<li>Social and Organizational Psychology</li>
	<li>Psychometrics</li>
</ul></li>
<li>Public administration and political science
<ul>
	<li>Public administration</li>
	<li>Political science</li>
</ul></li>
<li>Religious studies and theology
<ul>
	<li>History of religions</li>
	<li>History of Christianity</li>
	<li>Theology and history of theology</li>
	<li>Bible studies</li>
	<li>Religious studies and theology, other</li>
</ul></li>
<li>Science of Teaching
<ul>
	<li>Science of Teaching</li>
</ul></li>
<li>Science and technology
<ul>
	<li>History and philosophy of science and technology</li>
</ul></li>
<li>Sociology
<ul>
	<li>Sociology</li>
</ul></li>
<li>Technology
<ul>
	<li>Materials technology</li>
	<li>Mechanical engineering</li>
	<li>Electrical engineering</li>
	<li>Civil engineering</li>
	<li>Chemical technology, process technology</li>
	<li>Geotechnics</li>
	<li>Technology assessment</li>
	<li>Nanotechnology</li>
	<li>Technology, other</li>
</ul></li>
<li>Veterinary medicine
<ul>
	<li>Veterinary medicine</li>
</ul></li>
<p><br>
</p></ul>"
475,"","<ul>
	<li>
		<a href=\"/events/tier1-launch-2012/press-announcement\">Persmededeling van viceminister-president Ingrid Lieten, Vlaams minister van innovatie, overheidsinvesteringen, media en armoedebestrijding</a></li>
	<li>
		<a href=\"http://deredactie.be/cm/vrtnieuws/videozone/archief/programmas/journaal/2.24934/2.24935/1.1466027\" target=\"_blank\">Bericht over de ingebruikname in \"Het journaal\" op één</a>.</li>
</ul>"
477,"","<p style=\"text-align: center;\">
	<img src=\"/assets/269\" alt=\"\" style=\"width: 129px; height: 88px;\" title=\"logo VR\">
</p><p style=\"text-align: center;\">
	<strong>PERSMEDEDELING VAN VICEMINISTER-PRESIDENT INGRID LIETEN<br>
	VLAAMS MINISTER VAN INNOVATIE, OVERHEIDSINVESTERINGEN, MEDIA EN ARMOEDEBESTRIJDING</strong>
</p><p style=\"text-align: center;\">
	<strong>Donderdag 25 oktober 2012</strong>
</p><p style=\"text-align: center;\">
	<strong>Eerste TIER 1  Supercomputer wordt in gebruik genomen aan de UGent.</strong>
</p><p style=\"text-align: justify;\">
	<strong>Vandaag wordt aan de UGent de eerste Tier 1 supercomputer van het Vlaams ComputerCentrum (VSC) plechtig in gebruik genomen. De supercomputer is een initiatief van de Vlaamse overheid om aan onderzoekers in Vlaanderen een bijzonder krachtige rekeninfrastructuur ter beschikking te stellen om zo beter het hoofd te kunnen bieden aan de maatschappelijke uitdagingen war we vandaag voor staan.“Het VSC moet ‘high performance computing’ toegankelijk maken voor kennisinstellingen en bedrijven. Hierdoor kunnen doorbraken gerealiseerd worden in domeinen als gezondheidszorg, chemie, en milieu”, zegt Ingrid Lieten. </strong>
</p><p style=\"text-align: justify;\">
	<br>
	In de internationale onderzoekswereld zijn de supercomputers niet meer weg te denken. Deze grote rekeninfrastructuren waren recent een noodzakelijke schakel in de ontdekking van het Higgsdeeltje. Hun rekencapaciteit laat toe steeds beter de werkelijkheid te simuleren. Hierdoor is een nieuwe manier om onderzoek te verrichten ontstaan, met belangrijke toepassingen voor onze economie en onze samenleving.
</p><p style=\"text-align: justify;\">
	<span style=\"line-height: 1.5em;\">“Dankzij supercomputers worden weersvoorspellingen over langere perioden steeds betrouwbaarder, of kunnen klimaatveranderingen en natuurrampen beter voorspeld worden. Auto’s worden veiliger omdat de constructeurs het verloop van botsingen en de impact op passagiers in detail kunnen simuleren. Ook aan de evolutie naar geneeskunde op maat van de patiënt, kan de supercomputer fundamenteel bijdragen. De ontwikkeling van geneesmiddelen gebeurt namelijk voor een groot deel via simulaties van chemische reacties”, zegt Ingrid Lieten.</span>
</p><p style=\"text-align: justify;\">
	<span style=\"line-height: 1.5em;\">Het Vlaamse Supercomputer Centrum staat open voor alle Vlaamse onderzoekers, zowel uit de kennisinstellingen en strategische onderzoekscentra als uit de bedrijven. Het levert opportuniteiten voor universiteiten en industrie, maar ook voor overheden, mutualiteiten en andere zorgorganisaties. De supercomputer moet  een belangrijke bijdrage leveren aan de zoektocht naar oplossingen voor de grote maatschappelijke uitdagingen, en dit in de meest uiteenlopende domeinen.  Zo kan de supercomputer  nieuwe geneesmiddelen ontwikkelen of  demografische evoluties voor humane en sociale wetenschappen analyseren, zoals de vergrijzing en hoe daarmee om te gaan. Maar de supercomputer zal ook ingezet worden om state of the art windmolens te ontwerpen en ingewikkelde modellen te berekenen voor het voorspellen van klimaatsveranderingen.</span>
</p><p style=\"text-align: justify;\">
	Om de mogelijkheden van de supercomputer beter bekend te maken en het gebruik te stimuleren in Vlaanderen, krijgt de Herculesstichting de opdracht om het Vlaamse Supercomputer Centrum actief te promoten en opleidingen te voorzien. De Herculesstichting is het Vlaamse agentschap voor de financiering van middelzware en zware infrastructuur voor fundamenteel en strategisch basisonderzoek.   Zij zullen ervoor zorgen dat associaties, kennisinstellingen, SOCs, het bedrijfsleven, enz. even vlot toegang krijgen tot de TIER1 supercomputer. De huisvesting en technische exploitatie blijven bij de associaties.
</p><p style=\"text-align: justify;\">
	<span style=\"line-height: 1.5em;\">“Met de ingebruikname van de TIER1 staat Vlaanderen nu echt op de kaart in Europa wat betreft ‘high performance computing’. Vlaamse onderzoekers  krijgen de mogelijkheid om aan te sluiten bij belangrijke Europese onderzoeksprojecten, zowel op het vlak van fundamenteel als van toegepast onderzoek”, zegt Ingrid Lieten.</span>
</p><p style=\"text-align: justify;\">
	Het Vlaams Supercomputer Centrum beheert zowel de zogenaamde ‘TIER2’ computers, die lokaal bij de universiteiten staan, als de ‘TIER1’ computer, die voor nog complexere toepassingen gebruikt wordt.
</p><h4 style=\"text-align: justify;\">
<span style=\"line-height: 1.5em;\">Persinfo:</span></h4><p style=\"text-align: justify;\">
	Lot Wildemeersch, woordvoerster Ingrid Lieten<br>
	0477 810 176 | lot.wildemeersch@vlaanderen.be<br>
	www.ingridlieten.be
</p><p style=\"text-align: justify;\">
	<img src=\"/assets/271\" alt=\"\" style=\"width: 130px; height: 94px;\" title=\"richting morgen\">
</p>"
479,"","<p>
	<img src=\"/assets/273\" alt=\"\" style=\"width: 100%;\" rel=\"width: 100%;\" title=\"uitnodiging tier 1 launch\"></p>"
481,"","<table style=\"width: 100%;\">
<tbody>
<tr>
	<td><img src=\"/assets/277\" alt=\"Logo VSC kleur\" title=\"Logo VSC kleur\" width=\"215\" height=\"100\">
	</td>
	<td align=\"right\"><span style=\"font-size: larger;\"><span style=\"color: #808080;\">March 23 2009</span></span><br> <strong><span style=\"font-size: larger;\"><span style=\"color: #00a4ca;\">Launch Flemish Supercomputer Centre</span></span></strong>
	</td>
</tr>
</tbody>
</table><p style=\"margin: 0em 0em 0.5em; text-align: justify;\">The official launch took place on 23 March 2009 in the Promotiezaal of the  Universiteitshal of the K.U.Leuven, Naamsestraat 22, 3000 Leuven.
</p><ul>
	<li>
	<a href=\"/events/vsc-launch-2009/program\" class=\"internal-link\">Program</a>, with links to some of the presentations.
	</li>
	<li>
	<a href=\"/events/vsc-launch-2009/invitation\" class=\"internal-link\">Invitation</a>
	</li>
</ul><p style=\"margin-top: 0em; margin-right: 0em; margin-bottom: 0.5em; margin-left: 0em; \">The press mentioning the VSC launch event:
</p><ul>
	<li>
	An article in <a href=\"http://primeurmagazine.com/\">EnterTheGrid - PrimeurWeekly</a>, edition 23 March 2009
	</li>
	<li>
	<a href=\"https://nieuws.kuleuven.be/nl/campuskrant/0809/07/het-vlaams-supercomputercentrum-kan-tellen\" target=\"_blank\">An article in the K.U.Leuven Campuskrant, edition 25 March 2009</a> (in Dutch)
	</li>
</ul><ul>
	<li>
	An article on the web site of Knack (in Dutch)
	</li>
	<li>
	An article in the French edition of datanews, 24 maart 2009 (in French)
	</li>
</ul><p><a href=\"/events/vsc-launch-2009/figures\" class=\"internal-link\"><img src=\"/assets/81\" title=\"uitnodiging figuren 800px\" width=\"100%\" alt=\"uitnodiging figuren 800px\" class=\"image-inline\" style=\"border-style: none !important;\"></a>
</p><p style=\"text-align: justify;\">The images at the top of this page are courtesy of <a href=\"https://www.numeca.com/home\" target=\"_blank\">NUMECA International</a> and research groups at Antwerp University, the Vrije Universiteit Brussel and the KU Leuven.
</p>"
483,"","<table width=\"90%\" border=\"2\" align=\"center\" cellpadding=\"5\" cellspacing=\"1\">     <tbody>         <tr>             <td style=\"text-align: justify; \"><p>The program contains links to some of the presentations. The copyright for the presentations remains with the original authors and not with the VSC. Reproducing parts of these presentations or using them in other presentations can only be done with the agreement of the author(s) of the presentation.</p></td>         </tr>     </tbody> </table>  <table width=\"100%\" cellspacing=\"1\" cellpadding=\"1\" border=\"0\">     <tbody>         <tr>             <td valign=\"top\" colspan=\"2\">14u15 </td>             <td valign=\"top\" colspan=\"2\">Scientific program</td>         </tr>         <tr>             <td width=\"20\">    </td>             <td valign=\"top\" colspan=\"2\">14u15 </td>             <td valign=\"top\">Dr. ir. Kurt Lust (Vlaams Supercomputer Centrum). Presentation of the VSC<br><a href=\"/assets/279\">Presentation (PDF)</a></td>         </tr>         <tr>             <td> </td>             <td valign=\"top\" colspan=\"2\">14u30 </td>             <td>Prof. dr. Patrick Bultinck (Universiteit Gent). <em><a href=\"#Bultinck\">In silico</a></em><a href=\"#Bultinck\"> Chemistry: Quantum Chemistry and Supercomputers<br>             </a><a href=\"/assets/281\">Presentation (PDF)</a></td>         </tr>         <tr>             <td> </td>             <td valign=\"top\" colspan=\"2\">14u45 </td>             <td valign=\"top\">Prof. dr. Wim Vanroose (Universiteit Antwerpen). <a href=\"#Vanroose\">Large scale calculations of molecules in laser fields</a><br><a href=\"/assets/283\">Presentation (PDF)</a></td>         </tr>         <tr>             <td> </td>             <td valign=\"top\" colspan=\"2\">15u00 </td>             <td>Prof. dr. Stefaan Tavernier (Vrije Universiteit Brussel). <a href=\"#Tavernier\">Grid applications in particle and astroparticle physics: The CMS and IceCube projects<br>             </a><a href=\"/assets/285\">Presentation (PDF)</a></td>         </tr>         <tr>             <td> </td>             <td valign=\"top\" colspan=\"2\">15u15 </td>             <td valign=\"top\">Prof. dr. Dirk Van den Poel (Universiteit Gent). <a href=\"#VandenPoel\">Research using HPC capabilities in the field of economics/business &amp; management science<br>             </a><a href=\"/assets/291\">Presentation (PDF)</a></td>         </tr>         <tr>             <td> </td>             <td valign=\"top\" colspan=\"2\">15u30 </td>             <td valign=\"top\">Dr. Kris Heylen (K.U.Leuven). <a href=\"#Heylen\">Supercomputing and Linguistics<br>             </a><a href=\"/assets/287\">Presentation (PDF)</a></td>         </tr>         <tr>             <td> </td>             <td valign=\"top\" colspan=\"2\">15u45</td>             <td valign=\"top\">Dr. ir. Lies Geris (K.U.Leuven). <a href=\"#Geris\">Modeling in biomechanics and biomedical engineering<br></a><a href=\"/assets/289\">Presentatie (PDF)</a></td>         </tr>         <tr>             <td> </td>             <td valign=\"top\" colspan=\"2\">16u00 </td>             <td valign=\"top\">Prof. dr. ir. Chris Lacor (Vrije Universiteit Brussel)  and  Prof. Dr. Stefaan Poedts (K.U.Leuven). <a href=\"#LacorPoedts\">Supercomputing in CFD and MHD</a></td>         </tr>         <tr>             <td valign=\"top\" colspan=\"2\">16u15 </td>             <td valign=\"top\" colspan=\"2\">Coffee break</td>         </tr>         <tr>             <td valign=\"top\" colspan=\"2\">17u00 </td>             <td valign=\"top\" colspan=\"2\">Academic session</td>         </tr>         <tr>             <td> </td>             <td valign=\"top\" colspan=\"2\">17u00 </td>             <td valign=\"top\">Prof. dr. ir. Karen Maex, Chairman of the steering group of the Vlaams Supercomputer Centrum<br><a href=\"/assets/293\">Presentatie (PDF)</a></td>         </tr>         <tr>             <td> </td>             <td valign=\"top\" colspan=\"2\">17u10 </td>             <td valign=\"top\">Prof. dr. dr. Thomas Lippert, Director of the Institute for Advanced Simulation and head of the Jülich Supercomputer Centre, Forschungszentrum Jülich. European view on supercomputing and PRACE<br><a href=\"/assets/295\">Presentation (PDF)</a></td>         </tr>         <tr>             <td> </td>             <td valign=\"top\" colspan=\"2\">17u50 </td>             <td valign=\"top\">Prof. dr. ir. Charles Hirsch, President of the HPC Working Group of the Royal Flemish Academy of Belgium for Sciences and the Arts (KVAB)<br><a href=\"/assets/297\">Presentation (PDF)</a></td>         </tr>         <tr>             <td> </td>             <td valign=\"top\" colspan=\"2\">18u00 </td>             <td valign=\"top\">Prof. dr. ir. Bart De Moor, President of the Board of Directors of the Hercules Foundation<br><a href=\"/assets/299\">Presentation (PDF)</a></td>         </tr>         <tr>             <td> </td>             <td valign=\"top\" colspan=\"2\">18u10 </td>             <td valign=\"top\">Minister Patricia Ceysens, Flemish Minister for Economy, Enterprise, Science, Innovation and Foreign Trade</td>         </tr>         <tr>             <td valign=\"top\" colspan=\"2\">18u30 </td>             <td valign=\"top\" colspan=\"2\">Reception</td>         </tr>     </tbody> </table>  <h1>Abstracts</h1> <h2><a name=\"Bultinck\"></a><span style=\"font-size: medium;\" rel=\"font-size: medium;\">Prof. dr. Patrick Bultinck. <em>In silico</em> Chemistry: Quantum Chemistry and Supercomputers</span></h2> <p><em>Universiteit Gent/Ghent University, Faculty of Sciences, Department of Inorganic and Physical Chemistry</em></p> <p>Quantum Chemistry deals with the chemical application of quantum mechanics to understand the nature of chemical substances, the reasons for their (in)stability but also with finding ways to predict properties of novel molecules prior to their synthesis. The working horse of quantum chemists is therefore no longer the laboratory but the supercomputer. The reason for this is that quantum chemical calculations are notoriously computationally demanding.<br> These computational demands are illustrated by the scaling of computational demands with respect to the size of molecules and the level of theory applied. An example from Vibrational Circular Dichroism calculations shows how supercomputers play a role in stimulating innovation in chemistry.</p> <p><strong>Prof. dr. Patrick Bultinck</strong> (° Blankenberge, 1971) is professor in Quantum Chemistry, Computational and inorganic chemistry at Ghent University, Faculty of Sciences, Department of Inorganic and Physical Chemistry. He is author of roughly 100 scientific publications and performs research in quantum chemistry with emphasis on the study of concepts such as the chemical bond, the atom in the molecule and aromaticity. Another main topic is the use of computational (quantum) chemistry in drug discovery. In 2002 and 2003 P. Bultinck received grants from the European Center for SuperComputing in Catalunya for his computationally demanding work in this field.</p> <h2><a name=\"Vanroose\"></a><span style=\"font-size: medium;\" rel=\"font-size: medium;\">Prof. dr. Wim Vanroose. Large scale calculations of molecules in laser fields</span></h2> <p><em>Universiteit Antwerpen, Department of Mathematics and Computer Science</em></p> <p>Over the last decade, calculations with large scale computer has caused a revolution <br> in the understanding of the ultrafast dynamics that plays at the microscopic level.  We give an overview of the international efforts to advance the  computational tools for this area of science. We also discuss how the results of the calculations are guiding chemical experiments.</p> <p><strong>Prof. dr. Wim Vanroose</strong> is  BOF-Research professor at the Department of Mathematics and Computer Science, Universiteit Antwerpen. He is involved in international efforts to build to computational tools for large scale simulations for ultrafast microscopic dynamics. Between 2001 and 2004 he was a computational scientist at NERSC computing center at the Berkeley Lab, Berkeley USA.</p> <h2><a name=\"Tavernier\"></a><span style=\"font-size: medium;\" rel=\"font-size: medium;\">Prof. dr. Stefaan Tavernier. Grid applications in particle and astroparticle physics: The CMS and IceCube projects</span></h2> <p><em>Vrije Universiteit Brussel, Faculty of Science and Bio-engineering Sciences, Department of Physics, Research Group of Elementary Particle Physics</em></p> <p>The large hadron collider LHC at the international research centre CERN near Geneva is due to go into operation at the end of 2009. It will be the most powerful particle accelerator ever, and will give us a first glimpse of the new phenomena that that are expected to occur at these energies. However, the analysis of the data produced by the experiments around this accelerator also represents an unprecedented challenge. The VUB, UGent and UA  participate in the CMS project. This is one of the four major experiments to be performed at this accelerator. One year of CMS operation will result in about 106 GBytes of data. To cope with this flow of data, the CMS collaboration has setup a GRID computing infrastructure with distributed computer infrastructure scattered over the participating laboratories in 4 continents.<br> The IceCube Neutrino Detector is a neutrino observatory currently under construction at the South Pole. IceCube is being constructed in deep Antarctic ice by deploying thousands of optical sensors at depths between 1,450 and 2,450 meters. The main goal of the experiment is to detect very high energy neutrinos from the cosmos. The neutrinos are not detected themselves. Instead, the rare instance of a collision between a neutrino and an atom within the ice is used to deduce the kinematical parameters of the incoming neutrino. The sources of those neutrinos could be black holes, gamma ray bursts, or supernova remnants. The data that IceCube will collect will also contribute to our understanding of cosmic rays, supersymmetry, weakly interacting massive particles (WIMPS), and other aspects of nuclear and particle physics. The analysis of the data produced by ice cube requires similar computing facilities as the analysis of the LHC data.</p> <p><strong>Prof. dr. Stefaan Tavernier</strong> is professor of physics at the Vrije Universiteit Brussel. He obtained a Ph.D. at the Faculté des sciences of Orsay(France)  in 1968, and a \"Habilitation\" at de VUB in 1984. He spent most of his scientific career working on research projects at the international research centre CERN in Geneva. He has been project leader for the CERN/NA25 project, and he presently is the spokesperson of the CERN/Crystal Clear(RD18) collaboration. His main expertise is in experimental methods for particle physics. He has over 160 publications in peer reviewed international journals, made several contributions to books and has several patents. He is also the author of a textbook on experimental methods in nuclear and particle physics.</p> <h2><a name=\"VandenPoel\"></a><span style=\"font-size: medium;\" rel=\"font-size: medium;\">Prof. dr. Dirk Van den Poel. Research using HPC capabilities in the field of economics/business &amp; management science</span></h2> <p><em>Universiteit Gent/Ghent University, Faculty of Economics and Business Administration, Department of Marketing, <a target=\"_blank\" href=\"http://www.crm.UGent.be\">www.crm.UGent.be</a> and <a target=\"_blank\" href=\"http://www.mma.UGent.be\">www.mma.UGent.be</a></em></p> <p>HPC capabilities in the field of economics/business &amp; management science are most welcome when optimizing specific quantities (e.g. maximizing sales, profits, service level, or minimizing costs) subject to certain constraints. Optimal solutions for common problems are usually computationally infeasible even with the biggest HPC installations, therefore researchers develop heuristics or use techniques such as genetic algorithms to come close to optimal solutions. One of the nice properties they possess is that they are typically easily parallelizable. In this talk, I will give several examples of typical research questions, which need an HPC infrastructure to obtain good solutions in a reasonable time window. These include the optimization of marketing actions towards different marketing segments in the domain of analytical CRM (customer relationship management) and solving multiple-TSP (traveling salesman problem) under load balancing, alternatively known as the vehicle routing problem under load balancing.</p> <p><strong>Prof. dr. Dirk Van den Poel</strong> (° Merksem, 1969) is professor of marketing modeling/analytical customer relationship management (aCRM) at Ghent University. He obtained his MSc in management/business engineering as well as PhD from K.U.Leuven. He heads the modeling cluster of the Department of Marketing at Ghent University. He is program director of the Master of Marketing Analysis, a one-year program in English about predictive analytics in marketing. His main interest fields are aCRM, data mining (genetic algorithms, neural networks, random forests, random multinomial logit: RMNL), text mining, optimal marketing resource allocation and operations research.</p> <h2><a name=\"Heylen\"></a><span style=\"font-size: medium;\" rel=\"font-size: medium;\">Dr. Kris Heylen. Supercomputing and Linguistics</span></h2> <p><em>Katholieke Universiteit Leuven,  Faculty of Arts, Research Unit Quantitative Lexicology and Variational Linguistics (QLVL)</em></p> <p>Communicating through language is arguably one of the most complex processes that the most powerful computer we know, the human brain,  is capable of.  As a science, Linguistics aims to uncover the intricate system of patterns and structures that make up human language and that allow us to convey meaning through words and sentences. Although linguists have been investigating and describing these structures for ages, it is only recently that large amounts of electronic data and the computational power to analyse them have become available and have turned linguistics into a truly data-driven science. The primary data for linguistic research is ordinary, everyday language use like conversations or texts. These are collected in very large electronic text collections, containing millions of words and these collections are then mined for meaningful structures and patterns.  With increasing amounts of data and ever more advanced statistical algorithms,  these analyses are not longer feasible on individual servers but require the computational power of interconnected super computers. <br> In the presentation, I  will briefly describe two case studies of computationally heavy linguistic research.  A first case study has to do with the pre-processing of linguistic data. In order to find patterns at different levels of abstraction, each word in the text collection has to be enriched with information about its word class (noun, adjective, verb,..) and  syntactic function within the sentence (subject, direct object, indirect object...). A piece of software, called a parser, can add this information automatically.  For our research, we wanted to parse a text collection of 1.3 billion words, i.e. all issues from a 7 year period of 6 Flemish daily newspapers, representing a staggering 13 years of computing on an ordinary computer. Thanks to the K.U.Leuven's supercomputer, this could be done in just a few months. This data has now been made available to the wider research community.</p> <p><strong>Dr. Kris Heylen</strong> obtained a Master in Germanic Linguistics (2000) and a Master in Artificial Intelligence (2001) from the K.U.Leuven. In 2005, he was awarded a PhD in Linguistics at the K.U.leuven for his research into the statistical modelling of German word order variation. Since 2006, he is a postdoctoral fellow at the Leuven research unit Quantitative Lexicology and Variational Linguistics (QLVL), where he has further pursued his research into statistical language modelling with a focus on lexical patterns and word meaning in Dutch.</p> <h2><a name=\"Geris\"></a><span style=\"font-size: medium;\" rel=\"font-size: medium;\">Dr. ir. Lies Geris. Modeling in biomechanics and biomedical engineering</span></h2> <p><em>Katholieke Universiteit Leuven, Faculty of Engineering, Department of Mechanical Engineering, Division of Biomechanics and Engineering Design</em></p> <p>The first part of the presentation will discuss the development and applications of a mathematical model of fracture healing.  The model encompasses several key-aspects of the bone regeneration process, such as the formation of blood vessels and the influence of mechanical loading on the progress of healing.  The model is applied to simulate adverse healing conditions leading to a delayed or nonunion.  Several potential therapeutic approaches are tested in silico in order to find the optimal treatment strategy.   Going towards patient specific models will require even more computer power than is the case for the generic examples presented here.<br> The second part of the presentation will give an overview of other modeling work in the field of biomechanics and biomedical engineering, taking place in Leuven and Flanders.  The use of super computer facilities is required to meet the demand for more detailed models and patient specific modeling.</p> <p>Dr. ir. Liesbet Geris is a post-doctoral research fellow of the Research Foundation Flanders (FWO) working at the Division of Biomechanics and Engineering Design of the Katholieke Universiteit Leuven, Belgium.  From the K.U.Leuven, she received her MSc degree in Mechanical Engineering in 2002 and her PhD degree in Engineering in 2007, both summa cum laude.  In 2007 she worked for 4 months as an academic visitor at the Centre of Mathematical Biology of Oxford University.  Her research interests encompass the mathematical modeling of bone regeneration during fracture healing, implant osseointegration and tissue engineering applications.  The phenomena described in the mathematical models reach from the tissue level, over the cell level, down to the molecular level. She works in close collaboration with experimental and clinical researchers from the university hospitals Leuven, focusing on the development of mathematical models of impaired healing situations and the in silico design of novel treatment strategies.  She is the author of 36 refereed journal and proceedings articles, 5 chapters and reviews and 18 peer-reviewed abstracts.  She has received a number of awards, including the Student Award (2006) of the European Society of Biomechanics (ESB) and the Young Investigator Award (2008) of the International Federation for Medical and Biological Engineering (IFMBE).</p> <h2><a name=\"LacorPoedts\"></a><span style=\"font-size: medium;\" rel=\"font-size: medium;\">Prof. dr. ir. Chris Lacor<sup>1</sup> en Prof. dr. Stefaan Poedts<sup>2</sup>. Supercomputing in CFD and MHD</span></h2> <p><em><sup>1</sup>Vrije Universiteit Brussel, Faculty of Applied Sciences, Department of Mechanical Engineering<br> <sup>2</sup>Katholieke Universiteit Leuven, Faculty of Sciences, Department of Mathematics, Centre for Plasma Astrophysics</em></p> <p>CFD is an application field in which the available computing power is typically always lagging behind. With the increase of computer capacity CFD is looking towards more complex applications – because of increased geometrical complication or multidisciplinary aspects e.g. aeroacoustics, turbulent combustion, biological flows, etc – or more refined models such as Large Eddy Simulation (LES) or Direct Numerical Simulation (DNS).  In this presentation some demanding application fields of CFD will be highlighted, to illustrate this. <br> Computational MHD has a broad range of applications. We will survey some of the most CPU demanding applications in Flanders in the context of examples of the joint initiatives combining expertise from multiple disciplines, the VSC will hopefully lead to, such as the customised applications built in the COOLFluiD and AMRVAC-CELESTE3D projects.</p> <p><strong>Prof. dr. ir.  Chris Lacor</strong> obtained a degree in Electromechanical Engineering at VUB in 79 and his PhD in 86 at the same university. Currently he is Head of the Research Group Fluid Mechanics and Thermodynamics of the Faculty of Engineering at VUB. His main research field is Computational Fluid Dynamics (CFD). He stayed at the NASA Ames CFD Branch as an Ames associate in 87 and at EPFL  IMF in 89 where he got in contact with the CRAY supercomputers. In the early 90ies he was co-organizer of supercomputing lectures for the VUB/ULB CRAY X-MP computer. His current research focuses on Large Eddy Simulation, high-order accurate schemes and efficient solvers in the context of a variety of applications such as Computational Aeroacoustics, Turbulent Combustion, Non-Deterministic methods and Biological Flows. He is author of more than 100 articles in journals and on international conferences. He is also a fellow of the Flemish Academic Centre for Science and the Arts (VLAC).</p> <p><strong>Prof. dr. Stefaan Poedts</strong> obtained his degree in Applied Mathematics in 1984 at the K.U.Leuven. As 'research assistant' of the Belgian National Fund for Scientific Research he obtained a PhD in Sciences (Applied Mathematics) in 1988 at the same university. He spent two years at the Max-Planck-Institut für Plasmaphysik in Garching bei München and five years at the FOM-Instituut voor Plasmafysica 'Rijnhuizen'.  In October 1996 he returned to the K.U.Leuven as Research Associate of the FWO-Vlaanderen at the Centre for Plasma Astrophysics (CPA) in the Department of Mathematics. Since October 1, 2000 he is Academic Staff at the K.U.Leuven, presently as Full Professor. His research interests include solar astrophysics, space weather and controlled thermonuclear fusion. He co-authored two books and 170 journal articles on these subjects. He is president of the European Solar Physics Division (EPS &amp; EAS) and chairman of the Leuven Mathematical Modeling and Computational Science Centre. He is also member of ESA’s Space Weather Working Team and Solar System Working Group.</p>"
485,"","<table style=\"width: 100%;\">
<tbody>
<tr>
	<td>
		<img src=\"/assets/277\" alt=\"Logo VSC kleur\" title=\"Logo VSC kleur\" width=\"215\" height=\"100\">
	</td>
	<td align=\"right\">
		<span style=\"font-size: larger;\"><span style=\"color: #808080;\">March 23 2009</span></span><br>
		<strong><span style=\"font-size: larger;\"><span style=\"color: #00a4ca;\">Launch Flemish Supercomputer Center</span></span></strong>
	</td>
</tr>
</tbody>
</table><p style=\"text-align: justify;\">
	The Flemish Supercomputer Centre (Vlaams Supercomputer Centrum) cordially invites you to its official launch on <strong>23 March 2009</strong>.
</p><p style=\"text-align: justify; \">
	<br>
	<strong>Supercomputing</strong> is a crucial technology for the twenty-first century. Fast and efficient compute power is needed for leading scientific research, the industrial development and the competitiveness of our industry. For this reason the Flemish government and the five university associations have decided to set up a Flemish Supercomputer Centre (VSC). This centre will combine the clusters at the various Flemish universities in a single high-performance network and expand it with a large cluster that can withstand international comparison. The VSC will make available a high-performance and user-friendly supercomputer infrastructure and expertise to users from academic institutions and the industry.
</p><p style=\"margin: 0em 0em 0.5em;\">
	<strong>Program</strong>
</p><table style=\"width: 100%;\">
<tbody>
<tr>
	<td>
	</td>
	<td style=\"vertical-align: top;\">
		14.15
	</td>
	<td style=\"vertical-align: top;\">
		Scientists from various disciplines tell about their experiences with HPC and grid computing
	</td>
</tr>
<tr>
	<td>
	</td>
	<td style=\"vertical-align: top;\">
		16.15
	</td>
	<td style=\"vertical-align: top;\">
		Coffee break
	</td>
</tr>
<tr>
	<td>
	</td>
	<td style=\"vertical-align: top;\">
		17.00
	</td>
	<td style=\"vertical-align: top;\">
		Official program, in the presence of minister Ceysens, Flemish minister of economy, enterprise, science, innovation and foreign trade of Flanders.
	</td>
</tr>
<tr>
	<td>
	</td>
	<td style=\"vertical-align: top;\">
		18.30
	</td>
	<td style=\"vertical-align: top;\">
		Reception
	</td>
</tr>
</tbody>
</table><p>
	<a href=\"/events/vsc-launch-2009/program\" class=\"internal-link\">A detailed program is available by clicking on this link</a>. All presentations will be in English.
</p><p>
	<strong>Location</strong>
</p><p>
	Promotiezaal of the <a href=\"https://www.google.be/maps/search/Naamsestraat+22,+3000+Leuven/@50.805935,4.432983,583739m/data=!3m1!4b1?source=s_q&hl=nl&dg=dbrw&newdg=1\" target=\"_blank\">Universiteitshal of the K.U.Leuven,</a>
</p><p>
	<a href=\"https://www.google.be/maps/search/Naamsestraat+22,+3000+Leuven/@50.805935,4.432983,583739m/data=!3m1!4b1?source=s_q&hl=nl&dg=dbrw&newdg=1\" target=\"_blank\">Naamsestraat 22, 3000 Leuven</a>.
</p><p>
	<strong>Please register</strong> by 16 March 2009 using this electronic form.
</p><p style=\"text-align: justify;\">
	<strong>Plan and parking</strong>
</p><p style=\"text-align: justify;\">Parkings in the neighbourhood:<br>
</p><ul>
	<li>
	Parking garage Ladeuze, Mgr. Ladeuzeplein 20, Leuven.</li>
	<li>
	H. Hart parking, Naamsestraat 102, Leuven.</li>
</ul><p style=\"text-align: justify;\">
	The Universiteitshal is within walking distance of the train station of Leuven. Bus 1 (Heverlee Boskant) and 2 (Heverlee Campus) stop nearby.
</p><p style=\"margin: 0em;\">
	<a href=\"/events/vsc-launch-2009/figures\"><img src=\"/assets/81\" alt=\"invitation figures\" title=\"uitnodiging figuren 800px\" style=\"width: 800px; height: 201px; border-style: none !important;\"></a>
</p><p style=\"text-align: justify;\">
	The images at the top of this page are courtesy of <a href=\"https://www.numeca.com/home\" target=\"_blank\">NUMECA International</a> and research groups at Antwerp University, the Vrije Universiteit Brussel and the K.U.Leuven.
</p>"
487,"","<table style=\"width: 100%;\">
<tbody>
<tr>
	<td><a href=\"/assets/63\" target=\"_blank\"><img src=\"/assets/63\" alt=\"NUMECA simulatie schip met vrij wateroppervlak\" title=\"NUMECA simulatie schip met vrij wateroppervlak\" width=\"250\"></a>
	</td>
	<td>
		<p>Free-surface simulation.
		</p>
		<p>Figure courtesy of <a href=\"https://www.numeca.com/home\" target=\"_blank\">NUMECA International.</a>
		</p>
	</td>
</tr>
<tr>
	<td><a href=\"/assets/65\" target=\"_blank\"><img src=\"/assets/65\" alt=\"NUMECA turbine met koeling\" title=\"NUMECA turbine met koeling\" width=\"250\"></a>
	</td>
	<td>
		<p>Simulation of a turbine with coolring.
		</p>
		<p>Figure courtesy of <a href=\"https://www.numeca.com/home\" target=\"_blank\">NUMECA International.</a>
		</p>
	</td>
</tr>
<tr>
	<td><a href=\"/assets/949\" target=\"_blank\"><img src=\"/assets/949\" alt=\"UA climbfig extract\" title=\"UA climbfig extract\" width=\"250\"></a>
	</td>
	<td>
		<p>Purkinje cell model.
		</p>
		<p>Figure courtesy of Erik De Schutter, <a href=\"http://www.tnb.ua.ac.be\" target=\"_blank\">Theoretical Neurobiology,</a> Universiteit Antwerpen.
		</p>
	</td>
</tr>
<tr>
	<td><a href=\"/assets/69\" target=\"_blank\"><img src=\"/assets/69\" alt=\"UA NO2 on graphene\" title=\"UA NO2 on graphene\" width=\"250\"></a>
	</td>
	<td>
		<p>This figure shows the electron density at adsorption of NO<sub>2</sub> at on graphene, computed using density functional theory (using the software package absint).
		</p>
		<p>Figure courtesy of Francois Peeters, <a href=\"https://www.uantwerpen.be/en/research-groups/cmt/\" target=\"_blank\">Condensed Matter Theory (CMT) group</a>, Universiteit Antwerpen.
		</p>
	</td>
</tr>
<tr>
	<td align=\"center\"><a href=\"/assets/71\" target=\"_blank\"><img src=\"/assets/71\" align=\"middle\" title=\"UA DNA\" alt=\"UA DNA\" width=\"125\"></a>
	</td>
	<td>
		<p>Figure courtesy of Christine Van Broeckhoven, research group <a href=\"http://www.molgen.vib-ua.be/\" target=\"_blank\">Molecular Genetics</a>, Universiteit Antwerpen.
		</p>
	</td>
</tr>
<tr>
	<td><a href=\"/assets/73\" target=\"_blank\"><img src=\"/assets/73\" alt=\"CPA 3D simulation\" title=\"KULeuven CPA 3D simulation 200px\" width=\"250\"></a>
	</td>
	<td>Figure courtesy of the <a href=\"https://wis.kuleuven.be/CmPA\" target=\"_blank\">Centre for Plasma-Astrophysics</a>, K.U.Leuven.
	</td>
</tr>
<tr>
	<td><a href=\"/assets/75\" target=\"_blank\"><img src=\"/assets/75\" alt=\" CPA 3D shock\" title=\"KULeuven CPA 3D shock\" width=\"250\"></a>
	</td>
	<td>Figure courtesy of the <a href=\"https://wis.kuleuven.be/CmPA\" style=\"color: #2c6ea1; text-decoration: none; \" target=\"_blank\">Centre for Plasma-Astrophysics</a>, K.U.Leuven.
	</td>
</tr>
<tr>
	<td><a href=\"/assets/77\" target=\"_blank\"><img src=\"/assets/77\" alt=\"KULeuven CPA F15 grid\" title=\"KULeuven CPA F15 grid\" width=\"250\"></a>
	</td>
	<td>Figure courtesy of the <a href=\"https://wis.kuleuven.be/CmPA\" style=\"color: #2c6ea1; text-decoration: none; \" target=\"_blank\">Centre for Plasma-Astrophysics</a>, K.U.Leuven.
	</td>
</tr>
<tr>
	<td><a href=\"/assets/79\" target=\"_blank\"><img src=\"/assets/79\" alt=\"VUB CERN zaal\" title=\"VUB CERN zaal\" width=\"250\"></a>
	</td>
	<td>Figure courtesy of the research group <a href=\"http://w3.iihe.ac.be/\">Physics of Elementary Particles - IIHE</a>, Vrije Universiteit Brussel.
	</td>
</tr>
</tbody>
</table>"
489,"","<table border=\"0\" cellpadding=\"1\" cellspacing=\"1\" height=\"34\" width=\"1028\">
	<tbody>
		<tr>
			<td>
				<p>
					De eerste jaarlijkse bijeenkomst was een succes, met dank aan al de sprekers en deelnemers. We kijken al uit om de gebruikersdag volgend jaar te herhalen en om een aantal van de opgeworpen ideeën te implementeren.</p>
				<p>
					Hieronder vind je de presentaties van de VSC 2014 gebruikersdag:</p>
			</td>
			<td>
				<p>
					The first annual event was a success, thanks to all the presenters and participates. We are already looking forward to implementing some of the ideas generated and gathering again next year.</p>
				<p>
					Below you can download the presentations of the VSC 2014 userday:</p>
			</td>
		</tr>
	</tbody>
</table><p style=\"margin-left: 120px;\">
	<a href=\"/assets/315\">State of the VSC</a>, Flemish Supercomputer (<em>Dane Skow, HPC manager Hercules Foundation</em>)</p><p style=\"margin-left: 120px;\">
	<a href=\"/assets/303\">Computational Neuroscience</a> (<em>Michele Giugliano, University of Antwerp</em>)</p><p style=\"margin-left: 120px;\">
	<a href=\"/assets/305\">The value of HPC for Molecular Modeling applications</a> (<em>Veronique Van Speybroeck, Ghent University</em>)</p><p style=\"margin-left: 120px;\">
	<a href=\"/assets/307\">Parallel, grid-adaptive computations for solar atmosphere dynamics</a> (<em>Rony Keppens, University of Leuven</em>)</p><p style=\"margin-left: 120px;\">
	<a href=\"/assets/309\">HPC for industrial wind energy applications</a> (<em>Rory Donnelly, 3E</em>)</p><p style=\"margin-left: 120px;\">
	<a href=\"/assets/311\">The PRACE architecture and future prospects into Horizon 2020</a> (<em>Sergi Girona, PRACE</em>)</p><p style=\"margin-left: 120px;\">
	<a href=\"/assets/313\">Towards A Pan-European Collaborative Data Infrastructure, European Data Infrastructure</a> (<em>Morris Riedel, EUDAT</em>)</p><table border=\"0\" cellpadding=\"1\" cellspacing=\"1\" height=\"34\" width=\"1049\">
	<tbody>
		<tr>
			<td>
				Zoals je hieronder kan zijn was een mooi aantal deelnemers aanwezig. Wie wenst kan <a href=\"/events/userday-2014/pictures\">meer foto's</a> vinden onder de link.</td>
			<td>
				A nice number of participants attended the userday as you can see below. Click to see <a href=\"/events/userday-2014/pictures\">more pictures</a>.</td>
		</tr>
	</tbody>
</table><p style=\"text-align: center;\">
	<a href=\"/events/userday-2014/pictures\"><img src=\"/assets/301\" alt=\"More pictures\" style=\"width: 500px; height: 259px;\" rel=\"width: 500px; height: 259px;\" title=\"VSC User Day 16 01 2014\"></a></p>"
491,"","<p>
	<a href=\"http://www.theinternationalauditorium.be/\">The International Auditorium</a><br>
	Kon. Albert II laan 5, 1210 Brussels</p><p>
	The VSC User Day is the first annual meeting of current and prospective users of the Vlaams Supercomputing Center (VSC) along with staff and supporters of the VSC infrastructure. We will hold a series of presentations describing the status and results of the past year as well as afternoon sessions talking about plans and priorities for 2014 and beyond. This is an excellent opportunity to become more familiar with the VSC and it personnel, become involved in constructing plans and priorities for new projects and initiatives, and network with fellow HPC interested parties.<br>
	The day ends with a networking hour at 17:00 allowing time for informal discussions and followup from the day's activities.<br>
	 </p><p>
	<em>Program</em></p><table border=\"0\" cellpadding=\"1\" cellspacing=\"1\" style=\"width: 821px; height: 651px;\">
	<tbody>
		<tr>
			<td style=\"vertical-align: top;\">
				9:30h</td>
			<td style=\"vertical-align: top;\">
				Welcome coffee</td>
		</tr>
		<tr>
			<td style=\"vertical-align: top;\">
				10:00h</td>
			<td style=\"vertical-align: top;\">
				Opening VSC USER DAY<br>
				<em>Marc Luwel, Director Hercules Foundation</em></td>
		</tr>
		<tr>
			<td style=\"vertical-align: top;\">
				10:10h</td>
			<td style=\"vertical-align: top;\">
				State of the VSC, Flemish Supercomputer<br>
				<em>Dane Skow, HPC manager Hercules Foundation</em></td>
		</tr>
		<tr>
			<td style=\"vertical-align: top;\">
				10:40h</td>
			<td style=\"vertical-align: top;\">
				Computational Neuroscience<br>
				<em>Michele Giugliano, University of Antwerp</em></td>
		</tr>
		<tr>
			<td style=\"vertical-align: top;\">
				11:00h</td>
			<td style=\"vertical-align: top;\">
				The value of HPC for Molecular Modeling applications<br>
				<em>Veronique Van Speybroeck, Ghent</em> <em>University</em></td>
		</tr>
		<tr>
			<td style=\"vertical-align: top;\">
				11:20h</td>
			<td style=\"vertical-align: top;\">
				<span style=\"background-color: rgb(240, 248, 255);\" rel=\"background-color: rgb(240, 248, 255);\">Coffee Break and posters</span></td>
		</tr>
		<tr>
			<td style=\"vertical-align: top;\">
				11:50h</td>
			<td style=\"vertical-align: top;\">
				Parallel, grid-adaptive computations for solar atmosphere dynamics<br>
				<em>Rony Keppens, University of Leuven</em></td>
		</tr>
		<tr>
			<td style=\"vertical-align: top;\">
				12:10h</td>
			<td style=\"vertical-align: top;\">
				HPC for industrial wind energy applications<br>
				<em>Rory Donnelly, 3E</em></td>
		</tr>
		<tr>
			<td style=\"vertical-align: top;\">
				12:30h</td>
			<td style=\"vertical-align: top;\">
				<span style=\"background-color: rgb(240, 248, 255);\" rel=\"background-color: rgb(240, 248, 255);\">Lunch</span></td>
		</tr>
		<tr>
			<td style=\"vertical-align: top;\">
				13:30h</td>
			<td style=\"vertical-align: top;\">
				The PRACE architecture and future prospects into Horizon 2020<br>
				<em>Sergi Girona, PRACE</em></td>
		</tr>
		<tr>
			<td style=\"vertical-align: top;\">
				14:00h</td>
			<td style=\"vertical-align: top;\">
				EUDAT – Towards A Pan-European Collaborative Data Infrastructure, European Data Infrastructure<br>
				<em>Morris Reidel, EUDAT</em></td>
		</tr>
		<tr>
			<td style=\"vertical-align: top;\">
				14:20h</td>
			<td style=\"vertical-align: top;\">
				Breakout Sessions:<br>
				<br>
				1 : Long term strategy / Outreach,   information and Documentation<br>
				2 : Industry and Research / Visualization<br>
				3 : Training and support / Integration of Data and Computation</td>
		</tr>
		<tr>
			<td style=\"vertical-align: top;\">
				15:20h</td>
			<td style=\"vertical-align: top;\">
				<span style=\"background-color: rgb(240, 248, 255);\" rel=\"background-color: rgb(240, 248, 255);\">Coffee break and posters</span></td>
		</tr>
		<tr>
			<td style=\"vertical-align: top;\">
				16:00h</td>
			<td style=\"vertical-align: top;\">
				Summary Presentations from Rapporteurs breakout sessions</td>
		</tr>
		<tr>
			<td style=\"vertical-align: top;\">
				16:30h</td>
			<td style=\"vertical-align: top;\">
				Closing remarks and Q&amp;A<br>
				<em>Bart De Moor, chair Hercules Foundation</em></td>
		</tr>
		<tr>
			<td style=\"vertical-align: top;\">
				17:00h</td>
			<td style=\"vertical-align: top;\">
				<span style=\"background-color: rgb(240, 248, 255);\" rel=\"background-color: rgb(240, 248, 255);\">Network reception</span></td>
		</tr>
	</tbody>
</table>"
493,"","<p>De eerste jaarlijkse bijeenkomst was een succes, met dank aan al de sprekers en deelnemers. We kijken al uit om de gebruikersdag volgend jaar te herhalen en om een aantal van de opgeworpen ideeën te implementeren.
</p>
<p>Hieronder vind je de presentaties van de VSC 2014 gebruikersdag:
</p>"
495,"","<p>The first annual event was a success, thanks to all the presenters and participates. We are already looking forward to implementing some of the ideas generated and gathering again next year.</p><p>Below you can download the presentations of the VSC 2014 userday:</p>"
497,"","<p><a href=\"/assets/315\">State of the VSC</a>, Flemish Supercomputer (<em>Dane Skow, HPC manager Hercules Foundation</em>)<br><a href=\"/assets/303\">Computational Neuroscience</a> (<em>Michele Giugliano, University of Antwerp</em>)<br><a href=\"/assets/305\">The value of HPC for Molecular Modeling applications</a> (<em>Veronique Van Speybroeck, Ghent University</em>)<br><a href=\"/assets/307\">Parallel, grid-adaptive computations for solar atmosphere dynamics</a> (<em>Rony Keppens, University of Leuven</em>)<br><a href=\"/assets/309\">HPC for industrial wind energy applications</a> (<em>Rory Donnelly, 3E</em>)<br><a href=\"/assets/311\">The PRACE architecture and future prospects into Horizon 2020</a> (<em>Sergi Girona, PRACE</em>)<br><a href=\"/assets/313\">Towards A Pan-European Collaborative Data Infrastructure, European Data Infrastructure</a>(<em>Morris Riedel, EUDAT</em>)</p><p><a href=\"/events/userday-2014/program\">Full program of the day</a></p>"
499,"","<p>Zoals je hieronder kan zijn was een mooi aantal deelnemers aanwezig. Wie wenst kan <a href=\"/events/userday-2014/pictures\">meer foto's</a> vinden onder de link.</p>"
501,"","<p>A nice number of participants attended the userday as you can see below. Click to see <a href=\"/events/userday-2014/pictures\">more pictures</a>.</p>"
503,"","<p>
	<a href=\"/events/userday-2014/pictures\"><img src=\"/assets/301\" alt=\"More pictures\" title=\"VSC User Day 16 01 2014\"></a>
</p>"
505,"","<h1>Next- generation Supercomputing in Flanders: value creation for your business!</h1><p><strong>Tuesday 27 Januari 2015</strong>
</p><p>Technopolis Mechelen<br>
</p><p>The first industry day was a success, thanks to all the presenters and participates. We especially would like to thank the minister for his presence. The success stories of European HPC centres showed how benificial HPC can be for all kinds of industry. The testimonials of the Flemish firms who already are using  large scale computing could only stress the importance HPC. We will continue to work on the ideas generated at this meeting so that VSC can strengthen its service to industry.
</p><p style=\"text-align: center;\"><a href=\"/events/industryday-2015/pictures\"><img src=\"/assets/317\" alt=\"All participants to the VSC industry day 2015. More pictures after the click\" title=\" 86P6117\" width=\"80%\"></a>
</p><p>Below you can download the presentations of the VSC 2015 industry day. <a href=\"/events/industryday-2015/pictures\">Pictures</a> are published.
</p><p>The importance of High Performance Computing for future science, technology and economic growth<br>
			<em>Prof. Dr Bart De Moor, Herculesstichting </em>
		</p><p><a href=\"/assets/319\">The 4 Forces of Change for Supercomputing</a><br>
			<em>Cliff Brereton, director Hartree Centre (UK) </em>
		</p><p><a href=\"/assets/321\">The virtual Engineering Centre and its multisector virtual prototyping activities</a><br>
			<em>Dr Gillian Murray, Director UK virtual engineering centre (UK)</em>
		</p><p><a href=\"/assets/323\">How SMEs can benefit from High-Performence-Computing</a><br>
			<em>Dr Andreas Wierse, SICOS BW GmbH (D) </em>
		</p><p><a href=\"/assets/325\">European HPC landscape- its initiatives towards supporting innovation and its regional perspectives</a><br>
			<em>Serge Bogaerts, HPC & Infrastructure Manager, CENAERO (B)<br>
			Belgian delegate to the Prace Council</em>
		</p><p><a href=\"/assets/327\">Big data and Big Compute for Drug Discovery & Development of the future</a><br>
			<em>Dr Pieter Peeters, Senior Director Computational Biology, Janssen R&D (B) </em>
		</p><p><a href=\"/assets/329\">HPC key enabler for R&D innovation @ Bayer CropScience</a><br>
			<em>Filip Nollet, Computation Life Science Platform<br>
			Architect Bayer Cropscience (B) </em>
		</p><p><a href=\"/assets/331\">How becoming involved in VSC: mechanisms for HPC industrial newcomers</a><br>
			<em>Dr Marc Luwel, Herculesstichting</em><br>
			<em>Dr Ewald Pauwels, Ugent - Tier1</em>
		</p><p>Closing<br>
			<em>Philippe Muyters, Flemish Minister of Economics and Innovation</em>
		</p><p><a href=\"/events/industryday-2015/program\">Full program</a></p>"
507,"","<p>The VSC Industry day is organised for the first time to create awareness about the potential of HPC for industry and to help firms overcome the hurdles to use supercomputing. We are proud to present an exciting program with success stories of European HPC centres that successfully collaborate with industry and testimonials of some Flemish firms who already have discovered the opportunities of large scale computing. The day ends with a networking hour allowing time for informal discussions.</p><table border=\"0\" cellpadding=\"7\" cellspacing=\"0\" width=\"80%\">
	<tbody>
		<tr>
			<td colspan=\"3\" height=\"9\" valign=\"TOP\">
			<p><strong>Program - Next-generation supercomputing in Flanders: value creation for your business! </strong></p>
			</td>
		</tr>
		<tr>
			<td colspan=\"1\" height=\"18\" valign=\"TOP\" width=\"15%\">
			<p>13.00-13.30</p>
			</td>
			<td colspan=\"2\" height=\"18\" valign=\"TOP\" width=\"85%\">
			<p>Registration</p>
			</td>
		</tr>
		<tr>
			<td height=\"18\" valign=\"TOP\" width=\"15%\">
			<p>13.30-13.35</p>
			</td>
			<td colspan=\"2\" height=\"18\" valign=\"TOP\" width=\"40%\">
			<p>Welcome and introduction<br>
			<em>Prof. Dr Colin Whitehouse (chair) </em></p>
			</td>
		</tr>
		<tr>
			<td height=\"36\" valign=\"TOP\" width=\"15%\">
			<p>13.35-13.45</p>
			</td>
			<td colspan=\"2\" height=\"36\" valign=\"TOP\" width=\"85%\">
			<p>The importance of High Performance Computing for future science, technology and economic growth<br>
			<em>Prof. Dr Bart De Moor, Herculesstichting </em></p>
			</td>
		</tr>
		<tr>
			<td height=\"18\" valign=\"TOP\" width=\"15%\">
			<p>13.45-14.05</p>
			</td>
			<td colspan=\"2\" height=\"18\" valign=\"TOP\" width=\"85%\">
			<p>The 4 Forces of Change for Supercomputing<br>
			<em>Cliff Brereton, director Hartree Centre (UK) </em></p>
			</td>
		</tr>
		<tr>
			<td height=\"27\" valign=\"TOP\" width=\"15%\">
			<p>14.05-14.25</p>
			</td>
			<td colspan=\"2\" height=\"27\" valign=\"TOP\" width=\"85%\">
			<p>The virtual Engineering Centre and its multisector virtual prototyping activities<br>
			<em>Dr Gillian Murray, Director UK virtual engineering centre (UK)</em></p>
			</td>
		</tr>
		<tr>
			<td height=\"18\" valign=\"TOP\" width=\"15%\">
			<p>14.25-14.45</p>
			</td>
			<td colspan=\"2\" height=\"18\" valign=\"TOP\" width=\"85%\">
			<p>How SMEs can benefit from High-Performence-Computing<br>
			<em>Dr Andreas Wierse, SICOS BW GmbH (D) </em></p>
			</td>
		</tr>
		<tr>
			<td colspan=\"1\" height=\"18\" valign=\"TOP\" width=\"15%\">
			<p>14.45-15.15</p>
			</td>
			<td colspan=\"2\" height=\"18\" valign=\"TOP\" width=\"85%\">
			<p>Coffeebreak</p>
			</td>
		</tr>
		<tr>
			<td height=\"18\" valign=\"TOP\" width=\"15%\">
			<p>15.15-15.35</p>
			</td>
			<td colspan=\"2\" height=\"18\" valign=\"TOP\" width=\"85%\">
			<p>European HPC landscape- its initiatives towards supporting innovation and its regional perspectives<br>
			<em>Serge Bogaerts, HPC &amp; Infrastructure Manager, CENAERO (B)<br>
			Belgian delegate to the Prace Council</em></p>
			</td>
		</tr>
		<tr>
			<td height=\"27\" valign=\"TOP\" width=\"15%\">
			<p>15.35-15.55</p>
			</td>
			<td colspan=\"2\" height=\"27\" valign=\"TOP\" width=\"85%\">
			<p>Big data and Big Compute for Drug Discovery &amp; Development of the future<br>
			<em>Dr Pieter Peeters, Senior Director Computational Biology, Janssen R&amp;D (B) </em></p>
			</td>
		</tr>
		<tr>
			<td height=\"18\" valign=\"TOP\" width=\"15%\">
			<p>15.55-16.15</p>
			</td>
			<td colspan=\"2\" height=\"18\" valign=\"TOP\" width=\"85%\">
			<p>HPC key enabler for R&amp;D innovation @ Bayer CropScience<br>
			<em>Filip Nollet, Computation Life Science Platform<br>
			Architect Bayer Cropscience (B) </em></p>
			</td>
		</tr>
		<tr>
			<td height=\"27\" valign=\"TOP\" width=\"15%\">
			<p>16.15-16.35</p>
			</td>
			<td colspan=\"2\" height=\"27\" valign=\"TOP\" width=\"85%\">
			<p>How becoming involved in VSC: mechanisms for HPC industrial newcomers<br>
			<em>Dr Marc Luwel, Herculesstichting</em></p>
			</td>
		</tr>
		<tr>
			<td height=\"18\" valign=\"TOP\" width=\"15%\">
			<p>16.35-17.05</p>
			</td>
			<td colspan=\"2\" height=\"18\" valign=\"TOP\" width=\"85%\">
			<p>Q&amp;A discussion<br>
			Panel/chair</p>
			</td>
		</tr>
		<tr>
			<td height=\"18\" valign=\"TOP\" width=\"15%\">17.05-17.15</td>
			<td colspan=\"2\" height=\"18\" valign=\"TOP\" width=\"85%\">
			<p>Closing<br>
			<em>Philippe Muyters, Flemish Minister of Economics and Innovation</em></p>
			</td>
		</tr>
		<tr>
			<td height=\"18\" valign=\"TOP\" width=\"15%\">17.15-18.15</td>
			<td colspan=\"2\" height=\"18\" valign=\"TOP\" width=\"85%\">Networking reception</td>
		</tr>
	</tbody>
</table>"
509,"","<p>Below you find the complete list of Tier-1-projects since the start of the regular project application programme.</p>"
511,"User support","<p>KU Leuven/UHasselt: <a href=\"mailto:HPCinfo@kuleuven.be\">HPCinfo@kuleuven.be</a><br>
	Ghent University: <a href=\"mailto:hpc@ugent.be\">hpc@ugent.be</a><br>
	Antwerp University: <a href=\"mailto:hpc@uantwerpen.be\">hpc@uantwerpen.be</a><br>
	VUB: <a href=\"mailto:hpc@vub.ac.be\">hpc@vub.ac.be</a>
</p>
<p><a href=\"/support/contact-support\">Please take a look at the information that you should provide with your support question.</a>.
</p>"
513,"","<h2>Tier-1</h2><ul><li><a href=\"http://hervsc.staging.statik.be/infrastructure/hardware/hardware-tier1-muk\">The main tier-1 system is muk</a>, aimed at large parallel computing jobs that require a high-bandwidth low-latency interconnect. Compute time on muk is only available upon approval of a project. See the <a href=\"https://vscentrum.be/en/tier1-allocation\">pages on tier-1 allocation</a>.</li></ul><h2>Experimental setup</h2><ul><li><a href=\"http://hervsc.staging.statik.be/infrastructure/hardware/k20x-phi-hardware\">There is a small GPU and Xeon Phi test system</a> which is can be used by all VSC members on request (though a project approval is not required at the moment). <a href=\"http://hervsc.staging.statik.be/infrastructure/hardware/k20x-phi-hardware\">The documentation for this system is under development</a>.</li></ul><h2>Tier-2</h2><p>Four university-level cluster groups are also embedded in the VSC and partly funded from VSC budgets:</p><ul><li><a href=\"http://hervsc.staging.statik.be/infrastructure/hardware/hardware-ua\">The UAntwerpen clusters (hopper and turing)</a></li><li><a href=\"http://hervsc.staging.statik.be/infrastructure/hardware/hardware-vub\">The VUB cluster (hydra)</a></li><li><a href=\"http://www.ugent.be/hpc/en/infrastructure/overzicht.htm\">The UGent local clusters</a></li><li><a href=\"http://hervsc.staging.statik.be/infrastructure/hardware/hardware-kul\">The KU Leuven/UHasselt cluster (ThinKing and Cerebro)</a></li></ul>"
517,"","<p>The only short answer to this question is: maybe yes, maybe no. There are a number of things you need to figure out before.</p><h2>Will my application run on a supercomputer?</h2><p>Maybe yes, maybe no. All VSC clusters - and the majority of large supercomputers in the world - run the Linux operation system. So it doesn't run Windows or OS X applications. Your application will have to support Linux, and the specific variants that we use on our clusters, but these are popular versions and rarely pose problems. </p><p>Next supercomputers are not really build to run interactive applications well. They are built to be shared by many people and using command line applications. There are several issues:</p><ul><li>Since you share the machine with many users, you may have to wait a while before your job might launch. This is organised through a queueing system: you submit your job to a waiting line and a scheduler decides who's next to run based on a large number of parameters: job duration, number of processors needed, have you run a lot of jobs recently, ... So by the time you job starts, you may have gone home already.</li><li>You don't sit at a monitor attached to the supercomputer. Even though that supercomputers can also be used for visualisation, you'll still need a suitable system on your desk to show the final image, and use software that can send the drawing commands or images generated on the supercomputer to your desktop.</li></ul><h2>Will my application run faster on a supercomputer?</h2><p>You'll be disappointed to hear that the answer is actually quite often \"no\". It is not uncommon that an application runs faster on a good workstation than on a supercomputer. Supercomputers are optimised for large applications that access large chunks of memory (RAM or disk) in a particular way and are very parallel, i.e., they can keep a lot of processor cores busy. Their CPUs are optimised to do as much work in parallel as fast as possible, at the cost of lower performance for programs that don't exploit parallelism, while high-end workstation processors are more optimised for those programs that run sequentially or don't use a lot of parallelism and often have disksystems that can better deal with many small files.</p><p>That being said, even that doesn't have to be disastrous. Parallelism can come in different forms. Sometimes you may have to run the same program for a large number of test cases, and if the memory consumption for a program for a simple test case is reasonable, you may be able to run a lot of instances of that program simultaneously on the same multi-core processor chip. This is called <em>capacity computing</em>. And some applications are very well written and can exploit all the forms of parallelism that a modern supercomputer offers, provided you solve a large enough problem with that program. This is called <em>capability computing</em>. We support both at the VSC.<br></p><h2>OK, my application can exploit a supercomputer. What's next?</h2><p>Have a look our web page on <a href=\"/en/access-and-infrastructure/requesting-access\">requesting access in the general section</a>. It explains who can get access to the supercomputers. And as that text explains, you'll may need to install some additional software the system from which you want to access the clusters (which for the majority of our users is their laptop or desktop computer).</p><p>Basically, you communicate with the cluster through a protocol called \"SSH\" which stands for \"Secure SHell\". It encrypts all the information that is passed to the clusters, and also provides an authentication mechanism that is a bit safer than just sending passwords. The protocol can be used both to get a console on the system (a \"command line interface\" like the one offered by CMD.EXE on Widows or the term app on OS X) and to transfer files to the system. The absolute minimum you need before you can actually request your account, is a SSH client to generate the key that will be used to talk to the clusters. For Windows, you can <a href=\"/client/windows/keys-putty\">use PuTTY</a> (freely available, see <a href=\"/client/windows/console-putty\">the link on our PuTTY page</a>), on macOS/OS X you can <a href=\"/client/macosx/keys-openssh\">use the built-in OpenSSH client</a>, and Linux systems typically also <a href=\"/client/linux/keys-openssh\">come with OpenSSH</a>. But to actually use the clusters, you may want to install some additional software, such as a GUI sftp client to transfer files. We've got links to a lot of useful client software <a href=\"/cluster-doc/access-data-transfer\">on our web page on access and data transfer</a>.</p><h2>Yes, I'm ready</h2><p>Then follow the links on our <a href=\"/cluster-doc/account-request\">user portal page on requesting an account</a>. And don't forget we've got <a href=\"/en/education--training\">training programs</a> to get you started and <a href=\"/support/contact-support\">technical support</a> for when you run into trouble.</p>"
519,"","<p>Even if you don't do software development yourself (and software development includes, e.g., developing R- or Matlab routines), working on a supercomputer differs from using a PC, so some training is useful for everybody.</p><h2>Linux</h2><p>If you are familiar with a Linux or UNIX environment, there is no need to take any course. Working with Linux on a supercomputer is not that different from working with Linux on a PC, so you'll likely find your way around quickly.</p><p>Otherwise, there are several options to learn more about Linux</p><ul><li>We have some <a href=\"/cluster-doc/using-linux\">very basic pages on Linux use</a> in the user documentation. The <a href=\"/cluster-doc/using-linux/basic-linux-usage\">introductory page</a> contains a number of links to courses on the web.</li><li>Several institutions at the VSC also organise regular Linux introductory courses. Check the \"<a href=\"/en/education--training\">Education and Training</a>\" page on upcoming courses.</li></ul><h2>A basic HPC introduction</h2><p>Such a course at the VSC has a double goal: Learning more about HPC in general but also about specific properties of the system at the VSC that you need to know to run programs sufficiently efficiently.</p><ul><li>Several institutions at the VSC organise periodic introductions to their infrastructure or update sessions for users when new additions are made to the infrastructure. Check the \"<a href=\"/en/education--training\">Education and Training</a>\" page on upcoming courses.</li><li>We are working on a new introductory text that will soon be available on this site. The text covers both the software that you need to install on your own computer and working on the clusters, with specific information for your institution.</li><li>Or you can work your way through the documentation on the user portal. This is probably sufficient if you are already familiar with supercomputers. Of particular interest may be the page on our <a href=\"/cluster-doc/software/modules\">implementation of the module system</a>, the pages on <a href=\"/cluster-doc/running-jobs\">running jobs</a> (as there are different job submission systems around, we use Torque/Moab), and the <a href=\"/en/infrastructure/hardware\">pages about the available hardware</a> that also contain information about the settings needed for each specific system.</li></ul><h2>What next?</h2><p>We also run courses on many other aspects of supercomputing such as program development or use of specific applications. As the other courses, they are announced on our \"<a href=\"/en/education--training\">Education and Training</a>\" page. Or you can read a some good books, look at training programs offered at the European level through PRACE or check some web courses. We maintain links to several of those on the \"<a href=\"/support/tut-book\">Tutorials and books</a>\" pages.</p><p>Be aware that some tools that are useful to prototype applications on a PC, may be very inefficient when run at a large scale on a supercomputer. Matlab programs can often be accelerated through compiling with the Matlab compiler. R isn't the most efficient tool either. And Python is an excellent \"glue language\" to get a number of applications or optimised (non-Python) libraries to work together, but shouldn't be used for entire applications that consume a lot of CPU time either. We've got courses on several of those languages where you also learn how to use them efficiently, and you'll also notice that on some clusters there are restrictions on the use of these tools.</p>"
521,"","<nav class=\"nav nav--list \" id=\"widget-334\" role=\"navigation\">
            <h2 class=\"\">Preparing to use the clusters</h2>
    
        
    <ul class=\"menu1 \">
            
                                    
        <li class=\"first even\">
            <a href=\"http://hervsc.staging.statik.be/cluster-doc/account-request\" class=\"nav__link nav__link--accountrequest\">Account request</a>
                    </li>
                    
                                    
        <li class=\"odd\">
            <a href=\"http://hervsc.staging.statik.be/en/user-portal/account-management\" class=\"nav__link nav__link--accountmanagement\">Account management</a>
                    </li>
                    
                                    
        <li class=\"even\">
            <a href=\"http://hervsc.staging.statik.be/support/contact-support\" class=\"nav__link nav__link--supportservices\">User support</a>
                    </li>
                    
                                    
        <li class=\"odd last\">
            <a href=\"http://hervsc.staging.statik.be/support/tut-book\" class=\"nav__link nav__link--tutorialsandbooks\">Tutorials and books</a>
                    </li>
                </ul>

</nav>"
523,"","<p>© FWO</p><p>Use of this website means that you acknowledge and accept the terms and conditions below.</p><h3>Content disclaimer</h3><p>The FWO takes great care of its website and strives to ensure that all the information provided is as complete, correct, understandable, accurate and up-to-date as possible. In spite of all these efforts, the FWO cannot guarantee that the information provided on this website is always complete, correct, accurate or up-to-date. Where necessary, the FWO reserves the right to change and update information at its own discretion. The publication of official texts (legislation, Flemish Parliament Acts, regulations, etc.) on this website has no official character.</p><p>If the information provided on or by this website is inaccurate then the FWO will do everything possible to correct this as quickly as possible. Should you notice any errors, please contact the website administrator: <a href=\"mailto:kurt.lust@uantwerpen.be\">kurt.lust@uantwerpen.be</a>. The FWO makes every effort to ensure that the website does not become unavailable as a result of technical errors. However, the FWO cannot guarantee the website's availability or the absence of other technical problems.</p><p>The FWO cannot be held liable for any direct or indirect damage arising from the use of the website or from reliance on the information provided on or through the website. This also applies without restriction to all losses, delays or damage to your equipment, software or other data on your computer system.</p><h3>Protection of personal data</h3><p>The FWO is committed to protecting your privacy. Most information is available on or through the website without your having to provide any personal data. In some cases, however, you may be asked to provide certain personal details. In such cases, your data will be processed in accordance with the Law of 8 December 1992 on the protection of privacy with regard to the processing of personal data and with the Royal Decree of 13 February 2001, which implements the Law of 8 December 1992 on the protection of privacy with regard to the processing of personal data.</p><p>The FWO provides the following guarantees in this context:</p><ul><li>Your personal data will be collected and processed only in order to provide you with the information or service you requested online. The processing of your personal data is limited to the intended objective.</li><li>Your personal data will not be disclosed to third parties or used for direct marketing purposes unless you have formally consented to this by opting in.</li><li>The FWO implements the best possible safety measures in order to prevent abuse of your personal data by third parties.</li></ul><h3>Providing personal information through the online registration module</h3><p>By providing your personal information, you consent to this personal information being recorded and processed by the FWO and its representatives. The information you provided will be treated as confidential.</p><p>The FWO may also use your details to invite you to events or keep you informed about activities of the VSC.</p><h3>Cookies</h3><h3>What are cookies and why do we use them?</h3><p>Cookies are small text or data files that a browser saves on your computer when you visit a website.</p><p>This web site saves cookies on your computer in order to improve the website’s usability and also to analyse how we can improve our web services.</p><h3>Which cookies does this website use?</h3><ul><li>Functional cookies: Cookies used as part of the website’s security. These cookies are deleted shortly after your visit to our website ends.</li><li>Non-functional cookies<ul><li><em><strong>Google Analytics: _GA<br></strong></em>We monitor our website’s usage statistics with Google Analytics, a system which loads a number of cookies whenever you visit the website. These _GA cookies allow us to check how many visitors our website gets and also to collect certain demographic details (e.g. country of origin).</li></ul></li></ul><h3>Can you block or delete cookies?</h3><p>You can prevent certain cookies being installed on your computer by adjusting the settings in your browser’s options. In the ‘privacy’ section, you can specify any cookies you wish to block.</p><p>Cookies can also be deleted in your browser’s options via ‘delete browsing history’.</p><p>We use cookies to collect statistics which help us simplify and improve your visit to our website. As a result, we advise you to allow your browser to use cookies.</p><h3>Hyperlinks and references</h3><p>The website contains hyperlinks which redirect you to the websites of other institutions and organisations and to information sources managed by third parties. The FWO has no technical control over these websites, nor does it control their content, which is why it cannot offer any guarantees as to the completeness or correctness of the content or availability of these websites and information sources.</p><p>The provision of hyperlinks to other websites does not imply that the FWO endorses these external websites or their content. The links are provided for information purposes and for your convenience. The FWO accepts no liability for any direct or indirect damage arising from the consultation or use of such external websites or their content.</p><h3>Copyright</h3><p>All texts and illustrations included on this website, as well as its layout and functionality, are protected by copyright. The texts and illustrations may be printed out for private use; distribution is permitted only after receiving the authorisation of the FWO. You may quote from the website providing you always refer to the original source. Reproductions are permitted, providing you always refer to the original source, except for commercial purposes, in which case reproductions are never permitted, even when they include a reference to the source.</p><p>Permission to reproduce copyrighted material applies only to the elements of this site for which the FWO is the copyright owner. Permission to reproduce material for which third parties hold the copyright must be obtained from the relevant copyright holder.</p>"
529,"relates to","<ul><li><a href=\"http://statik.be\">muk cluster Ugent</a></li></ul>"
531,"Auick access","<ul><li><a href=\"http://statik.be\">available server software</a></li></ul>"
533,"New user","<p><a href=\"http://statik.be\">eerste link</a></p>"
535,"","<p>The UGent compute infrastructure consists of several specialised clusters, jointly called Stevin. These clusters share a lot of their file space so that users can easily move between clusters depending on the specific job they have to run.
</p><h2>Login nodes</h2><p>The HPC-UGent Tier-2 login nodes can be access through the generic name <code>login.hpc.ugent.be</code>.
</p><h3>Connecting to a specific login node</h3><p>There are multiple login nodes (gligar01-gligar03) and you will be connected with one of them when using the generic alias <code>login.hpc.ugent.be</code>. (You can check which one you are connected to using the <code>hostname</code> command).
</p><p>If you need to connect with as specific login node, use either <code style=\"font-size: 14px;\">gligar01.ugent.be</code>, <code>gligar02.ugent.be</code>, or <code>gligar03.ugent.be</code><span class=\"redactor-invisible-space\">.</span>
</p><h2>Compute clusters</h2><table>
<tbody>
<tr>
	<td>
	</td>
	<td>#nodes
	</td>
	<td>CPU
	</td>
	<td>Mem/node
	</td>
	<td>Diskspace/node
	</td>
	<td>Network
	</td>
</tr>
<tr>
	<td>
		<h3>delcatty</h3>
	</td>
	<td>128
	</td>
	<td>2 x 8-core Intel E5-2670<br>(Sandy Bridge @ 2.6 GHz)
	</td>
	<td>64 GB
	</td>
	<td>400 GB
	</td>
	<td>FDR InfiniBand
	</td>
</tr>
<tr>
	<td>
		<h3>phanpy</h3>
	</td>
	<td>16
	</td>
	<td>2 x 12-core Intel E5-2680v3<br>(Haswell-EP @ 2.5 GHz)
	</td>
	<td>512 GB
	</td>
	<td>3x 400 GB (SSD, striped)
	</td>
	<td>FDR InfiniBand
	</td>
</tr>
<tr>
	<td>
		<h3>golett</h3>
	</td>
	<td>196
	</td>
	<td>2 x 12-core Intel E5-2680v3<br>(Haswell-EP @ 2.5 GHz)
	</td>
	<td>64 GB
	</td>
	<td>500 GB
	</td>
	<td>FDR-10 InfiniBand<br><br>
	</td>
</tr>
<tr>
	<td>
		<h3>swalot</h3>
	</td>
	<td>128
	</td>
	<td>2 x 10-core Intel E5-2660v3<br>(Haswell-EP @ 2.6 GHz)
	</td>
	<td>128 GB
	</td>
	<td>1 TB
	</td>
	<td>FDR InfiniBand<br><br>
	</td>
</tr>
<tr>
	<td>
		<h3>skitty</h3>
	</td>
	<td>72
	</td>
	<td>2 x 18-core Intel Xeon Gold<br>6140 (Skylake @ 2.3 GHz)
	</td>
	<td>192 GB
	</td>
	<td>1 TB<br>240 GB SSD
	</td>
	<td>EDR InfiniBand
	</td>
</tr>
<tr>
	<td>
		<h3>victini</h3>
	</td>
	<td>96
	</td>
	<td>2 x 18-core Intel Xeon Gold<br>6140 (Skylake @ 2.3 GHz)
	</td>
	<td>96 GB
	</td>
	<td>1 TB<br>240 GB SSD
	</td>
	<td>10 GbE
	</td>
</tr>
</tbody>
</table><p>Only clusters with an InfiniBand interconnect network are suited for multi-node jobs. Other clusters are for single-node usage only.<br>
</p><h2>Shared storage</h2><p>General Parallel File System (GPFS) partitions:
</p><ul class=\"list--unordered\">
	<li><code>$VSC_HOME</code>: 35 TB</li>
	<li><code>$VSC_DATA</code>: 702 TB</li>
	<li><code>$VSC_SCRATCH</code>: 1 PB (equivalent to <code>$VSC_SCRATCH_KYUKON</code>)</li>
	<li><code>$VSC_SCRATCH_PHANPY</code>: 35TB (very fast, powered by SSDs)</li>
</ul>"
537,"","<p>When using the VSC-infrastructure for your research, you must acknowledge the VSC in all relevant publications. This will help the VSC secure funding, and hence you will benefit from it in the long run as well. It is also a contractual obligation for the VSC. </p><p>Please use the following phrase to do so in Dutch “De rekeninfrastructuur en dienstverlening gebruikt in dit werk, werd voorzien door het VSC (Vlaams Supercomputer Centrum), gefinancierd door het FWO en de Vlaamse regering – departement EWI”, or in English: “The computational resources and services used in this work were provided by the VSC (Flemish Supercomputer Center), funded by the Research Foundation - Flanders (FWO) and the Flemish Government – department EWI”.</p><p>Moreover, if you are in the KU Leuven association, you are also requested to add the relevant papers to the virtual collection \"High Performance Computing\" in Lirias so that we can easily generate the publication lists with relevant publications.</p>"
539,"","<p>Need technical support? <a href=\"/support/contact-support\">Contact your local help desk</a>.</p>"
543,"","<p>In order to smoothly go through account creation for students process several actions from the lecturer are required.
</p><ol class=\"list--ordered\">
	<li>Submit the request to<em> <a href=\"mailto:hpcinfo@icts.kuleuven.be?subject=Accounts%20requests%20for%20students%20attending%20the%20course\">HPCinfo(at)icts.kuleuven.be</a> </em>providing a short description of the course and explanation why HPC facilities are necessary for teaching the course. Please also add the attachment with the list of students attending the course (<span style=\"color:#FF0000;\">2 weeks before the beginning of the course</span>).</li>
	<li>Send the information to students that they have <span style=\"color:#FF0000;\">1 week time window </span>to apply for the account (the last day when account creating can be processed is the day before the course starts). Students should follow the <a href=\"/cluster-doc/account-request\">regular account creation routine</a>, which starts with generating private-public key pair and ends with submitting the public key via <a href=\"https://account.vscentrum.be/\">our account management web site</a>. After <span style=\"color:#FF0000;\">1 week</span> the lists of students that already submitted the request for the account and corresponding vsc-account numbers will be send to the lecturer.</li>
	<li>
	The students should be informed to bring the private key with them to be able to connect and attend the course.
	</li>
	<li>Since introductory credits are supposed to be used for private projects (e.g. master thesis computations) we encourage to create the project which will be used for computations related to the course. This will also give a lecturer an opportunity of tracing the use of the cluster during the course. For more information about the procedure of creating the project please refer to <a href=\"/cluster-doc/running-jobs/credit-system-basics\">the page on credit system basics</a>. Once the project is accepted, the students that already applied for the account will be automatically added to the project (<span style=\"color:#FF0000;\">1 week</span> <span style=\"color:#FF0000;\">before the beginning of the course</span>).</li>
	<li>Students that failed to submit request in a given time will have to follow regular procedure of applying for the account involving communication with the HPC support staff and delaying the account creation process (these students will have to motivate the reason of applying for the account and send a request for using the project credits). Students that submit the requests later than <span style=\"color:#FF0000;\">2 days before the beginning of the course </span>are not guaranteed to get the account in time.</li>
	<li>Both the accounts and the generated key-pairs are strictly <span style=\"color:#FF0000;\">PRIVATE</span> and students are not supposed to share the accounts, not even for the purpose of the course.</li>
	<li>Please remember to instruct your students to <span style=\"color:#FF0000;\">bring the private key</span> to the class. Students may forget it and without the key they will not be able to login to the cluster even if they have the accounts.</li>
	<li>If the reservation of few nodes is necessary during the exercise classes please let us know <span style=\"color:#FF0000;\">1 week</span> <span style=\"color:#FF0000;\">before the exercise class</span>, so that it can be scheduled. To submit the job during the class the following command should be used:
	<pre>$ qsub -A project-name -W group_list=project-name script-file</pre>
	where project-name refers to the project created by the lecturer for the purpose of the course.</li><li>Make sure that the software to connect to the cluster (Putty, Xming, Filzezilla, NX) is available in pc-class that will be used during the course. For KU Leuven courses: please follow the procedure at <a href=\"https://icts.kuleuven.be/sc/pcklas/ictspcklassen\" target=\"_blank\">https://icts.kuleuven.be/sc/pcklas/ictspcklassen</a> (<span style=\"color:#FF0000;\">1 month before the beginning of the course</span>).</li>
</ol>"
545,"","<h2>Purpose</h2><p>Estimating the amount of memory an application will use during execution is often non trivial, especially when one uses third-party software. However, this information is valuable, since it helps to determine the characteristics of the compute nodes a job using this application should run on.
</p><p>Although the tool presented here can also be used to support the software development process, better tools are almost certainly available.
</p><p>Note that currently only single node jobs are supported, MPI support may be added in a future release.
</p><h2>Prerequisites</h2><p>The user should be familiar with the linux bash shell.
</p><h2>Monitoring a program</h2><p>To start using monitor, first load the appropriate module:
</p><pre>$ module load monitor
</pre><p>Starting a program, e.g., simulation, to monitor is very straightforward
</p><pre>$ monitor simulation
</pre><p>monitor will write the CPU usage and memory consumption of simulation to standard error.  Values will be displayed every 5 seconds.  This is the rate at which monitor samples the program's metrics.
</p><h3>Log file</h3><p>Since monitor's output may interfere with that of the program to monitor, it is often convenient to use a log file.  The latter can be specified as follows:
</p><pre>$ monitor -l simulation.log simulation
</pre><p>For long running programs, it may be convenient to limit the output to, e.g., the last minute of the programs execution.  Since monitor provides metrics every 5 seconds, this implies we want to limit the output to the last 12 values to cover a minute:
</p><pre>$ monitor -l simulation.log -n 12 simulation
</pre><p>Note that this option is only available when monitor writes its metrics to a log file, not when standard error is used.
</p><h3>Modifying the sample resolution</h3><p>The interval at which monitor will show the metrics can be modified by specifying delta, the sample rate:
</p><pre>$ monitor -d 60 simulation
</pre><p>monitor will now print the program's metrics every 60 seconds.  Note that the minimum delta value is 1 second.
</p><h3>File sizes</h3><p>Some programs use temporary files, the size of which may also be a useful metric.  monitor provides an option to display the size of one or more files:
</p><pre>$ monitor -f tmp/simulation.tmp,cache simulation
</pre><p>Here, the size of the file simulation.tmp in directory tmp, as well as the size of the file cache will be monitored. Files can be specified by absolute as well as relative path, and multiple files are separated by ','.
</p><h3>Programs with command line options</h3><p>Many programs, e.g., matlab, take command line options.  To make sure these do not interfere with those of monitor and vice versa, the program can for instance be started in the following way:
</p><pre>$ monitor -delta 60 -- matlab -nojvm -nodisplay computation.m
</pre><p>The use of '--' will ensure that monitor does not get confused by matlab's '-nojvm' and '-nodisplay' options.
</p><h3>Subprocesses and multicore programs</h3><p>Some processes spawn one or more subprocesses.  In that case, the metrics shown by monitor are aggregated over the process and all of its subprocesses (recursively).  The reported CPU usage is the sum of all these processes, and can thus exceed 100 %.
</p><p>Some (well, since this is a HPC cluster, we hope most) programs use more than one core to perform their computations.  Hence, it should not come as a surprise that the CPU usage is reported as larger than 100 %.
</p><p>When programs of this type are running on a computer with <i>n</i> cores, the CPU usage can go up to <i>n</i> x 100 %.
</p><h3>Exit codes</h3><p>monitor will propagate the exit code of the program it is watching.  Suppose the latter ends normally, then monitor's exit code will be 0.  On the other hand, when the program terminates abnormally with a non-zero exit code, e.g., 3, then this will be monitor's exit code as well.
</p><p>When monitor has  to terminate in an abnormal state, for instance if it can't create the log file, its exit code will be 65.  If this interferes with an exit code of the program to be monitored, it can be modified by setting the environment variable MONITOR_EXIT_ERROR to a more suitable value.
</p><h2>Monitoring a running process</h2><p>It is also possible to \"attach\" monitor to a program or process that is already running.  One simply determines the relevant process ID using the ps command, e.g., 18749, and starts monitor:
</p><pre>$ monitor -p 18749
</pre><p>Note that this feature can be (ab)used to monitor specific subprocesses.
</p><h2>More information</h2><p>Help is available for monitor by issuing:
</p><pre>$ monitor -h
</pre>"
547,"Remark","<p>Logging in on the site does not yet function (expected around July 10), so you cannot yet see the overview of systems below.</p>"
549,"","<h2>Purpose</h2><p>Estimating the amount of memory an application will use during execution is often non trivial, especially when one uses third-party software. However, this information is valuable, since it helps to determine the characteristics of the compute nodes a job using this application should run on.
</p><p>Although the tool presented here can also be used to support the software development process, better tools are almost certainly available.
</p><p>Note that currently only single node jobs are supported, MPI support may be added in a future release.
</p><h2>Prerequisites</h2><p>The user should be familiar with the linux bash shell.
</p><h2>Monitoring a program</h2><p>To start using monitor, first load the appropriate module:
</p><pre>$ module load monitor
</pre><p>Starting a program, e.g., simulation, to monitor is very straightforward
</p><pre>$ monitor simulation
</pre><p>monitor will write the CPU usage and memory consumption of simulation to standard error.  Values will be displayed every 5 seconds.  This is the rate at which monitor samples the program's metrics.
</p><h3>Log file</h3><p>Since monitor's output may interfere with that of the program to monitor, it is often convenient to use a log file.  The latter can be specified as follows:
</p><pre>$ monitor -l simulation.log simulation
</pre><p>For long running programs, it may be convenient to limit the output to, e.g., the last minute of the programs execution.  Since monitor provides metrics every 5 seconds, this implies we want to limit the output to the last 12 values to cover a minute:
</p><pre>$ monitor -l simulation.log -n 12 simulation
</pre><p>Note that this option is only available when monitor writes its metrics to a log file, not when standard error is used.
</p><h3>Modifying the sample resolution</h3><p>The interval at which monitor will show the metrics can be modified by specifying delta, the sample rate:
</p><pre>$ monitor -d 60 simulation
</pre><p>monitor will now print the program's metrics every 60 seconds.  Note that the minimum delta value is 1 second.
</p><h3>File sizes</h3><p>Some programs use temporary files, the size of which may also be a useful metric.  monitor provides an option to display the size of one or more files:
</p><pre>$ monitor -f tmp/simulation.tmp,cache simulation
</pre><p>Here, the size of the file simulation.tmp in directory tmp, as well as the size of the file cache will be monitored. Files can be specified by absolute as well as relative path, and multiple files are separated by ','.
</p><h3>Programs with command line options</h3><p>Many programs, e.g., matlab, take command line options.  To make sure these do not interfere with those of monitor and vice versa, the program can for instance be started in the following way:
</p><pre>$ monitor -delta 60 -- matlab -nojvm -nodisplay computation.m
</pre><p>The use of '--' will ensure that monitor does not get confused by matlab's '-nojvm' and '-nodisplay' options.
</p><h3>Subprocesses and multicore programs</h3><p>Some processes spawn one or more subprocesses.  In that case, the metrics shown by monitor are aggregated over the process and all of its subprocesses (recursively).  The reported CPU usage is the sum of all these processes, and can thus exceed 100 %.
</p><p>Some (well, since this is a HPC cluster, we hope most) programs use more than one core to perform their computations.  Hence, it should not come as a surprise that the CPU usage is reported as larger than 100 %.
</p><p>When programs of this type are running on a computer with <i>n</i> cores, the CPU usage can go up to <i>n</i> x 100 %.
</p><h3>Exit codes</h3><p>monitor will propagate the exit code of the program it is watching.  Suppose the latter ends normally, then monitor's exit code will be 0.  On the other hand, when the program terminates abnormally with a non-zero exit code, e.g., 3, then this will be monitor's exit code as well.
</p><p>When monitor has  to terminate in an abnormal state, for instance if it can't create the log file, its exit code will be 65.  If this interferes with an exit code of the program to be monitored, it can be modified by setting the environment variable MONITOR_EXIT_ERROR to a more suitable value.
</p><h2>Monitoring a running process</h2><p>It is also possible to \"attach\" monitor to a program or process that is already running.  One simply determines the relevant process ID using the ps command, e.g., 18749, and starts monitor:
</p><pre>$ monitor -p 18749
</pre><p>Note that this feature can be (ab)used to monitor specific subprocesses.
</p><h2>More information</h2><p>Help is available for monitor by issuing:</p>"
551,"","<h2>What are toolchains?</h2>
<p>A toolchain is a collection of tools to build (HPC) software consistently. It consists of
</p>
<ul>
	<li>compilers for C/C++ and Fortran,</li>
	<li>a communications library (MPI), and</li>
	<li>mathematical libraries (linear algebra, FFT).</li>
</ul>
<p>Toolchains are versioned, and refreshed twice a year. All software available on the cluster is rebuild when a new version of a toolchain is defined to ensure consistency. Version numbers consist of the year of their definition, followed by either <code>a</code>
or <code>b</code>, e.g.,
<code>2014a</code>. 
Note that the software components are not necessarily the most recent releases, rather they are selected for stability and reliability.
</p>
<p>Two toolchain flavors are standard across the VSC on all machines that can support them: <code>intel</code> (based on Intel software components) and 
<code>foss</code> (based on free and open source software).
</p>
<p>It may be of interest to note that the Intel C/C++ compilers are more strict with respect to the standards than the GCC C/C++ compilers, while for Fortran, the GCC Fortran compiler tracks the standard more closely, while Intel's Fortran allows for many extensions added during Fortran's long history. When developing code, one should always build with both compiler suites, and eliminate all warnings.
</p>
<p>On average, the Intel compiler suite produces executables that are 5 to 10 % faster than those generated using the GCC compiler suite. However, for individual applications the differences may be more significant with sometimes significantly faster code produced by the Intel compilers while on other applications the GNU compiler may produce much faster code.
</p>
<p>Additional toolchains may be defined on specialised hardware to extract the maximum performance from that hardware.
</p>
<ul>
	<li>On Cerebro, the SGI UV shared memory system at the KU Leuven, you need to use the SGI MPI-library (called MPT for Message Passing Toolkit) to get the maximum performance from the interconnect (which offers hardware acceleration for some MPI functions). On that machine, two additional toolchains are defined, <code>intel-mpt</code> and 
	<code>foss-mpt</code>, equivalent to the standard 
	<code>intel</code> and 
	<code>foss</code> 
	toolchains respectively but with the MPI library replaced with MPT.</li>
</ul>
<h2><a name=\"intel-toolchain\">Intel toolchain</a></h2>
<p>The <code>intel</code> toolchain consists almost entirely of software components 
developed by Intel. When building third-party software, or developing your own, 
load the module for the toolchain:
</p>
<pre>$ module load intel/&lt;version&gt;
</pre>
<p>where <code>&lt;version&gt;</code> should be replaced by the one to be used, e.g., 
<code<2014a</code>. See the documentation on the software module system for more details.
</p>
<p>Starting with the <code>2014b</code> toolchain, the GNU compilers are also included in 
this toolchain as the Intel compilers use some of the libraries and as it is possible 
(though some care is needed) to link code generated with the Intel compilers with code 
compiled with the GNU compilers.
</p>
<h3><a name=\"intel-compilers\">Compilers: Intel and Gnu</a></h3>
<p>Three compilers are available:
</p>
<ul>
	<li>C: <code>icc</code></li>
	<li>C++: <code>icpc</code></li>
	<li>Fortran: <code>ifort</code></li>
</ul>
<p>Recent versions of
</p>
<p>For example, to compile/link a Fortran program <code>fluid.f90</code> to an executable 
<code>fluid</code> with architecture specific optimization, use:
</p>
<pre>$ ifort  -O2  -xhost  -o fluid  fluid.f90
</pre>
<p>Documentation on Intel compiler flags and options is 
<a href=\"https://software.intel.com/sites/default/files/Compiler_QRG_2013.pdf\">provided 
by Intel</a>. Do not forget to <em>load the toolchain module</em> first!
</p>
<h4><a name=\"intel-openmp\"></a>Intel OpenMP</h4>
<p>The compiler switch to use to compile/link OpenMP C/C++ or Fortran code is 
<code>-openmp</code>.  For example, to compile/link a OpenMP C program 
<code>scatter.c</code> to an executable 
<code>scatter</code> with architecture specific 
optimization, use:
</p>
<pre>$ icc  -openmp  -O2  -xhost  -o scatter  scatter.c
</pre>
<p>Remember to specify as many processes per node as the number of threads the executable 
is supposed to run. This can be done using the <code>ppn</code> resource, e.g., 
<code>-l nodes=1:ppn=10</code> for an executable that should be run with 10 OpenMP 
threads. The number of threads should not exceed the number of cores on a compute node.
</p>
<h3><a name=\"intel-mpi\">Communication library: Intel MPI</a></h3>
<p>For the intel toolchain, <code>impi</code>, i.e., Intel MPI is used as the 
communications library. To compile/link MPI programs, wrappers are supplied, so that 
the correct headers and libraries are used automatically. These wrappers are:
</p>
<ul>
	<li>C: <code>mpiicc</code></li>
	<li>C++: <code>mpiicpc</code></li>
	<li>Fortran: <code>mpiifort</code></li>
</ul>
<p>Note that the <em>names differ</em> from those of other MPI implementations. 
The compiler wrappers take the same options as the corresponding compilers.
</p>
<h4>Using the Intel MPI compilers</h4>
<p>For example, to compile/link a C program <code>thermo.c</code> to an executable 
<code>thermodynamics</code> with architecture specific optimization, use:
</p>
<pre>$ mpiicc -O2  -xhost  -o thermodynamics  thermo.c
</pre>
<p>Extensive documentation is 
<a href=\"https://software.intel.com/en-us/articles/intel-mpi-library-documentation\">provided 
by Intel</a>. Do not forget to <em>load the toolchain module</em> first.
</p>
<h4>Running an MPI program with Intel MPI</h4>
<p>Note that an MPI program must be run with the exact same version of the toolchain as 
it was originally build with. The listing below shows a PBS job script 
<code>thermodynamics.pbs</code> that runs the 
<code>thermodynamics</code> executable.
</p>
<pre>#!/bin/bash -l
module load intel/&lt;version&gt;
cd $PBS_O_WORKDIR n_proc=$( cat $PBS_NODEFILE  |  wc  -l )
mpirun  -np $n_proc  ./thermodynamics
</pre>
<p>The number of processes is computed from the length of the node list in the 
<code>$PBS_NODEFILE</code> file, which in turn is specified as a resource specification 
when submitting the job to the queue system.
</p>
<h3><a name=\"intel-mkl\">Intel mathematical libraries</a></h3>
<p>The Intel Math Kernel Library (MKL) is a comprehensive collection of highly optimized 
libraries that form the core of many scientific HPC codes. Among other functionality, 
it offers:
</p>
<ul>
	<li>BLAS (Basic Linear Algebra Subsystem), and extensions to sparse matrices</li>
	<li>Lapack (Linear algebra package) and ScaLAPACK (the distributed memory version)</li>
	<li>FFT-routines including routines compatible with the FFTW2 and FFTW3 libraries 
	(Fastest Fourier Transform in the West)</li>
	<li>Various vector functions and statistical functions that are optimised for the 
	vector instruction sets of all recent Intel processor families</li>
</ul>
<p>Intel offers 
<a href=\"https://software.intel.com/en-us/articles/intel-math-kernel-library-documentation\">extensive 
documentation</a> on this library and how to use it.
</p>
<p>There are two ways to link the MKL library:
</p>
<ul>
	<li>If you use icc, icpc or ifort to link your code, you can use the -mkl compiler 
	option:
	<ul>
		<li>-mkl=parallel or -mkl: Link the multi-threaded version of the library.</li>
		<li>-mkl=sequential: Link the single-threaded version of the library</li>
		<li>-mkl=cluster: Link the cluster-specific and sequential library, i.e., 
		ScaLAPACK will be included, but assumes one process per core (so no hybrid MPI/multi-threaded approach)</li>
	</ul>
	The Fortran95 interface library for lapack is not automatically included though. 
	You'll have to specify that library seperately. You can get the value from the 
	<a href=\"https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor\">MKL 
	Link Line Advisor</a>, see also the next item.</li>
	<li>Or you can specify all libraries explictly. To do this, it is strongly recommended 
	to use Intel's 
	<a href=\"https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor/\">MKL 
	Link Line Advisor</a>, and will also tell you how to link the MKL library with 
	code generated with the GNU and PGI compilers.<br>
	<strong>Note:</strong> On most VSC systems, the variable MKLROOT has a different 
	value from the one assumed in the Intel documentation. Wherever you see 
	<code>$(MKLROOT)</code> you may have to replace it with 
	<code>$(MKLROOT)/mkl</code>.</li>
</ul>
<p>MKL also offers a very fast streaming pseudorandom number generator, see the 
documentation for details.
</p>
<h3><a name=\"intel-versions\"></a>Intel toolchain version numbers</h3>
<table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width: 100%;\">
<tbody>
<tr>
	<th>
	</th>
	<th>2014a
	</th>
	<th>2014b
	</th>
	<th>2015a
	</th>
</tr>
<tr>
	<td>icc
	</td>
	<td>13.1.3 20130607
	</td>
	<td>13.1.3 20130607
	</td>
	<td>15.0.1 20141023
	</td>
</tr>
<tr>
	<td>icpc
	</td>
	<td>13.1.3 20130607
	</td>
	<td>13.1.3 20130607
	</td>
	<td>15.0.1 20141023
	</td>
</tr>
<tr>
	<td>ifort
	</td>
	<td>13.1.3 20130607
	</td>
	<td>13.1.3 20130607
	</td>
	<td>15.0.1 20141023
	</td>
</tr>
<tr>
	<td>Intel MPI
	</td>
	<td>4.1.3.045
	</td>
	<td>4.1.3.049
	</td>
	<td>5.0.2.044
	</td>
</tr>
<tr>
	<td>Intel MKL
	</td>
	<td>11.1.1.106
	</td>
	<td>11.1.2.144
	</td>
	<td>11.2.1.133
	</td>
</tr>
<tr>
	<td>GCC
	</td>
	<td>/
	</td>
	<td>4.8.3
	</td>
	<td>4.9.2
	</td>
</tr>
</tbody>
</table>
<h3><a name=\"intelInfo\"></a>Further information on Intel tools</h3>
<ul>
	<li>All Intel documentation of recent software versions is available in the 
	<a href=\"https://software.intel.com/en-us/intel-software-technical-documentation\">Intel 
	Software Documentation Library</a>. The documentation is typically available for the most recent version and sometimes one older version of te compiler and libraries.</li>
	<li>MKL
	<ul>
		<li><a href=\"https://software.intel.com/en-us/articles/intel-math-kernel-library-documentation/\">Link 
		page to the documentation of the most recent version on the Intel web site</a></li>
		<li><a href=\"https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor\">MKL 
		Link Line Advisor</a></li>
		<li>Older versions:
		<ul>
			<li>intel/2014a and intel/2014b toolchain, MKL 11.1: 
			<a href=\"https://software.intel.com/en-us/mkl_11.1_ug_lin\">User's Guide</a> 
			and <a href=\"https://software.intel.com/en-us/mkl_11.1_ref\">Reference Guide</a></li>
		</ul>
		</li>
	</ul>
	</li>
	<li><a href=\"/cluster-doc/development/blas-lapack#Links\">Generic BLAS/LAPACK/ScaLAPACK 
	documentation</a></li>
</ul>
<h2><a name=\"foss-toolchain\">FOSS toolchain</a></h2>
<p>The <code>foss</code> toolchain consists entirely of free and open source software 
components. When building third-party software, or developing your own, 
load the module for the toolchain:
</p>
<pre>$ module load foss/&lt;version&gt;
</pre>
<p>where <code>&lt;version&gt;</code> should be replaced by the one to be used, e.g., 
<code>2014a</code>. See the documentation on the software module system for more details.
</p>
<h3><a name=\"gcc-compilers\">Compilers: GNU</a></h3>
<p>Three GCC compilers are available:
</p>
<ul>
	<li>C: <code>gcc</code></li>
	<li>C++: <code>g++</code></li>
	<li>Fortran: <code>gfortran</code></li>
</ul>
<p>For example, to compile/link a Fortran program <code>fluid.f90</code> to an executable 
<code>fluid</code> with architecture specific optimization for processors that support AVX instructions, use:
</p>
<pre>$ gfortran -O2 -march=corei7-avx -o fluid fluid.f90
</pre>
<p>Documentation on GCC compiler flags and options is available on the 
<a href=\"http://gcc.gnu.org/onlinedocs/\">project's website</a>. Do not forget to load the 
toolchain module first!
</p>
<h4><a name=\"foss-openmp\"></a>GCC OpenMP</h4>
<p>The compiler switch to use to compile/link OpenMP C/C++ or Fortran code is 
<code>-fopenmp</code>. For example, to compile/link a OpenMP C program 
<code>scattter.c</code> to an executable 
<code>scatter</code> with optimization for processors that support the AVX instruction 
set, use:
</p>
<pre>$ gcc -fopenmp -O2 -march=corei7-avx -o scatter scatter.c
</pre>
<p>Remember to specify as many processes per node as the number of threads the 
executable is supposed to run. This can be done using the <code>ppn</code> resource, e.g., 
<code>-l nodes=1:ppn=10</code> for an executable that should be run with 10 OpenMP threads. 
The number of threads should not exceed the number of cores on a compute node.
</p>
<p>Note that the OpenMP runtime library used by GCC is of inferior quality when compared 
to Intel's, so developers are strongly encouraged to use the 
<a href=\"#intel-toolchain\"><code>intel</code> toolchain</a> when developing/building OpenMP software.
</p>
<h3><a name=\"openmpi\">Communication library: OpenMPI</a></h3>
<p>For the <code>foss</code> toolchain, OpenMPI is used as the communications library. 
To compile/link MPI programs, wrappers are supplied, so that the correct headers and 
libraries are used automatically. These wrappers are:
</p>
<ul>
	<li>C: <code>mpicc</code></li>
	<li>C++: <code>mpic++</code></li>
	<li>Fortran: <code>mpif77</code>, 
	<code>mpif90</code></li>
</ul>
<p>The compiler wrappers take the same options as the corresponding compilers.
</p>
<h4>Using the MPI compilers from OpenMPI</h4>
<p>For example, to compile/link a C program <code>thermo.c</code> to an executable 
<code>thermodynamics</code> with architecture specific optimization for the AVX 
instruction set, use:
</p>
<pre>$ mpicc -O2 -march=corei7-avx -o thermodynamics thermo.c
</pre>
<p>Extensive documentation is <a href=\"http://www.open-mpi.org/doc/\">provided on the 
project's website</a>. Do not forget to load the toolchain module first.
</p>
<h4>Running an OpenMPI program</h4>
<p>Note that an MPI program must be run with the exact same version of the toolchain as 
it was originally build with. The listing below shows a PBS job script 
<code>thermodynamics.pbs</code> that runs the 
<code>thermodynamics</code> executable.
</p>
<pre>#!/bin/bash -l 
module load intel/&lt;version&gt; 
cd $PBS_O_WORKDIR 
mpirun ./thermodynamics
</pre>
<p>The hosts and number of processes is retrieved from the queue system, that gets this 
information from the resource specification for that job.
</p>
<h3><a name=\"foss-mathlibs\">FOSS mathematical libraries</a></h3>
<p>The foss toolchain contains the basic HPC mathematical libraries, it offers:
</p>
<ul>
	<li><a href=\"http://www.openblas.net/\">OpenBLAS</a> (Basic Linear Algebra Subsystem)</li>
	<li><a href=\"http://www.netlib.org/lapack/\">Lapack </a>(Linear Algebra PACKage)</li>
	<li>ScaLAPACK (Scalable Linear Algebra PACKage)</li>
	<li><a href=\"http://www.fftw.org/\">FFTW</a> (Fastest Fourier Transform in the West)</li>
</ul>
<h3><a name=\"foss-versions\"></a>Version numbers FOSS toolchain</h3>
<table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width: 100%;\">
<tbody>
<tr>
	<th>
	</th>
	<th>2014a
	</th>
	<th>2014b
	</th>
	<th>2015a
	</th>
</tr>
<tr>
	<td>GCC
	</td>
	<td>4.8.2
	</td>
	<td>4.8.3
	</td>
	<td>4.9.2
	</td>
</tr>
<tr>
	<td>OpenMPI
	</td>
	<td>1.6.5
	</td>
	<td>1.8.1
	</td>
	<td>1.8.3
	</td>
</tr>
<tr>
	<td>OpenBLAS
	</td>
	<td>0.2.8
	</td>
	<td>0.2.9
	</td>
	<td>0.2.13
	</td>
</tr>
<tr>
	<td>LAPACK
	</td>
	<td>3.5.0
	</td>
	<td>3.5.0
	</td>
	<td>3.5.0
	</td>
</tr>
<tr>
	<td>ScaLAPACK
	</td>
	<td>2.0.2
	</td>
	<td>2.0.2
	</td>
	<td>2.0.2
	</td>
</tr>
<tr>
	<td>FFTW
	</td>
	<td>3.3.3
	</td>
	<td>3.3.4
	</td>
	<td>3.3.4
	</td>
</tr>
</tbody>
</table>
<h3><a name=\"fossInfo\"></a>Further information on FOSS components</h3>
<ul>
	<li><a href=\"https://gcc.gnu.org/onlinedocs/\">Overview of GCC manuals 
	(all versions)</a></li>
	<li>OpenMPI documentation
	<ul>
		<li><a href=\"http://www.open-mpi.org/doc/v1.8/\">1.8.x (foss/2014b and foss/2015a)</a></li>
		<li><a href=\"http://www.open-mpi.org/doc/v1.6/\">1.6.x (foss/2014a)</a></li>
	</ul>
	</li>
	<li>The <a href=\"http://www.openblas.net/\">OpenBLAS project page</a> and 
	<a href=\"https://github.com/xianyi/OpenBLAS/wiki\">documentation Wiki</a></li>
	<li><a href=\"/cluster-doc/development/blas-lapack#Links\">Generic BLAS/LAPACK/ScaLAPACK 
	documentation</a></li>
</ul>"
555,"","<p>The documentation page you visited applies to the KU Leuven Tier-2 setup (THinking and Cerebro). For more information about these systems, visit <a href=\"/infrastructure/hardware/hardware-kul\">the hardware description page</a>.</p>"
557,"","<p>The documentation page you visited applies to the UGent Tier-2 setup Stevin. For more information about the setup, visit <a href=\"/infrastructure/hardware/hardware-ugent\">the UGent hardware page</a>.</p>"
559,"","<p>The documentation page you visited applies to the UAntwerp Hopper cluster. Some or all of it may also apply to the older Turing cluster, but that system does not fully implement the VSC environment module structure. For more details about the specifics of those systems, visit <a href=\"/infrastructure/hardware/hardware-ua\">the UAntwerp hardware page</a>.</p>"
561,"","<p>The documentation page you visited applies to the VUB Hydra cluster. For more specifics about the Hydra cluster, check <a href=\"/infrastructure/hardware/hardware-vub\">the VUB hardware page</a>.</p>"
563,"","<p>The documentation page you visited applies to the Tier-1 cluster Muk installed at UGent. Check <a href=\"/infrastructure/hardware/hardware-tier1-muk\">the Muk hardware description</a> for more specifics about this system.</p>"
565,"","<p>The documentation page you visited applies to client systems running a recent version of Microsoft Windows (though you may need to install some additional software as specified on the page).</p>"
567,"","<p>The documentation page you visited applies to client systems with a recent version of Microsoft Windows and a UNIX-compatibility layer. We tested using the <a href=\"https://www.cygwin.com/\">freely available Cygwin system</a> maintained by Red Hat.<br></p>"
569,"","<p>The documentation page you visited applies to Apple Mac client systems with a recent version of OS X installed, though you may need some additional software as specified on the page.</p>"
571,"","<p>The documentation page you visited applies to client systems running a popular Linux distribution (though some of the packages you need may not be installed by default).</p>"
577,"","<p>Eerste aanpak</p><ul><li>Titel Systems</li><li>Call-to-Action Label system name, Node Docu Target, Type [label.cat.link]</li><li>Style-&gt;Container: block--related</li></ul>"
579,"","<p>Tweede aanpak</p><ul><li>Text widget met enkel de titel Systems</li><li>Asset widget, selecteer uit System Icons/Regular<span></span></li><li>Maar eigenlijk zou het mooier zijn als dit allemaal in één widget zou zitten, de icoontjes tegen elkaar zouden staan of in ieder geval dichter, en misschien in een grijs blok of zo?</li></ul>"
585,"","<p>The page you're trying to visit, does not exist or has been moved to a different URL.</p><p>Some common causes of this problem are:</p><ol><li>Maybe you arrived at the page through a search engine. Search engines - including the one implemented on our own pages, which uses the Google index - don't immediately know that a page has been moved or does not exist anymore and continue to show old pages in the search results.</li><li>Maybe you followed a link on another site. The site owner may not yet have noticed that our web site has changed.</li><li>Or maybe you followed a link in a somewhat older e-mail or document. It is entirely normal that links age and don't work anymore after some time.</li><li>Or maybe you found a bug on our web site? Even though we check regularly for dead links, errors can occur. You can contact us at <a href=\"mailto:Kurt.Lust@uantwerpen.be\">Kurt.Lust@uantwerpen.be</a>.</li></ol>"
605,"","<p>You're looking for:
</p><ul>
	<li>Contact information or other information about our organisation? Go to the \"<a href=\"/en/about-vsc\">About the VSC</a>\" section</li>
	<li>A high-level overview of our services and infrastructure? Go to the \"<a href=\"/en/access-and-infrastructure\">Access and Infrastructure</a>\" section</li>
	<li>Information on our training programs? Go to the \"<a href=\"/en/education-and-trainings\">Education and Training</a>\" section.</li>
	<li>Examples of concrete projects on our largest cluster? We've got a <a href=\"/en/projects\">list of projects</a> available.</li>
	<li>Some examples of HPC being used in actual applications? We've got some use cases in <a href=\"https://www.vscentrum.be/en/academics-use-cases\">academics</a>, <a href=\"/en/industry-use-cases\">industry</a> and <a href=\"/en/use-cases-for-a-broad-audience\">some texts targeted to a broader, less technical audience</a> on our web site (some cases only in Dutch).</li>
</ul>"
611,"","<h2>Inline code with &lt;code&gt;...&lt;/code&gt;</h2><p>We used inline code on the old vscentrum.be to clearly mark system commands etc. in text.
</p><ul>
	<li>For this we used the &lt;code&gt; tag.</li>
	<li>There was support in the editor to set this tag</li>
	<li>It doesn't seem to work properly in the current editor. If the fragment of code contains a slash (/), the closing tag gets omitted.</li>
</ul><p>Example: At UAntwerpen you'll have to use <code>module avail MATLAB</code> and 
	<code>module load MATLAB/2014a</code> respectively.
</p><p>However, If you enter both &lt;code&gt;-blocks on the same line in a HTML file, the editor doesn't process them well: <code>module avail MATLAB</code> and &lt;code&gt;module load MATLAB.
</p><p>Test: <code>test 1</code> en <code>test 2</code>.</p><h2>Code in &lt;pre&gt;...&lt;/pre&gt;</h2><p>This was used a lot on the old vscentrum.be site to display fragments of code or display output in a console windows.
</p><ul>
	<li>Readability of fragments is definitely better if a fixed width font is used as this is necessary to get a correct alignment.</li>
	<li>Formatting is important: Line breaks should be respected. The problem with the CMS seems to be that the editor respects the line breaks, the database also stores them as I can edit the code again, but the CMS removes them when generating the final HTML-page as I don't see the line breaks again in the resulting HTML-code that is loaded into the browser.</li>
</ul><pre>#!/bin/bash -l
#PBS -l nodes=1:nehalem
#PBS -l mem=4gb
module load matlab
cd $PBS_O_WORKDIR
...
</pre><p>And this is a test with a very long block:
</p><pre>ln03-1003: monitor -h
### usage: monitor [-d &lt;delta&gt;] [-l &lt;logfile&gt;] [-f &lt;files&gt;]
# [-h] [-v] &lt;cmd&gt; | -p &lt;pid&gt;
# Monitor can be used to sample resource utilization of a process
# over time. Monitor can sample a running process if the latter's PID
# is specified using the -p option, or it can start a command with
# parameters passed as arguments. When one has to specify flags for
# the command to run, '--' can be used to delimit monitor's options, e.g.,
# monitor -delta 5 -- matlab -nojvm -nodisplay calc.m
# Resources that can be monitored are memory and CPU utilization, as
# well as file sizes.
# The sampling resolution is determined by delta, i.e., monitor samples
# every &lt;delta&gt; seconds.
# -d &lt;delta&gt; : sampling interval, specified in
# seconds, or as [[dd:]hh:]mm:ss
# -l &lt;logfile&gt; : file to store sampling information; if omitted,
# monitor information is printed on stderr
# -n &lt;lines&gt; : retain only the last &lt;lines&gt; lines in the log file,
# note that this option only makes sense when combined
# with -l, and that the log file lines will not be sorted
# according to time
# -f &lt;files&gt; : comma-separated list of file names that are monitored
# for size; if a file doesn't exist at a given time, the
# entry will be 'N/A'
# -v : give verbose feedback
# -h : print this help message and exit
# &lt;cmd&gt; : actual command to run, followed by whatever
# parameters needed
# -p &lt;pid&gt; : process ID to monitor
#
# Exit status: * 65 for any montor related error
# * exit status of &lt;cmd&gt; otherwise
# Note: if the exit code 65 conflicts with those of the
# command to run, it can be customized by setting the
# environment variables 'MONITOR_EXIT_ERROR' to any value
# between 1 and 255 (0 is not prohibited, but this is probably.
# not what you want).
</pre><h2>The &lt;code&gt; style in the editor</h2><p>In fact, the Code style of the editor works on a paragraph basis and all it does is put the paragraph between &lt;pre&gt; and &lt;/pre&gt;-tags, so the problem mentioned above remains. The next text was edited in WYSIWIG mode:
</p><pre>#!/bin/bash -l
#PBS -l nodes=4:ivybridge
...
</pre><p>Another editor bug is that it isn't possible to switch back to regular text mode at the end of a code fragment if that is at the end of the text widget: The whole block is converted back to regular text instead and the formatting is no longer shown.
</p>"
613,"","<p>After the successful first VSC users day in January 2014, the second users day took place at the University of Antwerp on Monday November 30 2015. The users committee organized the day. The plenary sessions were given by an external and an internal speaker. Moreover, 4 workshops were organized:
</p><ul>
	<li>VSC for starters (UAntwerp)<br> <i>Upscaling to HPC. We will present you some best practices, give advice when using HPC clusters and show some pros and cons when moving from desktop to HPC. Even more experienced researchers may be interested.<br></i></li>
	<li>Specialized Tier-2 infrastructure: shared memory (KU Leuven)<br> <i>Shared memory: when distributing data is not/no longer an option. We will introduce you to the available shared memory infrastructure by means of some use cases.</i></li>
	<li>Big data (UGent)<br> <i>We present Hanythingondemand (hod), a solution for running Hadoop, Spark and other services on HPC clusters.</i></li>
	<li>Cloud and grid access (VUB)<br> <i>The availability of grid and cloud resources is not so well known in VSC. We will introduce you to the cloud environment, explain how it can be useful to you and show how you can gain access. </i></li>
</ul><h2>Some impressions...</h2><p>
	<a href=\"https://beeldbank.uantwerpen.be/index.php/collection/zoom/755b4dad5ae6430b802a0716fffc2a76453de3161bae495ca95ce3225ca091feece20a1ef2154b52b4ff0986228f87e6/1#1\"><img src=\"https://www.vscentrum.be/assets/1025\" alt=\"More pictures\" title=\"VSC User Day 30 11 2015\"></a>
</p><p>More pictures can be found in the <a href=\"https://beeldbank.uantwerpen.be/index.php/collection/zoom/755b4dad5ae6430b802a0716fffc2a76453de3161bae495ca95ce3225ca091feece20a1ef2154b52b4ff0986228f87e6/1#1\">image bank</a>.
</p><h2>Program</h2><table>
<tbody>
<tr>
	<td>09:50
	</td>
	<td>Welcome – Bart De Moor (chair Hercules Foundation)
	</td>
</tr>
<tr>
	<td>10:00
	</td>
	<td>Invited lecture: <a href=\"/events/userday-2015/lectures#DerekGroen\">High performance and multiscale computing: blood, clay, stars and humans</a> – Derek Groen (Centre for Computational Science, University College London) [<a href=\"/assets/1057\">slides - PDF 8.3MB</a>]</td>
</tr>
<tr>
	<td>11:00
	</td>
	<td>Coffee
	</td>
</tr>
<tr>
	<td>11:30
	</td>
	<td>Workshops / hands-on sessions (parallel sessions)
	</td>
</tr>
<tr>
	<td>12:45
	</td>
	<td>Lunch
	</td>
</tr>
<tr>
	<td>14:00
	</td>
	<td>Lecture internal speaker: <a href=\"/events/userday-2015/lectures#JohanMeyers\">High-performance computing of wind farms in the atmospheric boundary layer</a> – Johan Meyers (Department of Mechanical Engineering, KU Leuven) [<a href=\"/assets/1055\">slides - PDF 9.9MB</a>]</td>
</tr>
<tr>
	<td>14:30
	</td>
	<td>‘1 minute’ poster presentations
	</td>
</tr>
<tr>
	<td>14:45
	</td>
	<td>Workshops / hands-on sessions (parallel sessions)
	</td>
</tr>
<tr>
	<td>16:15
	</td>
	<td>Coffee & <a href=\"/events/userday-2015/posters\">Poster session</a>
	</td>
</tr>
<tr>
	<td>17:00
	</td>
	<td>Closing – Dirk Roose (representative of users committee)
	</td>
</tr>
<tr>
	<td>17:10
	</td>
	<td>Drink
	</td>
</tr>
</tbody>
</table><h2>Titles and abstracts<br></h2><p>An overview of the posters that will be presented during the poster session is <a href=\"/events/userday-2015/posters\">available here</a>.
</p>"
619,"","<p>TurboVNC is a good
way
to provide access to remote visualization applications that works together with VirtualGL  - a popular package for remote visualization.
</p><h2></h2><h2>Installing TurboVNC client (viewer)</h2><ul>
	<li>
	Download the most recent version of the client from the <a href=\"https://sourceforge.net/projects/turbovnc/files/\" target=\"_blank\">TurboVNC download page on SourceForge</a>.</li>
	<li>
	Continue with configuration of your client.
	</li>
</ul><h2>TurboVNC client Configuration & Start Guide</h2><p><em>Note: These instructions are for the KU Leuven visualization nodes only. The UAntwerp visualization node also uses TurboVNC, but the setup is different as the visualization node is currently not in the job queueing system and as TurboVNC is also supported on the regular login nodes (but without OpenGL support). Specific instructions for the use of TurboVNC on the UAntwerp clusters can be found on the page \"<a href=\"/infrastructure/hardware/hardware-ua/visualization\">Remote visualization @ UAntwerp</a></em>\".
</p><ol class=\"list--ordered\">
	<li>Request an interactive job on visualization partition:
	<pre>$ qsub -I -X -l partition=visualization	-l pmem=6gb -l nodes=1:ppn=20
	</pre></li>
	<li>Once you are on one of visualization nodes (r10n3 or r10n4) load the TurboVNC module:
	<pre>$ module load TurboVNC/1.2.3-foss-2014a
	</pre></li>
	<li>Create password to authenticate your session:
	<pre>$ vncpasswd
	</pre>
	In case of problems with saving your password please create the appropriate path first:
	<pre>$ mkdir .vnc; touch .vnc/passwd; vncpasswd
	</pre>
	</li>
	<li>Start VNC server on the visualization node (optionally with geometry settings):
	<pre>$ vncserver (-depth 24 -geometry 1600x1000)
	</pre>
	As a result you will get the information about the display &lt;d&gt; <d> that you are using (r10n3:<d>), e.g.for &lt;d&gt;=1
	<pre>Desktop 'TurboVNC: r10n3:1 (vsc30000)' started on display r10n3:1
	</pre>
	</d></d></li>
	<li>
	Establish the ssh tunnel connection: <br><br>
	In Linux/ Mac OS:
	<pre>     $ ssh -L 590&lt;d&gt;<d>:host:590&lt;d&gt;<d> -N vsc30000@login.hpc.kuleuven.be
e.g. $ ssh -L 5901:r10n3:5901 -N vsc30000@login.hpc.kuleuven.be</d></d>
	</pre>
	<br>
	In Windows:
	<br>
	In putty go to Connection-SSH-Tunnels tab and add the source port 590&lt;d&gt; (e.g. 5901) and destination host:590&lt;d&gt; (e.g. r10n3:5901).
	<br><img src=\"/assets/1007\" alt=\"TVNC set tunnel\" width=\"394\" height=\"378\" style=\"width: 394px; height: 378px;\">
	<br>Once the tunnel is added it will appear in the list of forwarded ports:
	<br><img src=\"/assets/1009\" alt=\"TVNC tunnel\" width=\"397\" height=\"383\" style=\"width: 397px; height: 383px;\">
	<br>With that settings continue <a href=\"/client/windows/console-putty\">login to the cluster</a>.
	</li>
	<li>Start VNC viewer connection <br>
	Start the client: VSC server as localhost:&lt;d&gt;<d> (where &lt;d&gt;<d> is display number), e.g. localhost:1 
	<br>
	<img src=\"/assets/1011\" alt=\"TVNC connect\" width=\"287\" height=\"96\" style=\"width: 287px; height: 96px;\">
	</d></d>
	<br>Authenticate with your password
	<br><img src=\"/assets/1013\" alt=\"TVNC authenticate\" width=\"193\" height=\"131\" style=\"width: 193px; height: 131px;\">
	</li>
	<li> After your work is done do not forget to close your connection:
	<pre>     $ vncserver -kill :&lt;d&gt;<d>; exit
e.g. $ vncserver -kill :1; exit</d>
	</pre>
	</li>
</ol><h2>How to start using visualization node?</h2><ol class=\"list--ordered\">
	<li>TurboVNC works with the tab Window Manager twm (more info on how to use it can be found on the <a href=\"https://en.wikipedia.org/wiki/Twm\" target=\"_blank\">Wikipedia twm page</a> or on the <a href=\"https://linux.die.net/man/1/twm\" target=\"_blank\">twm man page</a>). <br><img src=\"/assets/1015\" alt=\"twm\" width=\"487\" height=\"450\" style=\"width: 487px; height: 450px;\">
	</li>
	<li> To start a new terminal use left click of the mouse and choose xterm
	<br><img src=\"/assets/1017\" alt=\"twm\" width=\"100\" height=\"447\" style=\"width: 100px; height: 447px;\">
	</li>
	<li>Load the appropriate visualization module (Paraview, VisIt, VMD, Avizo, e.g.
	<pre>$ module load Paraview
	</pre></li>
	<li>Start the application. In general the application has to be started using VirtualGL package, e.g.
	<pre>$ vglrun –d :0 paraview
	</pre>
	but to make it easier we created scripts (starting with capital letters: Paraview, Visit, VMD) that can execute the necessary commands and start the application, e.g.
	<pre>$ Paraview
	</pre>
	</li>
	<li>
	For checking how much GPUs are involved in your visalization you may execute gpuwatch in the new terminal:
	<pre>$ gpuwatch
	</pre></li>
</ol><h2>Attached documents</h2><ul class=\"list--unordered\">
</ul><p><a href=\"https://www.vscentrum.be/assets/1005\">Slides from the lunchbox session</a>
</p><ul class=\"list--unordered\">
</ul>"
621,"","<p>The <code>intel</code> toolchain consists almost entirely of software components 
developed by Intel. When building third-party software, or developing your own, 
load the module for the toolchain:
</p><pre>$ module load intel/&lt;version&gt;
</pre><p>where <code>&lt;version&gt;</code> should be replaced by the one to be used, e.g., <code>2016b</code><code<2014a< code=\"\">. See the documentation on the software module system for more details.
	</code<2014a<>
</p><p>Starting with the <code>2014b</code> toolchain, the GNU compilers are also included in 
this toolchain as the Intel compilers use some of the libraries and as it is possible 
(though some care is needed) to link code generated with the Intel compilers with code 
compiled with the GNU compilers.
</p><h2><a name=\"intel-compilers\"></a>Compilers: Intel and Gnu</h2><p>Three compilers are available:
</p><ul>
	<li>C: <code>icc</code></li>
	<li>C++: <code>icpc</code></li>
	<li>Fortran: <code>ifort</code></li>
</ul><p>Compatible versions of the GNU C (<code>gcc</code>), C++ (<code>g++</code>) and Fortran (<code>gfortran</code>) compilers are also provided.
</p><p>For example, to compile/link a Fortran program <code>fluid.f90</code> to an executable 
	<code>fluid</code> with architecture specific optimization, use:
</p><pre>$ ifort -O2 -xhost -o fluid fluid.f90
</pre><p>For documentation on available compiler options, we refer to the <a href=\"#FurtherInfo\">links to the Intel documentation at the bottom of this page</a>. Do not forget to <em>load the toolchain module</em> first!
</p><h3><a name=\"intel-openmp\"></a>Intel OpenMP</h3><p>The compiler switch to use to compile/link OpenMP C/C++ or Fortran code is <code>-qopenmp</code> in recent versions of the compiler (toolchain intel/2015a and later) or  
	<code>-openmp</code> in older versions.  For example, to compile/link a OpenMP C program 
	<code>scatter.c</code> to an executable 
	<code>scatter</code> with architecture specific 
optimization, use:
</p><pre>$ icc -qopenmp -O2 -xhost -o scatter scatter.c
</pre><p>Remember to specify as many processes per node as the number of threads the executable 
is supposed to run. This can be done using the 
	<code>ppn</code> resource, e.g., 
	<code>-l nodes=1:ppn=10</code> for an executable that should be run with 10 OpenMP 
threads. The number of threads should not exceed the number of cores on a compute node.
</p><h2><a name=\"intel-mpi\"></a>Communication library: Intel MPI</h2><p>For the intel toolchain, <code>impi</code>, i.e., Intel MPI is used as the 
communications library. To compile/link MPI programs, wrappers are supplied, so that 
the correct headers and libraries are used automatically. These wrappers are:
</p><ul>
	<li>C: <code>mpiicc</code></li>
	<li>C++: <code>mpiicpc</code></li>
	<li>Fortran: <code>mpiifort</code></li>
</ul><p>Note that the <em>names differ</em> from those of other MPI implementations. 
The compiler wrappers take the same options as the corresponding compilers.
</p><h3>Using the Intel MPI compilers</h3><p>For example, to compile/link a C program <code>thermo.c</code> to an executable 
	<code>thermodynamics</code> with architecture specific optimization, use:
</p><pre>$ mpiicc -O2 -xhost -o thermodynamics thermo.c
</pre><p>For further documentation, we refer to <a href=\"#FurtherInfo\">the links to the Intel documentation at the bottom of this page</a>. Do not forget to <em>load the toolchain module</em> first.
</p><h3>Running an MPI program with Intel MPI</h3><p>Note that an MPI program must be run with the exact same version of the toolchain as 
it was originally build with. The listing below shows a PBS job script 
	<code>thermodynamics.pbs</code> that runs the 
	<code>thermodynamics</code> executable.
</p><pre>#!/bin/bash -l
module load intel/&lt;version&gt;
cd $PBS_O_WORKDIR
mpirun -np $PBS_NP ./thermodynamics
</pre><p>The resource manager passes the number of processes to the job script through the environment variable <code>$PBS_NP</code>, but if you use a recent implementation of Intel MPI, you can even omit <code>-np $PBS_NP</code> as Intel MPI recognizes the Torque resource manager and requests the number of cores itself from the resource manager if the number is not specified.
</p><h2><a name=\"intel-mkl\"></a>Intel mathematical libraries</h2><p>The Intel Math Kernel Library (MKL) is a comprehensive collection of highly optimized 
libraries that form the core of many scientific HPC codes. Among other functionality, 
it offers:
</p><ul>
	<li>BLAS (Basic Linear Algebra Subsystem), and extensions to sparse matrices</li>
	<li>Lapack (Linear algebra package) and ScaLAPACK (the distributed memory version)</li>
	<li>FFT-routines including routines compatible with the FFTW2 and FFTW3 libraries 
	(Fastest Fourier Transform in the West)
	</li>
	<li>Various vector functions and statistical functions that are optimised for the 
	vector instruction sets of all recent Intel processor families
	</li>
</ul><p>For further documentation, we refer to <a href=\"#FurtherInfo\">the links to the Intel documentation at the bottom of this page</a>.
</p><p>There are two ways to link the MKL library:
</p><ul>
	<li>If you use icc, icpc or ifort to link your code, you can use the -mkl compiler 
	option:
	<ul>
		<li>-mkl=parallel or -mkl: Link the multi-threaded version of the library.</li>
		<li>-mkl=sequential: Link the single-threaded version of the library</li>
		<li>-mkl=cluster: Link the cluster-specific and sequential library, i.e., 
		ScaLAPACK will be included, but assumes one process per core (so no hybrid MPI/multi-threaded approach)
		</li>
	</ul>
	The Fortran95 interface library for lapack is not automatically included though. 
	You'll have to specify that library seperately. You can get the value from the 
	<a href=\"https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor\" target=\"_blank\">MKL 	Link Line Advisor</a>, see also the next item.</li>
	<li>Or you can specify all libraries explictly. To do this, it is strongly recommended 
	to use Intel's 
	<a href=\"https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor/\" target=\"_blank\">MKL 	Link Line Advisor</a>, and will also tell you how to link the MKL library with 
	code generated with the GNU and PGI compilers.
	<br>
	<strong>Note:</strong> On most VSC systems, the variable MKLROOT has a different 
	value from the one assumed in the Intel documentation. Wherever you see 
	<code>$(MKLROOT)</code> you may have to replace it with 
	<code>$(MKLROOT)/mkl</code>.</li>
</ul><p>MKL also offers a very fast streaming pseudorandom number generator, see the 
documentation for details.
</p><h2><a name=\"intel-versions\"></a>Intel toolchain version numbers</h2><table style=\"width: 100%;\" border=\"1\" cellpadding=\"1\" cellspacing=\"1\">
<tbody>
<tr>
	<th>
	</th>
	<th>2018a</th><th>2017b</th><th>2017a
	</th>
	<th>2016b
	</th>
	<th>2016a
	</th>
	<th>2015b
	</th>
	<th>2015a
	</th>
	<th>2014b
	</th>
	<th>2014a
	</th>
</tr>
<tr>
	<td>icc/icpc/ifort
	</td>
	<td>2018.1.163</td><td>2017.4.196</td><td>2017.1.132
	</td>
	<td>16.0.3 20160425
	</td>
	<td>16.0.1 20151021
	</td>
	<td>15.0.3 20150407
	</td>
	<td>15.0.1 20141023
	</td>
	<td>13.1.3 20130617
	</td>
	<td>13.1.3 20130607
	</td>
</tr>
<tr>
	<td>Intel MPI
	</td>
	<td>2018.1.163</td><td>2017.3.196</td><td>2017.1.132
	</td>
	<td>5.1.3.181
	</td>
	<td>5.1.2.150
	</td>
	<td>5.03.3048
	</td>
	<td>5.0.2.044
	</td>
	<td>4.1.3.049
	</td>
	<td>4.1.3.045
	</td>
</tr>
<tr>
	<td>Intel MKL
	</td>
	<td>2018.1.163</td><td>2017.3.196</td><td>2017.1.132
	</td>
	<td>11.3.3.210
	</td>
	<td>11.3.1.150
	</td>
	<td>11.2.3.187
	</td>
	<td>11.2.1.133
	</td>
	<td>11.1.2.144
	</td>
	<td>11.1.1.106
	</td>
</tr>
<tr>
	<td>GCC
	</td>
	<td>6.4.0</td><td>6.4.0</td><td>6.3.0
	</td>
	<td>4.9.4
	</td>
	<td>4.9.3
	</td>
	<td>4.9.3
	</td>
	<td>4.9.2
	</td>
	<td>4.8.3
	</td>
	<td>/
	</td>
</tr>
<tr>
	<td>binutils
	</td>
	<td>2.28</td><td>2.28</td><td>2.27
	</td>
	<td>2.26
	</td>
	<td>2.25
	</td>
	<td>2.25
	</td>
	<td>/
	</td>
	<td>/
	</td>
	<td>/
	</td>
</tr>
</tbody>
</table><h2><a name=\"FurtherInfo\"></a>Further information on Intel tools</h2><ul>
	<li>All Intel documentation of recent software versions is available in the 
	<a href=\"https://software.intel.com/en-us/documentation\" target=\"_blank\">Intel 	Software Documentation Library</a>. The documentation is typically available for the most recent version and sometimes one older version of te compiler and libraries.</li>
	<li>Some other useful documents:
	<ul>
		<li><a href=\"https://software.intel.com/en-us/articles/step-by-step-optimizing-with-intel-c-compiler\" target=\"_blank\">Step by Step Performance Optimization with Intel® C++ Compiler</a>. Despite the title, the remarks also hold for the C and Fortran compilers.</li>
		<li><a href=\"https://software.intel.com/en-us/compiler_15.0_ug_c\" target=\"_blank\">Direct link to the C/C++ compiler 15.0 user and reference guide</a> (2015a and 2015b toolchains)</li>
		<li><a href=\"https://software.intel.com/en-us/intel-cplusplus-compiler-16.0-user-and-reference-guide\" target=\"_blank\">Direct link to the C/C++ compiler 16.0 user and reference guide</a> (2016a and 2016b toolchains)</li>
		<li><a href=\"https://software.intel.com/en-us/intel-fortran-compiler-16.0-user-and-reference-guide\" target=\"_blank\">Direct link to the Fortran compiler 16.0 user and reference guide</a> (2016a and 2016b toolchains)</li>
		<li><a href=\"https://software.intel.com/en-us/articles/intel-mpi-library-documentation\" target=\"_blank\">Page with links to the documentation of the most recent version of Intel MPI</a></li>
	</ul></li>
	<li>MKL
	<ul>
		<li><a href=\"https://software.intel.com/en-us/articles/intel-math-kernel-library-documentation/\" target=\"_blank\">Link 
		page to the documentation of MKL 11.2/11.3 on the Intel web site
		</a> (toolchains 2015a till 2016b)</li>
		<li><a href=\"https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor\" target=\"_blank\">MKL 
		Link Line Advisor
		</a></li>
	</ul>
	</li>
	<li><a href=\"/cluster-doc/development/blas-lapack#Links\" target=\"_blank\">Generic BLAS/LAPACK/ScaLAPACK 	documentation</a></li>
</ul>"
623,"","<p>The <code>foss</code> toolchain consists entirely of free and open source software 
components. When building third-party software, or developing your own, 
load the module for the toolchain:
</p><pre>$ module load foss/&lt;version&gt;
</pre><p>where <code>&lt;version&gt;</code> should be replaced by the one to be used, e.g., 
	<code>2014a</code>. See the documentation on the software module system for more details.
</p><h2><a name=\"gcc-compilers\">Compilers: GNU</a></h2><p>Three GCC compilers are available:
</p><ul>
	<li>C: <code>gcc</code></li>
	<li>C++: <code>g++</code></li>
	<li>Fortran: <code>gfortran</code></li>
</ul><p>For example, to compile/link a Fortran program <code>fluid.f90</code> to an executable 
	<code>fluid</code> with architecture specific optimization for processors that support AVX instructions, use:
</p><pre>$ gfortran -O2 -march=corei7-avx -o fluid fluid.f90
</pre><p>Documentation on GCC compiler flags and options is available on the 
	<a href=\"http://gcc.gnu.org/onlinedocs/\">project's website</a>. Do not forget to load the 
toolchain module first!
</p><h3><a name=\"foss-openmp\"></a>GCC OpenMP</h3><p>The compiler switch to use to compile/link OpenMP C/C++ or Fortran code is 
	<code>-fopenmp</code>. For example, to compile/link a OpenMP C program 
	<code>scattter.c</code> to an executable 
	<code>scatter</code> with optimization for processors that support the AVX instruction 
set, use:
</p><pre>$ gcc -fopenmp -O2 -march=corei7-avx -o scatter scatter.c
</pre><p>Remember to specify as many processes per node as the number of threads the 
executable is supposed to run. This can be done using the 
	<code>ppn</code> resource, e.g., 
	<code>-l nodes=1:ppn=10</code> for an executable that should be run with 10 OpenMP threads. 
The number of threads should not exceed the number of cores on a compute node.
</p><p>Note that the OpenMP runtime library used by GCC is of inferior quality when compared 
to Intel's, so developers are strongly encouraged to use the 
	<a href=\"#intel-toolchain\"><code>intel</code> toolchain</a> when developing/building OpenMP software.
</p><h2><a name=\"openmpi\">Communication library: Open MPI</a></h2><p>For the <code>foss</code> toolchain, Open MPI is used as the communications library. 
To compile/link MPI programs, wrappers are supplied, so that the correct headers and 
libraries are used automatically. These wrappers are:
</p><ul>
	<li>C: <code>mpicc</code></li>
	<li>C++: <code>mpic++</code></li>
	<li>Fortran: <code>mpif77</code>, 
	<code>mpif90</code></li>
</ul><p>The compiler wrappers take the same options as the corresponding compilers.
</p><h3>Using the MPI compilers from Open MPI</h3><p>For example, to compile/link a C program <code>thermo.c</code> to an executable 
	<code>thermodynamics</code> with architecture specific optimization for the AVX 
instruction set, use:
</p><pre>$ mpicc -O2 -march=corei7-avx -o thermodynamics thermo.c
</pre><p>Extensive documentation is <a href=\"https://www.open-mpi.org/doc/\">provided on the Open MPI project's website</a>. Do not forget to load the toolchain module first.
</p><h3>Running an Open MPI program</h3><p>Note that an MPI program must be run with the exact same version of the toolchain as 
it was originally build with. The listing below shows a PBS job script 
	<code>thermodynamics.pbs</code> that runs the 
	<code>thermodynamics</code> executable.
</p><pre>#!/bin/bash -l 
module load intel/&lt;version&gt; 
cd $PBS_O_WORKDIR 
mpirun ./thermodynamics
</pre><p>The hosts and number of processes is retrieved from the queue system, that gets this 
information from the resource specification for that job.
</p><h2><a name=\"foss-mathlibs\">FOSS mathematical libraries</a></h2><p>The foss toolchain contains the basic HPC mathematical libraries, it offers:
</p><ul>
	<li><a href=\"http://www.openblas.net/\" target=\"_blank\">OpenBLAS</a> (Basic Linear Algebra Subsystem)</li>
	<li><a href=\"http://www.netlib.org/lapack/\" target=\"_blank\">Lapack </a>(Linear Algebra PACKage)</li>
	<li>ScaLAPACK (Scalable Linear Algebra PACKage)</li>
	<li><a href=\"http://www.fftw.org/\" target=\"_blank\">FFTW</a> (Fastest Fourier Transform in the West)</li>
</ul><h2>Other components</h2><ul>
	<li>From the 2015b series on, binutils was added to the toolchain. The binutils package contains the assembler used by gcc, and the standard OS assembler doesn't always support the newer instructions that are used on newer cluster nodes.</li>
</ul><h2><a name=\"foss-versions\"></a>Version numbers</h2><table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width: 100%;\">
<tbody>
<tr>
	<th>
	</th>
	<th>2018a</th><th>2017b</th><th>2017a</th><th>2016b
	</th>
	<th>2016a
	</th>
	<th>2015b
	</th>
	<th>2015a
	</th>
	<th>2014b
	</th>
	<th>2014a
	</th>
</tr>
<tr>
	<td>GCC
	</td>
	<td>6.4.0</td><td>6.4.0</td><td>6.3</td><td>5.4
	</td>
	<td>4.9.3
	</td>
	<td>4.9.3
	</td>
	<td>4.9.2
	</td>
	<td>4.8.3
	</td>
	<td>4.8.2
	</td>
</tr>
<tr>
	<td>OpenMPI
	</td>
	<td>2.1.2</td><td>2.1.1</td><td>2.0.2</td><td>1.10.3
	</td>
	<td>1.10.2
	</td>
	<td>1.8.8
	</td>
	<td>1.8.4
	</td>
	<td>1.8.1
	</td>
	<td>1.6.5
	</td>
</tr>
<tr>
	<td>OpenBLAS
	</td>
	<td>0.2.20</td><td>0.2.20</td><td>0.2.19</td><td>0.2.18
	</td>
	<td>0.2.15
	</td>
	<td>0.2.14
	</td>
	<td>0.2.13
	</td>
	<td>0.2.9
	</td>
	<td>0.2.8
	</td>
</tr>
<tr>
	<td>LAPACK
	</td>
	<td>3.8.0</td><td>3.8.0</td><td>3.3.6</td><td>3.6.1
	</td>
	<td>3.6.0
	</td>
	<td>3.5.0
	</td>
	<td>3.5.0
	</td>
	<td>3.5.0
	</td>
	<td>3.5.0
	</td>
</tr>
<tr>
	<td>ScaLAPACK
	</td>
	<td>2.0.2</td><td>2.0.2</td><td>2.0.2</td><td>2.0.2
	</td>
	<td>2.0.2
	</td>
	<td>2.0.2
	</td>
	<td>2.0.2
	</td>
	<td>2.0.2
	</td>
	<td>2.0.2
	</td>
</tr>
<tr>
	<td>FFTW
	</td>
	<td>3.3.7</td><td>3.3.6</td><td>3.3.6</td><td>3.3.4
	</td>
	<td>3.3.4
	</td>
	<td>3.3.4
	</td>
	<td>3.3.4
	</td>
	<td>3.3.4
	</td>
	<td>3.3.3
	</td>
</tr>
<tr>
	<td>binutils
	</td>
	<td>2.28</td><td>2.28</td><td>2.27</td><td>2.26
	</td>
	<td>2.25
	</td>
	<td>2.25
	</td>
	<td>/
	</td>
	<td>/
	</td>
	<td>/
	</td>
</tr>
</tbody>
</table><h2>Further information on FOSS components</h2><ul>
	<li><a href=\"https://gcc.gnu.org/onlinedocs/\" target=\"_blank\">Overview of GCC manuals 
	(all versions)
	</a></li>
	<li>OpenMPI documentation
	<ul>
		<li><a href=\"https://www.open-mpi.org/doc/v2.0/\" target=\"_blank\">2.0.x (foss/2017a)</a></li><li><a href=\"https://www.open-mpi.org/doc/v1.10/\" target=\"_blank\">1.10.x (foss/2016a and foss/2016b)</a></li><li><a href=\"https://www.open-mpi.org/doc/v1.8/\" target=\"_blank\">1.8.x (foss/2014b, foss/2015a and foss/2015b)</a></li>
		<li><a href=\"https://www.open-mpi.org/doc/v1.6/\" target=\"_blank\">1.6.x (foss/2014a)</a></li>
	</ul>
	</li>
	<li>The <a href=\"http://www.openblas.net/\" target=\"_blank\">OpenBLAS project page</a> and 
	<a href=\"https://github.com/xianyi/OpenBLAS/wiki\" target=\"_blank\">documentation Wiki</a></li>
	<li><a href=\"/cluster-doc/development/blas-lapack#Links\">Generic BLAS/LAPACK/ScaLAPACK 
	documentation
	</a></li>
	<li><a href=\"http://www.fftw.org/#documentation\" target=\"_blank\">FFTW documentation</a></li>
	<li><a href=\"https://sourceware.org/binutils/docs/\" target=\"_blank\">GNU binutils documentation</a></li>
</ul>"
625,"","<p>MPI and OpenMP both have their advantages and disadvantages.
</p>
<p>MPI can be used on 
    distributed memory clusters and can scale to thousands of nodes. However, it was 
    designed in the days that clusters had nodes with only one or two cores. Nowadays CPUs 
    often have more than ten cores and sometimes support multiple hardware threads (or logical cores) per 
    physical core (and in fact may need multiple threads to run at full performance). At the same 
    time, the amount of memory per hardware thread is not increasing and is in fact quite
    low on several architectures that rely on a large number of slower cores or hardware
    threads to obtain a high performance within a reasonable power budget. Starting
    one MPI process per hardware thread is then a waste of resources as each process needs
    its communication buffers, OS resources, etc. Managing the hundreds of thousands of MPI
    processes that we are nowadays seeing on the biggest clusters, is very hard.
</p>
<p>OpenMP on the other hand is limited to shared memory parallelism, typically
    within a node of a cluster. Moreover, many OpenMP programs don't scale past some
    tens of threads partly because of thread overhead in the OS implementation and partly
    because of overhead in the OpenMP run-time.
</p>
<p>Hybrid programs try to combine the advantages of both to deal with the
    disadvantages. Hybrid programs use a limited number of MPI processes (\"MPI ranks\") 
    per node and use OpenMP threads to further exploit the parallelism within the node.
    An increasing number of applications is designed or re-engineered in this way.
    The optimum number of MPI processes (and hence OpenMP threads per process) depends
    on the code, the cluster architecture and the problem that is being solved, but 
    often one or, on newer CPUs such as the Intel Haswell, two MPI processes per socket (so 
    two to four for a typical two-socket node) is close to optimal. Compiling and
    starting such applications requires some care as we explain on this page.
</p>
<h2>Preparing your hybrid application to run</h2>
<p>To compile and link your hybrid application, you basically have to combine the 
      instructions for MPI and OpenMP
      programs: use 
	<code>mpicc -fopenmp</code> for the GNU
      compilers and 
	<code>mpiicc -qopenmp</code> for the Intel
      compilers (
	<code>mpiicc -openmp</code> for older versions) or the corresponding
      MPI Fortran compiler wrappers for Fortran programs.
</p>
<h2>Running hybrid programs on the VSC clusters</h2>
<p>When running a hybrid MPI/OpenMP program, fewer MPI processes
      have to be started than there are logoical cores available to
      the application as every process uses multiple cores in
      OpenMP parallelism. Yet when requesting logical cores per node to the
      scheduler, one still has to request the total number of cores
      needed per node. Hence the PBS property \"ppn\" should not be read
      as \"processes per node\" but rather as \"logical cores per node\" or 
      \"
	<u>p</u>rocessing units <u>p</u>er <u>n</u>ode\". Instead we
      have to tell the MPI launcher (
	<code>mpirun</code> for most applications) to 
      launch fewer processes than there are logical cores on a node and tell
      each MPI process to use the correct number of OpenMP threads.
</p>
<p>For optimal performance, the threads of one MPI process should be
      put together as close as possible in the logical core hierarchy
      implied by the cache and core topology of a given node. E.g., on a
      dual socket node it may make a lot of sense to run 2 MPI processes
      with each MPI process using all cores on a single socket. In other
      applications, it might be better to run only one MPI process per
      node, or multiple MPI processes per socket. In more technical words,
      each MPI process runs in its MPI domain consisting of a number of logical cores,
      and we want these domains to be non-overlapping and fixed in time during the life of
      the MPI job and the logical cores in the domain to be \"close\" to each other.
      This optimises the use the memory hierarchy (cache and RAM).
</p>
<p>OpenMP has several environment variables that can then control the number
      of OpenMP threads and the placement of the threads in the MPI domain. All of these
      may also be overwritten by the application, so it is not a
      bullet-proof way to control the behaviour of OpenMP applications.
      Moreover, some of these environment variables are
      implementation-specific and hence are different between the Intel
      and GNU OpenMP runtimes. The most important variable is 
	<code>OMP_NUM_THREADS</code>. It
      sets the number of threads to be used in parallel regions. As
      parallel constructs can be nested, a process may still start more
      threads than indicated by 
	<code>OMP_NUM_THREADS</code>. However,
      the total number of threads can be limited by the variable 
	<code>OMP_THREAD_LIMIT</code>.
</p>
<h2>Script mympirun (VSC)</h2>
<p>The mympirunn script is developed by the UGent VSC-team to cope
      with differences between different MPI implementations
      automatically. It offers support for hybrid programs through
      the 
	<code>--hybrid</code> command line switch to specify the number of 
      processes per node. The number of threads per process can then be
      computed by dividing the number of logical cores per node by the
      number of processes per node.
</p>
<p>E.g., to run a hybrid MPI/OpenMP program on 2 nodes using 20
      cores on each node and running 4 MPI ranks per node (hence 5
      OpenMP threads per MPI rank), your script would contain
</p>
<pre>#PBS -l nodes=2:ppn20
</pre>
<p> near the top to request the resources from the scheduler. It
      would then load the appropriate module with the mympirun command:
</p>
<pre>module load vsc-mympirun
</pre>
<p> (besides other modules that are needed to run your application)
      and finally start your application:
</p>
<pre>mympirun --hybrid=4 ./hybrid_mpi
</pre>
<p> assuming your executable is called hybrid_mpi and resides in the
      working directory. The mympirun launcher will automatically
      determine the correct number of MPI processes to start based on
      the resource specifications and the given number of processes per
      node (the 
	<code>--hybrid</code> switch).
</p>
<h2>Intel toolchain</h2>
<p>
	On Intel MPI defining the MPI domains is done through the environment variable 
	<code>I_MPI_PIN_DOMAIN</code>.
    Note however that the Linux scheduler is still
    free to move all threads of a MPI process to any core within its MPI domain
    at any time, so there may be a point in further pinning the OpenMP threads through
    the OpenMP environment variables also.
    This is definitely the case if there are more logical cores available
    in the process partition than there are OpenMP threads. Some environment
    variables to influence the thread placement are 
    the Intel-specific variable 
	<code>KMP_AFFINITY</code> and the OpenMP 3.1
    standard environment variable 
	<code>OMP_PROC_BIND</code>.
</p>
<p>In our case, we want to use all logical cores of a node but make sure
    that all cores for a domain are as close together as possible. The
    easiest way to accomplish this is to set 
	<code>OMP_NUM_THREADS</code>
	to the desired number of OpenMP threads per MPI process and then set
	<code>I_MPI_PIN_DOMAIN</code> to the value omp:
</p>
<pre>export I_MPI_PIN_DOMAIN=omp
</pre>
<p>The longer version is
</p>
<pre>export I_MPI_PIN_DOMAIN=omp,compact
</pre>
<p>where compact tells the launcher explicitly to pack threads for
      a single MPI process as close together as possible. This layout is
      the default on current versions of Intel MPI so it is not really
      needed to set this. An alternative, when running 1 MPI process per
      socket, is to set
</p>
<pre>export I_MPI_PIN_DOMAIN=socket
</pre>
<p>To enforce binding of each OpenMP thread to a particular logical core, one can set
</p>
<pre>export OMP_PROC_BIND=true
</pre>
<p>As an example, assume again we want to run the program <code>hybridmpi</code>
	on 2 nodes containing 20 cores each, running 4 MPI processes per
      node, so 5 OpenMP threads per process.
</p>
<p>The following are then essential components of the job script:
</p>
<ul>
	<li>
	Specify the resource requirements: <br>
	<code>#PBS -lnodes=2:ppn=20</code>
	</li>
	<li>
	Load the modules, including one which contains Intel MPI,
          e.g., 
	<br>
	<code>module load intel</code>
	</li>
	<li>
	Create a list of unique hosts assigned to the job<br>
	<code>export HOSTS=`sort -u $PBS_NODEFILE | paste -s -d,`<br>
	</code>This step is very important; the program will not start
          with the correct number of MPI ranks if it is not provided
          with a list of unique host names. 
	<code><br>
	</code>
	</li>
	<li>
	Set the number of OpenMP threads per MPI process:<br>
	<code>export OMP_NUM_THREADS=5</code>
	</li>
	<li>
	Pin the MPI processes:<br>
	<code>export I_MPI_PIN_DOMAIN=omp</code>
	</li>
	<li>
	And launch hybrid_mpi using the Intel MPI launcher and
          specifying 4 MPI processes per host:
	<br>
	<code>mpirun -hosts $HOSTS -perhost 4 ./hybrid_mpi</code>
	</li>
</ul>
<p>In this case we do need to specify both the total number of MPI
      ranks and the number of MPI ranks per host as we want the same
      number of MPI ranks on each host. 
	<br>
	In case you need a more automatic script that is easy to adapt to
      a different node configuration or different number of processes
      per node, you can do some of the computations in Bash. The number
      of processes per node is set in the shell variable
	<code>MPI_RANKS_PER_NODE</code>. The above commands become:
</p>
<pre>#! /bin/bash -l
# Adapt nodes and ppn on the next line according to the cluster your're using!#PBS -lnodes=2:ppn=20
...
MPI_RANKS_PER_NODE=4
#
module load intel
#
export HOSTS=`sort -u $PBS_NODEFILE | paste -s -d,`
#
export OMP_NUM_THREADS=$(($PBS_NUM_PPN / $MPI_RANKS_PER_NODE))
#
export OMP_PROC_BIND=true
#
export I_MPI_PIN_DOMAIN=omp
#
mpirun -hosts $HOSTS -perhost $MPIPROCS_PER_NODE ./hybrid_mpi
</pre>
<h3>Intel documentation on hybrid programming</h3>
<p>Some documents on the Intel web site that contain more
      information on developing and running hybrid programs:
</p>
<ul>
	<li>
	\"<a href=\"https://software.intel.com/en-us/node/528819\">Interoperability with OpenMP API</a>\" in <a href=\"https://software.intel.com/en-us/articles/intel-mpi-library-documentation\">the MPI Reference Manual</a> explains the concept of MPI domains and 
how they should be used/set for hybrid programs.
	</li>
	<li>
	<a href=\"https://software.intel.com/en-us/articles/beginning-hybrid-mpiopenmp-development\" target=\"_blank\">Beginning Hybrid MPI/OpenMP Development</a>,
          useful if you develop your own code.
	</li>
</ul>
<h2>Foss toolchain (GCC and Open MPI)</h2>
<p>Open MPI has very flexible options for process and thread placement, but they are not always easy to use. There is however also a simple option to indicate the number of logical cores you want to assign to each MPI rank (MPI process): <code>-cpus-per-proc &lt;num&gt;</code> with &lt;num&gt; the number of logical cores assigned to each MPI rank.
</p>
<p>You may want to further control the thread placement one can using the standard OpenMP
        mechanism, e.g. the GNU-specific variable 
	<code>GOMP_CPU_AFFINITY</code>
	or the OpenMP 3.1 standard environment variable <code>OMP_PROC_BIND</code>.
        As long as we want to use all cores, it won't matter whether 
	<code>OMP_PROC_BIND</code>
	is set to true, close or spread. However, setting <code>OMP_PROC_BIND</code> to true is generally a safe choice to assure that all threads remain on the same core as they were started on to improve cache performance.
</p>
<p>Essential elements of our job script are:
</p>
<pre>#! /bin/bash -l
# Adapt nodes and ppn on the next line according to the cluster your're using!
#PBS -lnodes=2:ppn=20
...
#
module load foss
#
export OMP_NUM_THREADS=5
#
export OMP_PROC_BIND=true
#
mpirun -cpus-per-proc $OMP_NUM_THREADS ./hybrid_mpi
</pre>
<h3>Advanced issues </h3>
<p>Open MPI allows a lot of control over process placement and rank assignment. The Open MPI mpirun command has several options that influence this process:
</p>
<ul>
	<li><code>--map-by</code> influences the mapping of processes on the available processing resources</li>
	<li><code>--rank-by</code> influences the rank assignment</li>
	<li><code>--bind-to</code> influences the binding of processes to sets of processing resources</li>
	<li><code>--report-bindings</code> can then be used to report on the process binding.</li>
</ul>
<p>More information can be found in the manual pages for <code>mpirun</code> which can be found on <a href=\"https://www.open-mpi.org/doc/\">the Open MPI web pages</a> and in the following presentations:
</p>
<ul>
	<li>
	<a href=\"http://faculty.cs.uwlax.edu/%7Ejjhursey/papers/2011/hursey-cluster-poster-2011.pdf\" target=\"_blank\">Poster paper \"Locality-Aware Parallel Process Mapping for Multi-Core HPC Systems\"</a>
	</li>
	<li>
	<a href=\"https://www.slideshare.net/jsquyres/open-mpi-explorations-in-process-affinity-eurompi13-presentation\" target=\"_blank\">Slides from the presentation \"Open MPI Explorations in Process Affinity\" from EuroMPI'13</a>
	</li>
</ul>"
627,"","<ol class=\"list--ordered\">
	<li>Studying gene family evolution on the VSC Tier-2 and Tier-1 infrastructure<br><em>Setareh Tasdighian et al. (VIB/UGent)</em></li>
	<li>Genomic profiling of murine carcinoma models<br><em>B. Boeckx, M. Olvedy, D. Nasar, D. Smeets, M. Moisse, M. Dewerchin, C. Marine, T. Voet, C. Blanpain,D. Lambrechts (VIB/KU Leuven)</em></li>
	<li>Modeling nucleophilic aromatic substitution reactions with ab initio molecular dynamics<br><em>Samuel L. Moors et al. (VUB)</em></li>
	<li>Climate modeling on the Flemish Supercomputers<br><em>Fabien Chatterjee, Alexandra Gossart, Hendrik Wouters, Irina Gorodetskaya, Matthias Demuzere, Niels Souverijns, Sajjad Saeed, Sam Vanden Broucke, Wim Thiery, Nicole van Lipzig (KU Leuven)</em></li>
	<li>Simulating the evolution of large grain structures using the phase-field approach<br><em>Hamed Ravash, Liesbeth Vanherpe, Nele Moelans (KU Leuven)</em></li>
	<li>Multi-component multi-phase field model combined with tensorial decomposition<br><em>Inge Bellemans, Kim Verbeken, Nico Vervliet, Nele Moelans, Lieven De Lathauwer (UGent, KU Leuven)</em></li>
	<li>First-principle modeling of planetary magnetospheres: Mercury and the Earth<br><em>Jorge Amaya, Giovanni Lapenta (KU Leuven)</em></li>
	<li>Modeling the interaction of the Earth with the solar wind: the Earth magnetopause<br><em>Emanuele Cazzola, Giovanni Lapenta (KU Leuven)</em></li>
	<li>Jupiter's magnetosphere<br><em>Emmanuel Chané, Joachim Saur, Stefaan Poedts (KU Leuven)</em></li>
	<li>High-performance computing of wind-farm boundary layers<br><em>Dries Allaerts, Johan Meyers (KU Leuven)</em></li>
	<li>Large-eddy simulation study of Horns Rev windfarm in variable mean wind directions<br><em>Wim Munters, Charles Meneveau, Johan Meyers (KU Leuven)</em></li>
	<li>Modeling defects in the light absorbing layers of photovoltaic cells<br><em>Rolando Saniz, Jonas Bekaert, Bart Partoens, Dirk Lamoen (UAntwerpen)</em></li>
	<li>Molecular Spectroscopy : Where Theory Meets Experiment<br><em>Carl Mensch, Evelien Van de Vondel, Yannick Geboes, Pilar Rodríguez Ortega, Liene De Beuckeleer, Sam Jacobs, Jonathan Bogaerts, Filip Desmet, Christian Johannessen, Wouter Herrebout (UAntwerpen)</em></li>
	<li><span></span>On the added value of complex stock trading rules in short-term equity price direction prediction<br><em>Dirk Van den Poel, Céline Chesterman, Maxim Koppen, Michel Ballings (UGent University, University of Tennessee at Knoxville)</em></li>
	<li><span></span>First-principles study of the surface and adsorption properties of α-Cr<sub>2</sub>O<sub>3</sub><br><em>Samira Dabaghmanesh, Erik C. Neyts, Bart Partoens (UAntwerpen)</em></li>
	<li><span></span>The surface chemistry of plasma-generated radicals on reduced titanium dioxide<br><em>Stijn Huygh, Erik C. Neyts (UAntwerpen)</em></li>
	<li><span></span>The High Throughput Approach to Computational Materials Design<br><em>Michael Sluydts, Titus Crepain, Karel Dumon, Veronique Van Speybroeck, Stefaan Cottenier (UGent)</em></li>
	<li><span></span>Distributed Memory Reduction in Presence of Process Desynchronization<br><em>Petar Marendic, Jan Lemeire, Peter Schelkens (Vrije Universiteit Brussel, iMinds)</em></li>
	<li><span></span>Visualization @HPC KU Leuven<br><em>Mag Selwa (KU Leuven)</em></li>
	<li><span></span>Multi-fluid modeling of the solar chromosphere<br><em>Yana G. Maneva, Alejandro Alvarez-Laguna, Andrea Lani, Stefaan Poedts (KU Leuven)</em></li>
	<li><span></span>Molecular dynamics in momentum space<br><em>Filippo Morini (UHasselt)</em></li>
	<li><span></span>Predicting sound in planetary inner cores using quantum physics<br><em>Jan Jaeken, Attilio Rivoldini, Tim van Hoolst, Veronique Van Speybroeck, Michel Waroquier, Stefaan Rottener (UGent)</em></li>
	<li><span></span>High Fidelity CFD Simulations on Tier-1<br><em>Leonidas Siozos-Rousoulis, Nikolaos Stergiannis, Nathan Ricks, Ghader Ghorbaniasl, Chris Lacor (VUB)</em></li>
</ol>"
629,"","<h2><a name=\"DerekGroen\"></a>High performance and multiscale computing: blood, clay, stars and humans</h2><p><em>Speaker: Derek Groen (Centre for Computational Science, University College London)</em>
</p><p>Multiscale simulations are becoming essential across many scientific disciplines. The concept of having multiple models form a single scientific simulation, with each model operating on its own space and time scale, gives rise to a range of new challenges and trade-offs. In this talk, I will present my experiences with high performance and multiscale computing. I have used supercomputers for modelling clay-polymer nanocomposites [1], blood flow in the human brain [2], and dark matter structure formation in the early universe [3]. I will highlight some of the scientific advances we made, and present the technologies we developed and used to enable simulations across supercomputers (using multiple models where convenient). In addition, I will reflect on the non-negligible aspect of human effort and policy constraints, and share my experiences in enabling challenging calculations, and speeding up more straightforward ones.
</p><p>[<a href=\"/assets/1057\">slides - PDF 8.3MB</a>]</p><h3>References</h3><ol class=\"list--ordered\">
	<li>James L. Suter, Derek Groen, and Peter V. Coveney. <a href=\"http://dx.doi.org/10.1002/adma.201403361\">Chemically Specific Multiscale Modeling of Clay–Polymer Nanocomposites Reveals Intercalation Dynamics, Tactoid Self-Assembly and Emergent Materials Properties</a>. Advanced Materials, volume 27, issue 6, pages 966–984. (DOI: <a href=\"http://dx.doi.org/10.1002/adma.201403361\">10.1002/adma.201403361</a>)</li>
	<li>Mohamed A. Itani, Ulf D. Schiller, Sebastian Schmieschek, James Hetherington, Miguel O. Bernabeu, Hoskote Chandrashekar, Fergus Robertson, Peter V. Coveney, and Derek Groen. <a href=\"http://dx.doi.org/10.1016/j.jocs.2015.04.008\">An automated multiscale ensemble simulation approach for vascular blood flow</a>. Journal of Computational Science, volume 9, pages 150-155. (DOI: <a href=\"http://dx.doi.org/10.1016/j.jocs.2015.04.008\">10.1016/j.jocs.2015.04.008</a>)</li>
	<li>Derek Groen and Simon Portugies Zwart. <a href=\"http://dx.doi.org/10.1109/eScience.2015.81 \">From Thread to Transcontinental Computer: Disturbing Lessons in Distributed Supercomputing</a>. 2015 IEEE 11th International Conference on e-Science, IEEE, pages 565-571. (DOI: <a href=\"http://dx.doi.org/10.1109/eScience.2015.81 \">10.1109/eScience.2015.81</a>)
	</li>
</ol><h2><a name=\"JohanMeyers\"></a>High-performance computing of wind farms in the atmospheric boundary layer</h2><p><em>Speaker: Johan Meyers (Department of Mechanical Engineering, KU Leuven)</em>
</p><p>
The aerodynamics of large wind farms are governed by the interaction between turbine wakes, and by the interaction of the wind farm as a whole with the atmospheric boundary layer. The deceleration of the flow in the farm that is induced by this interaction, leads to an efficiency loss for wind turbines downstream in the farm that can amount up to 40% and more. Research into a better understanding of wind-farm boundary layer interaction is an important driver for reducing this efficiency loss. The physics of the problem involves a wide range of scales, from farm scale and ABL scale (requiring domains of several kilometers cubed) down to turbine and turbine blade scale with flow phenomena that take place on millimeter scale. Modelling such a system, requires a multi-scale approach in combination with extensive supercomputing. To this end, our simulation code SP-Wind is used. Implementation issues and parallelization are discussed. Next to that, new physical insights gained from our simulations at the VSC are highlighted.
</p><p>[<a href=\"/assets/1055\">slides - PDF 9.9MB</a>]</p>"
631,"","<p>Not only have supercomputers changed scientific research in a fundamental way, they also enable the development of new, affordable products and services which have a major impact on our daily lives.
</p>
<h2>Not only have supercomputers changed scientific research in a fundamental way  ...</h2>
<p>Supercomputers are indispensable for scientific research and for a modern R&D environment. ‘Computational Science’ is - alongside theory and experiment - the third fully fledged pillar of science. For centuries, scientists used pen and paper to develop new theories based on scientific experiments. They also set up new experiments to verify the predictions derived from these theories (a process often carried out with pen and paper). It goes without saying that this method was slow and cumbersome.
</p>
<p>As an astronomer you can not simply make Jupiter a little bigger to see what effect this would lager size would have on our solar system. As a nuclear scientist it would be difficult to deliberately lose control over a nuclear reaction to ascertain the consequences of such a move. (Super)computers can do this and are indeed revolutionizing science.
</p>
<p>Complex theoretical models - too advanced for ‘pen and paper’ results - are simulated on computers. The results they deliver, are then compared with reality and used for prediction purposes. Supercomputers have the ability to handle huge amounts of data, thus enabling experiments that would not be achievable in any other way. Large radio telescopes or the LHC particle accelerator at CERN could not function without supercomputers processing mountains of data.
</p>
<h2>… but also the industry and out society</h2>
<p>But supercomputers are not just an expensive toy for researchers at universities. Numerical simulation also opens up new possibilities in industrial R&D. For example in the search for new medicinal drugs, new materials or even the development of a new car model. Biotechnology also requires the large data processing capacity of a supercomputer. The quest for clean energy, a better understanding of the weather and climate evolution, or new technologies in health care all require a powerful supercomputer.
</p>
<p>Supercomputers have a huge impact on our everyday lives. Have you ever wondered why the showroom of your favourite car brand contains many more car types than 20 years ago? Or how each year a new and faster smartphone model is launched on the market? We owe all of this to supercomputers.
</p>"
637,"What is a supercomputer?","<p>A supercomputer is a very fast and extremely parallel computer. Many of its technological properties are comparable to those of your laptop or even smartphone. But there are also important differences.
</p>"
639,"Impact on research, industry and society","<p>Not only have supercomputers changed scientific research in a fundamental way, they also enable the development of new, affordable products and services which have a major impact on our daily lives.</p>"
645,"","<h2><a name=\"Muk\"></a>Tier-1b thin node supercomputer BrENIAC</h2><p>This system is since October 2016 in 
production use.
</p><h3>Purpose</h3><p>On this cluster you can run highly parallel, large scale computations that rely critically on efficient communication.
</p><h3>Hardware</h3><ul><li>580 computing nodes
	<ul><li>Two 14-core Intel Xeon processors (Broadwell, E5-2680v4)</li><li>128 GiB RAM (435 nodes) or 256 GiB (145 nodes)</li></ul></li><li>EDR InfiniBand interconnect
	<ul><li>High bandwidth (11.75 GB/s per direction, per link)</li><li>Slightly improved latency over FDR</li></ul></li><li>Storage system
	<ul><li>Capacity of 634 TB</li><li>Peak bandwidth of 20 GB/s</li></ul></li></ul><h2></h2><h2>Software<br></h2><p>You will find the standard Linux HPC software stack installed on the Tier-1 cluster. If required, user support will install additional (Linux) software you require, but you are responsible for taking care of the licensing issues (including associated costs).
</p><h3>Access</h3><p>You can get access to this infrastructure by applying for a starting grant, submitting a project proposal that will be evaluated on scientific and technical merits, or by buying compute time.</p><h2><a name=\"GPU\"></a></h2>"
649,"","<h2>The VSC account</h2><p>In order to use the infrastructure of the VSC, you need a VSC-userid, also called a VSC account. The account gives you access to most of the infrastructure, though only with a limited compute time allocation on some of the systems. Also, For the main Tier-1 compute cluster you need to submit a project application (or you should be covered by a project application within your research group). For some more specialised hardware you have to ask access separately, typically to the coordinator of your institution, because we want to be sure that that (usually rather expensive hardware) is used efficiently for the type of applications for which it was purchased.
</p><h2>Who can get a VSC account?</h2><ul>
	<li><strong>Researchers at the Flemish university associations</strong>. In many cases, this is done through a fully automated application process, but in some cases you must submit a request to your local support team. Specific details about these procedures can be found on the \"<a href=\"/cluster-doc/account-request\">Account request</a>\" page in the user documentation.</li>
	<li><strong>Master students in the framework of their master thesis</strong> if supercomputing is needed for the thesis. For this, you will first need the approval of your supervisor. The details about the procedure can again be found on the \"<a href=\"/cluster-doc/account-request\">Account request</a>\" page in the user documentation.</li>
	<li><strong>Use in courses at the University of Leuven and Hasselt University: L</strong>ecturers can also use the local Tier-2 infrastructure in the context of some courses (when the software cannot run in the PC classes or the computers in those classes are not powerful enough). Again, you can find all the details about the application process on the \"<a href=\"/cluster-doc/account-request\">Account request</a>\" page in the user documentation. It is important that the application is submitted on time, at least two weeks before the start of the computer sessions.</li>
	<li><strong>Researchers from iMinds and VIB</strong>. The application is made through your host university. The same applies to researchers at the university hospitals and research institutes under the direction or supervision of a university or a university college, such as the special university institutes mentioned in Article 169quater of the Decree of 12 June 1991 concerning universities in the Flemish Community.</li>
	<li><strong>Researchers at other Flemish public research institutions:</strong> You can get compute on the Tier-1 infrastructure through a project application or access the Tier-2 infrastructure through contact with one of the coordinators. </li>
	<li><strong>Businesses, non-Flemish public knowledge institutions and not-for-profit organisations</strong> can buy compute time on the infrastructure. The procedures are explained on the page \"<a href=\"/en/access-and-infrastructure/access-industry\">Buying compute time</a>\".</li>
</ul><h2>Additional information</h2><p>Before you apply for VSC account, it is useful to first check whether the infrastructure is suitable for your application. Windows or OS X programs for instance cannot run on our infrastructure as we use the Linux operating system on the clusters. The infrastructure also should not be used to run applications for which the compute power of a good laptop is sufficient. The pages on the <a href=\"/en/access-and-infrastructure/tier-1-clusters\">Tier-1</a> and <a href=\"/en/access-and-infrastructure/tier-2-clusters\">Tier-2 infrastructure</a> in this part of the website give a high-level description of our infrastructure. You can find more detailed information in the user documentation on the user portal. When in doubt, you can also contact your local support team. This does not require a VSC account.
</p><p>You should also first check the page \"<a href=\"/cluster-doc/account-request\">Account request</a>\" in the <a href=\"/en/user-portal\">user documentation</a> and install the necessary software on your PC. You can also find links to information about that software on the “Account Request” page.
</p><p>Furthermore, it can also be useful to take one of the introductory courses that we organise periodically at all universities. However, it is best to apply for your VSC account before the course since you also can then also do the exercises during the course. We strongly urge people who are not familiar with the use of a Linux supercomputer to take such a course. After all, we do not have enough staff to help everyone individually for all those generic issues.
</p><p>There is an exception to the rule that you need a VSC account to access the VSC systems: Users with a valid VUB account can access the Tier-2 systems at the VUB.
</p><p>Your account also includes two “blocks” of disk space: your home directory and data directory. Both are accessible from all VSC clusters. When you log in to a particular cluster, you will also be assigned one or more blocks of temporary disk space, called scratch directories. Which directory should be used for which type of data, is explained in the <a href=\"https://www.vscentrum.be/en/user-portal\">user documentation</a>.
</p><p>Your VSC account does not give you access to all available software. You can use all free software and a number of compilers and other development tools. For most commercial software, you must first prove that you have a valid license or the person who has paid the license on the cluster must allow you to use the license. For this you can contact your local support team.
</p>"
655,"","<p>A collaboration with the VSC offers your company a good number of benefits.
</p><ul>
	<li>Together
we will identify which expertise within the Flemish universities and their
associations is appropriate for you when rolling out High Performance Computing
(HPC) within your company.
	</li>
	<li>We
can also assist with the technical writing of a project proposal for financing for example through  the IWT (Agency for
Innovation by Science and Technology).
	</li>
	<li>You
can participate in courses on HPC, including tailor-made courses provided by the VSC.
	</li>
	<li>You
will have access to a supercomputer infrastructure with a dedicated, on-site
team assisting you during the start-up phase.
	</li>
	<li>As
a software developer, you can also deploy HPC software technologies to develop
more efficient software which makes better use of modern hardware.
	</li>
	<li>A
shorter turnaround time for your simulation or data study boosts productivity
and increases the responsiveness of your business to new developments.
	</li>
	<li>The
possibility to carry out more detailed simulations or to analyse larger amounts
of data can yield new insights which in turn lead to improved products and more
efficient processes.
	</li>
	<li>A
quick analysis of the data collected during a production process helps to
detect and correct abnormalities early on.
	</li>
	<li>Numerical
simulation and virtual engineering reduce the number of prototypes and
accelerate the discovery of potential design problems. As a result you are able
to market a superior product faster and cheaper. 
	</li>
</ul>"
659,"","<p>Modern microelectronics has created many new opportunities. Today powerful supercomputers enable us to collect and process huge amounts of data. Complex systems can be studied through numerical simulation without having to build a prototype or set up a scaled experiment beforehand. All this leads to a quicker and cheaper design of new products, cost-efficient processes and innovative services. To support this development in Flanders, the Flemish Government was founded in late 2007. Our accumulated expertise and infrastructure is also available to the industry for R&D.
</p>"
661,"Our offer to you","<p>Thanks to our embedding in academic institutions, we cannot only offer you infrastructure at competitive rates but also expert advice and training.</p>"
663,"About us","<p>The VSC is a collaboration between the Flemish Government and five Flemish university associations. Many of the VSC employees have a strong technical and scientific background. </p>"
671,"","<p>The VSC was launched in late 2007 as a collaboration between the Flemish Government and five Flemish university associations. Many of the VSC employees have a strong technical and scientific background. Our team also collaborates with many research groups at various universities and helps them and their industrial partners with all aspects of infrastructure usage.
</p><p>Besides a competitive infrastructure, the VSC team also offers full assistance with the introduction of High Performance Computing within your company.
</p><h2><a name=\"contact\"></a>Contact</h2><p>Coordinator industry access and services: <a href=\"mailto:industry@fwo.be\">industry@fwo.be</a></p><p>Alternatively, you can contact one of <a href=\"/en/contact\">the VSC coordinators</a>.<br>
</p>"
673,"","<p><a href=\"/en/hpc-for-industry/about-us#contact\">Get in touch with us!</a></p>"
681,"","<h2>Overview of the storage infrastructure </h2><p><span class=\"tx2\">Storage is an important part of a cluster. But not all the storage has the same characteristics. HPC cluster storage at KU Leuven consists of 3 different storage Tiers, optimized for different usage </span>
</p><ul>
	<li><span class=\"tx2\">NAS storage, fully back-up with snapshots for /home and /data </span></li>
	<li><span class=\"tx2\">Scratch storage, fast parallel filesystem </span></li>
	<li><span class=\"tx2\">Archive storage, to store large amounts of data for long time</span><span class=\"tx2\"></span></li>
</ul><p><span class=\"tx2\">The picture below gives a quick overview of the different components.</span><img style=\"width: 725px; height: 533px;\" src=\"/assets/1051\" alt=\"KU Leuven storage\" height=\"725\" width=\"533\">
</p><h2>Storage Types</h2><p>As described on <a target=\"_blank\" href=\"https://www.vscentrum.be/cluster-doc/access-data-transfer/where-store-data\">the web page \"Where can I store what kind of data?\"</a>  different types of data can be stored in different places. There is also an extra storage space for Archive use.
</p><h2>Archive Storage</h2><p>The archive tier is built with DDN WOS storage. It is intended to store data for longer term. The storage is optimized for capacity, not for speed. The storage by default is mirrored.
</p><p>No deletion rules are executed on this storage. The data will be kept until the user deletes it.
</p><p><strong>Use for</strong>: Storing data that will not be used for a longer period and which should be kept. Compute nodes have no direct access to that storage area and therefore it should not be used for jobs I/O operations.
</p><p><strong>How to request</strong>: Please send a request from the <a target=\"_blank\" href=\"https://admin.kuleuven.be/icts/onderzoek/hpc/hpc-storage\">storage request webpage</a>.
</p><p><strong>How much does it cost</strong>: For all the prices please refer to our <a target=\"_blank\" href=\"https://icts.kuleuven.be/sc/english/HPC\">service catalog (login required)</a>.<br>
</p><h2>Working with archive storage</h2><p>The archive storage should not be used to perform I/O in a compute job. Data should first be copied to the faster scratch filesystem. To accommodate user groups that have a large archive space, a staging area is foreseen. The staging area is a part of the same hardware platform as the fast scratch filesystem, but other rules apply. Data is not deleted automatically after 21 days. When the staging area is full it will be the user’s responsibility to make sure that enough space is available. Data created on scratch or in the staging location which needs to be kept for longer time should be copied to the archive.
</p><h2>Location of Archive/Staging</h2><p>The name of user's archive directory is in the format: /archive/leuven/arc_XXXXX, where XXXXX is a number and this will be given to the user by HPC admin once your archive requested is handled.
</p><p>The name of your staging directory is in this format: /staging/leuven/stg_XXXXX, where XXXXX is the same number as for the archive directory.
</p><h2>Use case: Data is in archive, how can I use it in a compute job?</h2><p>In this use case you want to start to compute on older data in your archive.
</p><p>If you want to compute with data in your archive stored in ‘archive_folder’. You can copy this data to your scratch using the following command:
</p><pre>rsync -a &lt;PATH_to_archive/archive_folder&gt; &lt;PATH_to_scratch&gt;
</pre><p>Afterwards you may want to archive the new produced results back to archive therefore you should follow the steps in the following use case.
</p><h2>Use case: Data produced on cluster, stored for longer time?</h2><p>  This procedure applies to the case when you have jobs producing output results on the scratch area and you want to archive those results in your archive area.
</p><p>  In that case you have a folder on scratch called ‘archive_folder’ in which you are working. And the same folder already exists in your archive space. Now you want to update your archive space with the new results produced on scratch
</p><p>You could run the command:
</p><pre>rsync -i -u -r --dry-run &lt;PATH_to_scratch/archive_folder&gt; &lt;PATH_to_archive/archive_folder&gt;
</pre><p>  This command will not perform the copy yet but it will give an overview of all data changed since last copy from archive. Therefore not all data needs to be copied back. If you agree with this overview you can run this command without the --dry-run’ option. If you are synching a large amount files, please contact HPC support for follow-up.
</p><h2>Use case : How to get local data on archive?</h2><p>Data that is stored at the user's local facilities can be copied to the archive through scp/bbcp/sftp methods. For this please refer to the appropriate VSC documentation:
</p><p><a href=\"https://www.vscentrum.be/client/linux/data-openssh\"></a>for linux: <a target=\"_blank\" href=\"https://www.vscentrum.be/client/linux/data-openssh\">openssh</a>
</p><p>for windows: <a target=\"_blank\" href=\"https://www.vscentrum.be/client/windows/filezilla\">filezilla</a> or <a target=\"_blank\" href=\"https://www.vscentrum.be/client/windows/winscp\">winscp</a>
</p><p>for OS X: <a target=\"_blank\" href=\"https://www.vscentrum.be/client/macosx/data-cyberduck\">data-cyberduck</a>.
</p><h2>Use case : How to check the disk usage?</h2><p>To check the occupied disk space additional option is necessary with du command:
</p><pre>du --apparent-size folder-name
</pre><h2>How to stage in or stage out using torque?</h2><p>Torque gives also the possibility to specify data staging as a job requirement. This way Torque will copy your data to scratch while your job is in the queue and will not start the job before all data is copied. The same mechanism is possible for stageout requirements. In the example below Torque will copy back your data from scratch when your job is finished to the archive storage tier:
</p><pre>qsub -W stagein=/scratch/leuven/3XX/vsc3XXXX@login1:/archive/leuven/arc_000XX/foldertostagein 
-W stageout=/scratch/leuven/3XX/vsc3XXXX/foldertostageout@login1:/archive/leuven/arc_000XX/
</pre><p><span class=\"tx2\"></span>
</p><p>
	Hostname is always one of the login nodes, because these are the only nodes where ‘archive’ is available on the cluster.
</p><p>For stagein the copy goes from /archive/leuven/arc_000XX/foldertostagein to /scratch/leuven/3XX/vsc3XXXX
</p><p>For stageout the copy goes from /scratch/leuven/3XX/vsc3XXXX/foldertostageout to /archive/leuven/arc_000XX/
</p><h2>Attached documents</h2><ul>
<li><a target=\"_blank\" href=\"https://www.vscentrum.be/assets/1059\">WOS storage quick start guide</a>
</li>
<li><a target=\"_blank\" href=\"https://www.vscentrum.be/assets/1027\">slides from storage info-session</a>
</li>
</ul>"
683,"","<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc nec interdum velit, et viverra arcu. Donec ac nisl vehicula orci mattis pellentesque vel sed magna. Ut vulputate ipsum in bibendum suscipit. Phasellus tristique molestie cursus. Suspendisse sed luctus diam. Duis dignissim tincidunt congue. Sed laoreet nunc ac hendrerit congue. Aenean semper dolor sit amet tincidunt pharetra. Fusce malesuada iaculis enim eu venenatis. Maecenas commodo laoreet eros eu feugiat. Integer dignissim sapien at vehicula fermentum. Sed quis odio in dui luctus tempus. Praesent porttitor nisl varius, mattis eros laoreet, eleifend magna. Curabitur vehicula vitae eros vel egestas. Fusce at metus velit.
</p><p>Test</p><h2>Test movie</h2><p>The movie below illustrates the use of supercomputing for the design of a cooling element from <a href=\"http://kanaalz.knack.be/nieuws/2-vlaamse-ingenieurs-ontketenen-revolutie-in-koeling/video-normal-864019.html\">a report on Kanaal Z</a>.
</p><p>Methode 1, conform de code voor embedding gegenereerd door de Kanaal Z website: speelt niet af...
</p><p>
	<iframe width=\"640\" height=\"360\" frameborder=\"0\" src=\"//kanaalz.knack.be/embed/video/864019\" scrolling=\"no\" allowfullscreen=\"yes\">
	</iframe>
</p><p>Methode 2: Video tag, werkt alleen in HTML5 browsers, en ik vrees dat Kanaal Z niet gelukkig is met deze methode...</p><p><video width=\"640\" height=\"360\" controls=\"controls\"></video></p><p><source src=\"http://flvpd.roularta.be/BBT/20170609_Z_DIABATIX_100512.mp4\"></p>"
687,"","<p><em><strong>The industry day has been postponed to a later date, probably in the autumn around the launch of the second Tier-1 system in Flanders.</strong></em></p><h2>Supercharge your business with supercomputing</h2><p>
	<strong>When?</strong> New date to be determined <br><strong>Where?</strong> <a href=\"https://www.technopolis.be/en/directions-and-contact/\">Technopolis, Mechelen</a><br><strong>Admission free, but registration required</strong>
</p><p>The VSC Industry day is the second in a series of annual events. The goals are to create awareness about the potential of HPC for industry and to help firms overcome the hurdles to use supercomputing. We are proud to present an exciting program with testimonials of some Flemish firms who already have discovered the opportunities of large scale computing, success stories from a European HPC centre that successfully collaborates with industry and a presentation by a HPC vendor who has been very successful delivering solutions to several industries.
</p><table border=\"0\" cellpadding=\"7\" cellspacing=\"0\" width=\"80%\">
<tbody>
<tr>
	<td colspan=\"3\" valign=\"TOP\" height=\"9\">
		<p><strong>Preliminary program - Supercharge your business with supercomputing</strong>
		</p><p><strong><em>Given that the industry day has been postponed, the program is subject to change.</em><br></strong></p>
	</td>
</tr>
<tr>
	<td colspan=\"1\" valign=\"TOP\" width=\"15%\" height=\"18\">
		<p>13.00-13.30
		</p>
	</td>
	<td colspan=\"2\" valign=\"TOP\" width=\"85%\" height=\"18\">
		<p>Registration and welcome drink
		</p>
	</td>
</tr>
<tr>
	<td valign=\"TOP\" width=\"15%\" height=\"18\">
		<p>13.30-13.45
		</p>
	</td>
	<td colspan=\"2\" valign=\"TOP\" width=\"40%\" height=\"18\">
		<p>Introduction and opening<br>
			<em>Prof. dr Colin Whitehouse (chair) </em>
		</p>
	</td>
</tr>
<tr>
	<td valign=\"TOP\" width=\"15%\" height=\"36\">
		<p>13.45-14.15
		</p>
	</td>
	<td colspan=\"2\" valign=\"TOP\" width=\"85%\" height=\"36\">
		<p>The future is now - physics-based simulation opens new gates in heart disease treatment<br>
			<em>Matthieu De Beule (<a href=\"http://www.feops.com/\">FEops</a>)</em>
		</p>
	</td>
</tr>
<tr>
	<td valign=\"TOP\" width=\"15%\" height=\"18\">
		<p>13.45-14.05
		</p>
	</td>
	<td colspan=\"2\" valign=\"TOP\" width=\"85%\" height=\"18\">
		<p>Hydrodynamic and morfologic modelling of the river Scheldt estuary<br>
			<em>Sven Smolders and Abdel Nnafie (<a href=\"http://www.waterbouwkundiglaboratorium.be/\">Waterbouwkundig Laboratorium</a>)</em>
		</p>
	</td>
</tr>
<tr>
	<td valign=\"TOP\" width=\"15%\" height=\"27\">
		<p>14.15-14.45
		</p>
	</td>
	<td colspan=\"2\" valign=\"TOP\" width=\"85%\" height=\"27\">
		<p>HPC in Metal Industry: Modelling Wire Manufacturing<br>
			<em>Peter De Jaeger (<a href=\"https://www.bekaert.com/\">Bekaert</a>)</em>
		</p>
	</td>
</tr>
<tr>
	<td colspan=\"1\" valign=\"TOP\" width=\"15%\" height=\"18\">
		<p>15.15-15.45
		</p>
	</td>
	<td colspan=\"2\" valign=\"TOP\" width=\"85%\" height=\"18\">
		<p>Coffee break
		</p>
	</td>
</tr>
<tr>
	<td valign=\"TOP\" width=\"15%\" height=\"18\">
		<p>15.45-16.15
		</p>
	</td>
	<td colspan=\"2\" valign=\"TOP\" width=\"85%\" height=\"18\">
		<p>NEC industrial customers HPC experiences <br><i>Fredrik Unger (<a href=\"https://de.nec.com/de_DE/global/solutions/hpc/index.html\">NEC</a>)</i></p>
	</td>
</tr>
<tr>
	<td valign=\"TOP\" width=\"15%\" height=\"27\">
		<p>16.15-16.45
		</p>
	</td>
	<td colspan=\"2\" valign=\"TOP\" width=\"85%\" height=\"27\">
		<p>Exploiting business potential with supercomputing<br>
			<em>Karen Padmore (HPC Wales and <a href=\"https://sesamenet.eu/\">SESAME</a> repres.) </em>
		</p>
	</td>
</tr>
<tr>
	<td valign=\"TOP\" width=\"15%\" height=\"27\">
		<p>16.45-17.05
		</p>
	</td>
	<td colspan=\"2\" valign=\"TOP\" width=\"85%\" height=\"27\">
		<p>What VSC has to offer to your business<br>
			<em>Ingrid Barcena Roig and Ewald Pauwels (VSC)</em></p>
	</td>
</tr>
<tr>
	<td valign=\"TOP\" width=\"15%\" height=\"18\">
		<p>17.05-17.25
		</p>
	</td>
	<td colspan=\"2\" valign=\"TOP\" width=\"85%\" height=\"18\">
		<p>Q&A discussion<br>
			Panel/chair
		</p>
	</td>
</tr>
<tr>
	<td valign=\"TOP\" width=\"15%\" height=\"18\">17.25-17.30
	</td>
	<td colspan=\"2\" valign=\"TOP\" width=\"85%\" height=\"18\">
		<p>Closing<br>
			<em>Prof dr. Colin Whitehouse (chair)</em>
		</p>
	</td>
</tr>
<tr>
	<td valign=\"TOP\" width=\"15%\" height=\"18\">17.30-18.30
	</td>
	<td colspan=\"2\" valign=\"TOP\" width=\"85%\" height=\"18\">Networking reception
	</td>
</tr>
</tbody>
</table><h2>Registration</h2><p>The registrations are closed now. Ones the new date is determined, a new registration form will be made available.</p><p><a href=\"https://www.technopolis.be/en/directions-and-contact/\">How to reach Technopolis</a>.<br></p>"
689,"","<p><a href=\"/events/industryday-2016\">VSC Industry Day - Thursday April 14, 2016</a></p>"
691,"","<p><a href=\"/events/industryday-2016\">VSC Industry Day - Thursday April 14, 2016</a></p>"
695,"","<h2>A batch system</h2><p>There are two important differences between a supercomputer and your personal laptop or smartphone apart from the amount of compute power it can deliver if used properly: As it is a large and expensive machine and as not every program can use all of its processing power, it is a multi-user machine, and furthermore it is optimised to run large parallel programs in such a way that they don't interfere too much with each other. So your compute resources will be as much as possible isolated from those assigned to another user. The latter is necessary to ensure fast and predictable execution of large parallel jobs as the performance of a parallel application will always be limited by the slowest node, process or thread. </p><p>This has some important consequences:
</p><ol>
	<li>As a user, you don't get the whole machine, but a specific part of it, and so you'll have to specify which part you need for how long.</li>
	<li>Often more capacity is requested than available at that time. Hence you may have to wait a little before you get the resources that you request. To organise this in a proper way, every supercomputer provides a queueing system. </li>
	<li>Moreover, as you often have to wait a bit before you get the requested resources, it is not well suited for interactive work. Instead, most work on a supercomputer is done in <strong>batch mode</strong>: Programs run without user interaction, reading their input from file and storing their results in files.</li>
</ol><p>In fact, another reason why interactive work is discouraged on most clusters is because interactive programs rarely fully utilise the available processors but waste a lot of time waiting for new user input. Since that time cannot be used by another user either (remember that your work is isolated from that from other users), is is a waste of very expensive compute resources.
</p><p>A <strong>job</strong> is an entity of work that you want to do on a supercomputer. A job consists of the execution of one or more programs and needs certain resources for some time to be able to execute. <strong>Batch jobs</strong> are described by a <strong>job script</strong>. This is like a regular linux shell script (usually for the bash shell), but it usually contains some extra information: a description of the resources that are needed for the job. A job is then submitted to the cluster and placed in a queue (managed by a piece of software called the <strong>queue manager</strong>). A <strong>scheduler</strong> will decide on the priority of the job that you submitted (based on the resources that you request, your past history and policies determined by the system managers of the cluster). It will use the <strong>resource manager</strong> to check which resources are available and to start the job on the cluster when suitable resources are available and the scheduler decides it is the job's time to run.
</p><p>At the VSC we use two software packages to perform these tasks. <strong>Torque</strong> is an open source package that performs the role of queue and resource manager. <strong>Moab</strong> is a commercial package that provides way more scheduling features than its open source alternatives. Though both packages are developed by the same company and are designed to work well with each other, they both have their own set of commands with often confusing command line options.
</p><h2>Anatomy of a job script</h2><p>A typical job script looks like:
</p><pre>#!/bin/bash
#PBS –l nodes=1:ppn=20
#PBS –l walltime=1:00:00
#PBS -o stdout.$PBS_JOBID
#PBS -e stderr.$PBS_JOBID

module load MATLAB
cd $PBS_O_WORKDIR

matlab -r fibo
</pre><p>We can distinguish 4 sections in the script:
</p><ol>
	<li>The first line simply tells that this is a shell script.</li>
	<li>The second block, the lines that start with <code>#PBS</code>, specify the resources and tell the resource manager where to store the standard output and standard error from the program. To ensure unique file names, the author of this script has chosen to put the \"Job ID\", a unique ID for every job, in the name.</li>
	<li>The next two lines create the proper environment to run the job: it loads a module and changes the working directory to the directory from which the job was submitted (this is what is stored in the environment variable<code>$PBS_O_WORKDIR</code>).</li>
	<li>Finally the script executes the commands that are the core of the job. In this simple example, this is just a single command, but it could as well be a whole bash script.</li>
</ol><p>In other pages of the documentation in this section, we'll go into more detail on specifying resource requirements, output redirection and notifications and on environment variables that are set by the scheduler and can be used in your job.
</p><p>Assuming that this script is called myscript.pbs, the job can then be submitted to the queueing system with the command <code>qsub myscript.pbs</code>.
</p><p>Note that if you use a system at the KU Leuven, including the Tier-1 system BrENIAC, you need credits. When submitting your job, you also need to tell <code>qsub</code> which credits to use. We refer to the page on \"<a href=\"/cluster-doc/running-jobs/credit-system-basics\">Credit system basics</a>\".</p><h2>Structure of this documentation section</h2><ul>
	<li>The page on <a href=\"cluster-doc/running-jobs/specifying-requirements\">specifying job requirements</a> describes everything that goes in the second block of your job script: the specification of the resources, notifications, etc.</li><li>The page on <a href=\"/cluster-doc/running-jobs/starting-programs-in-job\">starting programs in your job</a> describes the third and fourth block: Setting up the environment and starting a program.</li>
	<li>The page on <a href=\"/cluster-doc/running-jobs/submitting-managing-jobs\">starting and managing jobs</a> describes the main Torque and Moab commands to submit and then manage your jobs and to follow up how they proceed trough the scheduling software.</li>
	<li>The <a href=\"/cluster-doc/running-jobs/worker-framework\">worker framework</a> is a framework developed at the VSC to bundle a lot of small but related jobs into a larger parallel job. This makes life a lot easier for the scheduler as the scheduler is optimised to run a limited number of large long-duration jobs as efficient as possible and not to deal with thousands or millions of small short jobs.</li>
</ul><h2>Some background information</h2><p><em>For those readers who want some historical background to understand where the complexity comes from.</em>
</p><p>In the ’90s of the previous century, there was a popular resource manager called Portable Batch System, developed by a contractor for NASA. This was open-sourced. But that contractor was acquired by another company that then sold the rights to Altair Engineering that evolved the product into the closed-source product PBSpro (which was then open-sourced again in the summer of 2016). The open-source version was forked by another company that is now known as Adaptive Computing and renamed to Torque. Torque remained open–source. The name stands for Terascale Open-source Resource and QUEue manager. Even though the name was changed, the commands remained which explains why so many commands still have the abbreviation PBS in their name.
</p><p>The scheduler Moab evolved from MAUI, an open-source scheduler. Adaptive Computing, the company behind Torque and Moab, contributed a lot to MAUI but then decided to start over with a closed source product. They still offer MAUI on their website though. MAUI used to be widely used in large USA supercomputer centres, but most now throw their weight behind SLURM with or without another scheduler.
</p>"
697,"","<p>In general, there are two ways to pass the resource requirements or other job properties to the queue manager:
</p><ol>
	<li>They can be specified on the command line of the <code>qsub</code> command</li>
	<li>Or they can be put in the job script on lines that start with <code>#PBS</code> (so-called in-PBS directives). Each line can contain one or more command line options written in exactly the same way as on the command line of qsub. These lines <em><strong>have to</strong></em> come at the top of the job script, before any command (but after the line telling the shell that this is a bash script).</li>
</ol><p>And of course both strategies can be mixed at will: Some options can be put in the job script, while others are specified on the command line. This can be very useful, e.g., if you run a number of related jobs from different directories using the same script. The few things that have to change can then be specified at the command line. The options given at the command line always overrule those in the job script in case of conflict.
</p><h2>Resource specifications</h2><p>Resources are specified using the -l command line argument.
</p><h3>Wall time</h3><p>Walltime is specified through the option <code>-l walltime=HH:MM:SS</code>  with <code>HH:MM:SS</code> the walltime that you expect to need for the job. (The format <code>DD:HH:MM:SS</code> can also be used when the waltime exceeds 1 day, and <code>MM:SS</code> or simply <code>SS</code> are also viable options for very short jobs).
</p><p>To specify a run time of 30 hours, 25 minutes and 5 seconds, you'd use
</p><pre>$ qsub -l walltime=30:25:05 myjob.pbs<br>
</pre><p>on the command line or the line
</p><pre>#PBS -l walltime=30:25:05<br>
</pre><p>in the job script (or alternative <code>walltime=1:06:25:05</code>).
</p><p>If you omit this option, the queue manager will not complain but use a default value (one hour on most clusters).
</p><p>It is important that you do an effort to estimate the wall clock time that your job will need properly. If your job exceeds the specified wall time, it will be killed, but this is not an invitation to simply specify the longest wall time possible (the limit differs from cluster to cluster). To make sure that the cluster cannot be monopolized by one or a few users, many of our clusters have a stricter limit on the number of long-running jobs than on the number of jobs with a shorter wall time. And several clusters will also allow short jobs to pass longer jobs in the queue if the scheduler finds a gap (based on the estimated end time of the running jobs) that is short enough to run that job before it has enough resources to start a large higher-priority parallel job. This process is called backfilling.
</p><p>The maximum allowed wall time for a job is cluster-dependent. Since these policies can change over time (as do other properties from clusters), we bundle these on one page per cluster in the \"<a href=\"/infrastructure/hardware\">Available hardware</a>\" section.
</p><h3>Single- and multi-node jobs: Cores and memory<br></h3><p>The following options can be used to specify the number of cores, amount of RAM and virtual memory needed for the job:
</p><ul>
	<li><code>-l nodes=&lt;nodenum&gt;:ppn=&lt;cores per node&gt;</code>: This indicates that the node needs &lt;nodenum&gt; jobs with &lt;cores per node&gt; virtual cores per node. Depending on the settings for the particular system, this will be physical cores or hyperthreads on a physical core. </li>
	<li><code>-l pmem=&lt;memory&gt;</code>: The job needs &lt;memory&gt; RAM memory per core or hyperthread (the unit used by ppn). Thee units kb, mb, gb or tb can be used (though the latter does not make sense when talking about memory per core). Users are strongly advised to use this parameter also. If not specified, the system will use a default value, and that may be too small for your job and cause trouble if the scheduler puts multiple jobs on a single node. Moreover, recent versions of the resource manager software in use at the VSC can check for the actual use of resources in a more strict way, so when this is enabled, they may just terminate your job if it uses too much memory.</li>
	<li><code>-l pvmem=&lt;memory&gt;</code>: The job needs &lt;memory&gt; virtual memory per core or hyperthread (the unit used by ppn). This determines the total amount of RAM memory + swap space that can be used on any node. Similarly, kb, mb, gb or tb can be used, with gb making most sense. Note that on many clusters, there is not much swap space available. Moreover, swapping should be avoided as it causes a dramatic performance loss. Hence this option is not very useful in most cases.</li>
</ul><p>Node that specifying <code>-l nodes=&lt;nodenum&gt;:ppn=&lt;cores per node&gt;</code> does not guarantee you that you actually get &lt;nodenum&gt; physical nodes. You may get multiple groups of &lt;cores per node&gt; cores on a single node instead. E.g., -l nodes=4:ppn=5 may result in an allocation of 20 cores on a single node in a cluster that has nodes with 20 or more cores if that node also contains enough memory.
</p><p>Note also that the job script will only run once on the first node of your allocation. To start processes on the other nodes, you'll need to use tools like <code>pbsdsh</code> or <code>mpirun</code>/<code>mpiexec</code> to start those processes.
</p><h3>Single node jobs only: Cores and memory</h3><p>For single node jobs there is an alternative for specifying the amount of resident memory and virtual memory needed for the application. These settings make more sense from the point of view of starting a single multi-threaded application.
</p><ul>
	<li><code>-l nodes=1:ppn=&lt;cores per node&gt;</code>: This is still needed to specify the number of physical cores or hyperthreads needed for the job.</li>
	<li><code>-l mem=&lt;memory&gt;</code>: The job needs &lt;memory&gt; RAM memory<span class=\"redactor-invisible-space\"> on the node. Units are kb, mb, gb or tb as before.<br></span></li>
	<li><span class=\"redactor-invisible-space\"><code>-l vmem=&lt;memory&gt;</code>: The job needs &lt;memory&gt; virtual memory <span class=\"redactor-invisible-space\">on the node, i.e., RAM and swap space combined. As for the option <code>pvmem</code> above, this options is not useful on most clusters since the amount of swap space is very low and since swapping causes a very severe performance degradation.<br></span></span></li>
</ul><p>These options should not be used for multi-node jobs as the meaning of the parameter is undefined (<code>mem</code>) or badly defined (<code>vmem</code>) for multi-node jobs with different sections and different versions of the Torque manual specifying different behaviour for these options.
</p><h3>Specifying further node properties</h3><p>Several clusters at the VSC have nodes with different properties. E.g., a cluster may have nodes of two different CPU generations and your program may be compiled to take advantage of new instructions on the newer generation and hence not run on the older generation. Or some nodes may have more physical memory or a larger hard disk and support more virtual memory.  Or not all nodes may be connected to the same high speed interconnect (which is mostly an issue on the older clusters). You can then specify which node type you want by adding further properties to the <code>-l nodes=</code> specification.  E.g., assume a cluster with both Ivy Bridge and Haswell generation nodes. The Haswell CPU supports new and useful floating point instructions, but programs that use these will not run on the older Ivy Bridge nodes. The cluster will then specify the property ivybridge for the Ivy Bridge nodes and haswell for the Haswell nodes. Specifying <code>-l nodes=8:ppn=6:haswell</code> then tells the scheduler that you want to use nodes with the haswell property only (and in this case, since Haswell nodes often have 24 cores, you will likely get 2 physical nodes).
</p><p>The exact list of properties depend on the cluster and is given in the page for your cluster in the \"<a href=\"/infrastructure/hardware\">Available hardware</a>\" section of this manual. Note that even for a given cluster, this list may evolve over time, e.g., when new nodes are added to the cluster, so check these pages again from time to time!
</p><h3>Combining resource specifications</h3><p>It is possible to combine multiple <code>-l</code> options in a single one by separating the arguments with a colon (,). E.g., the block
</p><pre>#PBS -l walltime=2:30:00
#PBS -l nodes=2:ppn=16:sandybridge
#PBS -l pmem=2gb
</pre><p>is equivalent with the line
</p><pre>#PBS -l walltime=2:30:00,nodes=2:ppn=16:sandybridge,pmem=2gb
</pre><p>The same holds when using <code>-l</code> at the command line of <code>qsub</code>.
</p><h3>Enforcing the node specification</h3><p><em>These are very asocial options as they typically result in lots of resources remaining unused, so use them with care and talk to user support to see if you really need them. But there are some rare scenarios in which they are actually useful.</em>
</p><p>If you don't use all cores of a node in your job, the scheduler may decide to bundle the tasks of several nodes in your resource request on a single node, may put other jobs you have in the queue on the same node(s) or may - depending on how the system manager has configured the scheduler - put jobs of other users on the same node. In fact, most VSC clusters have a single user per node policy as misbehaving jobs of one user may cause a crash or performance degradation of another user's job.
</p><ul>
	<li>Using <code>-W x=nmatchpolicy:exactnode</code> will result in the scheduler giving you resourced on the exact number of nodes you request. However, other jobs may still be scheduled on the same nodes if not all cores are used.</li>
	<li>Using <code>-l naccesspolicy=singlejob</code> will make sure that no other job can use the nodes allocated to your job. In most cases it is very asocial to claim a whole node for a job that cannot fully utilise the resources on the node, but there are some rare cases when your program actually runs so much faster by leaving some resources unused that it actually improves the performance of the cluster. But these cases are very rare, so you shouldn't use this option unless, e.g., you are running the final benchmarks for a paper and want to exclude as much factors that can influence the results as possible.</li>
</ul><h2>Naming jobs and output files</h2><p>The default name of a job is derived from the file name of the job script. This is not very useful if the same job script is used to launch multiple jobs, e.g., by launching jobs from multiple directories with different input files. It is possible to overwrite the default name of the job with <code>-N &lt;job_name&gt;</code>.
</p><p>Most jobs on a cluster run in batch mode. This implies that they are not connected to a terminal, so the output send to the Linux stdout (standard output) and stderr (standard error) devices cannot be displayed on screen. Instead it is captured in two files that are put in the directory where your job was started at the end of your job. The default names of those files are &lt;job_name&gt;.o&lt;job id&gt; and &lt;job_name&gt;.e&lt;job id&gt; respectively, so made from the name of the job (the one assigned with -N if any, or the default one) and the number of the job assigned when you submit the job to the queue. You can however change those names using <code>-o &lt;output file&gt;</code> and <code>-e &lt;error file&gt;</code>.
</p><p>It is also possible to merge both output streams in a single output stream. The option <code>-j oe</code> will merge stderr into stdout (and hence the -e option does not make sense), the option <code>-j eo</code> will merge stdout into stderr.</p><h2>Notification of job events</h2><p>Our scheduling system can also notify you when a job starts or ends by e-mail. Jobs can stay queued for hours or sometimes even days before actually starting, so it is useful to be notified so that you can monitor the progress of your job while it runs or kill it when it misbehaves or produces clearly wrong results. Two command line options are involved in this process:
</p><ul>
	<li><code>-m abe</code> or any subset of these three letters determine for which event you'll receive a mail notification: job start (b), job ends (e) or job is aborted (a). In some scenarios tis may bombard you with e-mail if you have a lot of jobs starting, however at other times it is very useful to be notified that your job starts, e.g., to monitor if it is running properly and efficiently.</li>
	<li>With <code>-M &lt;mailadress&gt;</code>  you can set the mail address to which the notification will be send. On most clusters the default will be the e-mail address with which you registered your VSC-account, but on some clusters this fails and the option is required to receive the e-mail.</li>
</ul><h2>Other options</h2><p>This page describes the most used options in their most common use cases. There are however more parameters for resource specification and other options that can be used. For advanced users who want to know more, we refer to the documentation of the <code>qsub</code> command that mentions all options in the Torque manual on the <a href=\"http://www.adaptivecomputing.com/support/documentation-index/\" target=\"_blank\">Adaptive Computing documentation web site</a>.
</p><ul>
	<li><a href=\"http://docs.adaptivecomputing.com/torque/6-0-1/help.htm\" target=\"_blank\">Torque 6.0.1</a> (Antwerp clusters, Hydra and BrENIAC)</li>
	<li><a href=\"http://docs.adaptivecomputing.com/torque/5-1-2/help.htm\" target=\"_blank\">Torque 5.1.X</a> (Thinking, muk)</li>
</ul>"
699,"","<p>
	To set up your environment for using a particular (set of) software package(s), you can use the modules that are provided centrally.<br>
	On the Tier-2 of UGent and VUB, interacting with the modules is done via
	<a rel=\"nofollow\" href=\"http://lmod.readthedocs.io/en/latest/\">Lmod</a> (since August 2016),
using the 
	<code>module</code> command or the handy shortcut command <code>ml</code>.
</p><h2>Quick introduction</h2><p>A very quick introduction to Lmod. Below you will find more details and examples.
</p><ul>
	<li><code>ml</code> lists the currently loaded modules, and is equivalent with <code>module list</code></li>
	<li><code>ml GCC/4.9.3</code> loads the <code>GCC/4.9.3</code> module, and is equivalent with <code>module load GCC/4.9.3</code></li>
	<li><code>ml -GCC</code> unloads the currently loaded <code>GCC</code> module, and is equivalent with <code>module unload GCC</code></li>
	<li><code>ml av gcc</code> prints the currently available modules that match <i>gcc</i> (case-insensitively), and is equivalent with <code>module avail GCC</code></li>
	<li><code>ml show GCC/4.9.3</code> prints more information about the <code>GCC/4.9.3</code> module, and is equivalent with <code>module show GCC</code></li>
	<li><code>ml spider gcc</code> searches (case-insensitive) for <i>gcc</i> in all available modules over all clusters</li>
	<li><code>ml spider GCC/4.9.3</code> show all information about the module <code>GCC/4.9.3</code> and on which clusters it can be loaded.</li>
	<li><code>ml save mycollection</code> stores the currently loaded modules to a collection</li>
	<li><code>ml restore mycollection</code> restores a previously stored collection of modules</li>
</ul><h2><a name=\"commands\"></a>Module commands: using <code>module</code> (or <code>ml</code>)</h2><hr><h3>Listing loaded modules: <code>module list</code> (or <code>ml</code>)</h3><p>To get an overview of the currently loaded modules, use <code>module list</code> or <code>ml</code> (without specifying extra arguments).
</p><p>In a default environment, you should see a single <code>cluster</code> module loaded:
</p><pre>$ ml
Currently Loaded Modules:
  1) cluster/delcatty (S)
  Where:
   S:  Module is Sticky, requires --force to unload or purge
</pre><p>(for more details on sticky modules, see the section on <code>ml purge</code>)
</p><hr><h3>Searching for available modules: module avail (or ml av) and ml spider</h3><h4>Printing all available modules: module avail (or ml av)</h4><p>To get an overview of all available modules, you can use <code>module avail</code> or simply <code>ml av</code>:
</p><pre>$ ml av
----------------------------- /apps/gent/CO7/haswell-ib/modules/all -----------------------------
   ABAQUS/6.12.1-linux-x86_64           libXext/1.3.3-intel-2016a                  (D)
   ABAQUS/6.14.1-linux-x86_64    (D)    libXfixes/5.0.1-gimkl-2.11.5
   ADF/2014.02                          libXfixes/5.0.1-intel-2015a
   ...                                  ...
</pre><p>In the current module naming scheme, each module name consists of two parts:
</p><ul>
	<li> the part before the first <code>/</code>, corresponding to the <i>software name</i>; and</li>
	<li> the remainder, corresponding to the software <i>version</i>, the <i>compiler toolchain</i> that was used to install the software, and a possible <i>version suffix</i></li>
</ul><p>For example, the module name <code>matplotlib/1.5.1-intel-2016a-Python-2.7.11</code> will set up the environment for using <i>matplotlib</i> version <i>1.5.1</i>,
which was installed using the 
	<code>intel/2016a</code> compiler toolchain; the version suffix <code>-Python-2.7.11</code> indicates it was installed for Python version 2.7.11.
</p><p>The <code>(D)</code> indicates that this particular version of the module is the default,
but we strongly recommend to 
	<i>not</i> rely on this as the default can change at any point.
Usuall, the default will point to the latest version available.
</p><hr><h4>Searching for modules: ml spider</h4><p>The (Lmod-specific) <code>spider</code> subcommand lets you search for modules across all clusters.
</p><p>If you just provide a software name, for example <code>gcc</code>, it prints on overview of all available modules
for GCC.
</p><pre>$ ml spider gcc
---------------------------------------------------------------------------------
  GCC:
---------------------------------------------------------------------------------
     Versions:
        GCC/4.7.2
        GCC/4.8.1
        GCC/4.8.2
        GCC/4.8.3
        GCC/4.9.1
        GCC/4.9.2
        GCC/4.9.3-binutils-2.25
        GCC/4.9.3
        GCC/4.9.3-2.25
        GCC/5.3.0
     Other possible modules matches:
        GCCcore
---------------------------------------------------------------------------------
  To find other possible module matches do:
      module -r spider '.*GCC.*'
---------------------------------------------------------------------------------
  For detailed information about a specific \"GCC\" module (including how to load the modules) use the module's full name.
  For example:
     $ module spider GCC/4.9.3
---------------------------------------------------------------------------------
</pre><p>Note that <code>spider</code> is case-insensitive.
</p><p>If you use <code>spider</code> on a full module name like GCC/4.9.3-2.25 it will tell on which cluster(s) that module available:
</p><pre>$ ml spider GCC/4.9.3-2.25
---------------------------------------------------------------------------------
  GCC: GCC/4.9.3-2.25
---------------------------------------------------------------------------------
     Other possible modules matches:
        GCCcore
    You will need to load all module(s) on any one of the lines below before the \"GCC/4.9.3-2.25\" module
    is available to load.
      cluster/delcatty
      cluster/golett
      cluster/phanpy
      cluster/raichu
      cluster/swalot
    Help:
       The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada,
       as well as libraries for these languages (libstdc++, libgcj,...). - Homepage: http://gcc.gnu.org/
---------------------------------------------------------------------------------
  To find other possible module matches do:
      module -r spider '.*GCC/4.9.3-2.25.*'
</pre><p>This tells you that the module named <code>GCC/4.9.3-2.25</code> is available on the clusters <code>delcatty</code>, <code>golett</code>, <code>phanpy</code>, <code>raichu</code> and <code>swalot</code>.
It also tells you what the module contains and a URL to the homepage of the software.
</p><hr><h4>Available modules for a particular software package: module avail &lt;name&gt; (or ml av &lt;name&gt;)</h4><p>To check which modules are available for a particular software package, you can provide the software name to <code>ml av</code>.
</p><p>For example, to check which versions of IPython are available:
</p><pre>$ ml av ipython
----------------------------- /apps/gent/CO7/haswell-ib/modules/all -----------------------------
IPython/3.2.3-intel-2015b-Python-2.7.10    IPython/3.2.3-intel-2016a-Python-2.7.11 (D)
</pre><p>Note that the specified software name is treated case-insensitively.
</p><p>Lmod does a <i>partial</i> match on the module name, so sometimes you need to use <code>/</code> to indicate the end of the software name you are interested in:
</p><pre>$ ml av GCC/
----------------------------- /apps/gent/CO7/haswell-ib/modules/all -----------------------------
GCC/4.9.2    GCC/4.9.3-binutils-2.25    GCC/4.9.3    GCC/4.9.3-2.25    GCC/5.3.0    GCC/6.1.0-2.25 (D)
</pre><hr><h3>Inspecting a module using module show (or ml show)</h3><p>To see how a module would change the environment, use <code>module show</code> or <code>ml show</code>:
</p><pre>$ ml show matplotlib/1.5.1-intel-2016a-Python-2.7.11
----------------------------- /apps/gent/CO7/haswell-ib/modules/all -----------------------------
whatis(\"Description: matplotlib is a python 2D plotting library which produces publication quality figures in a variety of 
hardcopy formats and interactive environments across platforms. matplotlib can be used in python scripts, the python 
and ipython shell, web application servers, and six graphical user interface toolkits. - Homepage: http://matplotlib.org \")
conflict(\"matplotlib\")
load(\"intel/2016a\")
load(\"Python/2.7.11-intel-2016a\")
load(\"freetype/2.6.2-intel-2016a\")
load(\"libpng/1.6.21-intel-2016a\")
prepend_path(\"LD_LIBRARY_PATH\",\"/apps/gent/CO7/haswell-ib/software/matplotlib/1.5.1-intel-2016a-Python-2.7.11/lib\")
prepend_path(\"LIBRARY_PATH\",\"/apps/gent/CO7/haswell-ib/software/matplotlib/1.5.1-intel-2016a-Python-2.7.11/lib\")
setenv(\"EBROOTMATPLOTLIB\",\"/apps/gent/CO7/haswell-ib/software/matplotlib/1.5.1-intel-2016a-Python-2.7.11\")
setenv(\"EBVERSIONMATPLOTLIB\",\"1.5.1\")
setenv(\"EBDEVELMATPLOTLIB\",\"/apps/gent/CO7/haswell-ib/software/matplotlib/1.5.1-intel-2016a-Python-2.7.11/easybuild/matplotlib-1.5.1-intel-2016a-Python-2.7.11-easybuild-devel\")
prepend_path(\"PYTHONPATH\",\"/apps/gent/CO7/haswell-ib/software/matplotlib/1.5.1-intel-2016a-Python-2.7.11/lib/python2.7/site-packages\")
setenv(\"EBEXTSLISTMATPLOTLIB\",\"Cycler-0.9.0,matplotlib-1.5.1\")
help([[ matplotlib is a python 2D plotting library which produces publication quality figures in a variety of
 hardcopy formats and interactive environments across platforms. matplotlib can be used in python scripts, the python
 and ipython shell, web application servers, and six graphical user interface toolkits. - Homepage: http://matplotlib.org
</pre><p>Note that both the direct changes to the environment as well as other modules that will be loaded are shown.
</p><p>If you're not sure what all of this means: don't worry, you don't have to know; just load the module and try
using the software.
</p><hr><h3>Loading modules: module load &lt;modname(s)&gt; (or ml &lt;modname(s)&gt;)</h3><p>To effectively apply the changes to the environment that are specified by a module, use <code>module load</code> or <code>ml</code> and specify the name of the module.
</p><p>For example, to set up your environment to use matplotlib:
</p><pre>$ ml matplotlib/1.5.1-intel-2016a-Python-2.7.11
$ ml
Currently Loaded Modules:
  1) cluster/delcatty                                    (S)  12) zlib/1.2.8-intel-2016a
  2) GCCcore/4.9.3                                          13) libreadline/6.3-intel-2016a
  3) binutils/2.25-GCCcore-4.9.3                            14) ncurses/6.0-intel-2016a
  4) icc/2016.1.150-GCC-4.9.3-2.25                          15) Tcl/8.6.4-intel-2016a
  5) ifort/2016.1.150-GCC-4.9.3-2.25                        16) SQLite/3.9.2-intel-2016a
  6) iccifort/2016.1.150-GCC-4.9.3-2.25                     17) Tk/8.6.4-intel-2016a-no-X11
  7) impi/5.1.2.150-iccifort-2016.1.150-GCC-4.9.3-2.25      18) GMP/6.1.0-intel-2016a
  8) iimpi/8.1.5-GCC-4.9.3-2.25                             19) Python/2.7.11-intel-2016a
  9) imkl/11.3.1.150-iimpi-8.1.5-GCC-4.9.3-2.25             20) freetype/2.6.2-intel-2016a
 10) intel/2016a                                            21) libpng/1.6.21-intel-2016a
 11) bzip2/1.0.6-intel-2016a                                22) matplotlib/1.5.1-intel-2016a-Python-2.7.11
</pre><p>Note that even though we only loaded a single module, the output of <code>ml</code> shows that a whole bunch of modules were loaded, which are required dependencies for <i>matplotlib</i>,
including both the 
	<i>compiler toolchain</i> that was used to install <i>matplotlib</i> (i.e. <code>intel/2016a</code>, and its dependencies) and the module providing the <i>Python</i> installation
for which 
	<i>matplotlib</i> was installed (i.e. <code>Python/2.7.11-intel-2016a</code>).
</p><hr><h4>Conflicting modules</h4><p>It is important to note that <strong>only modules that are compatible with each other can be loaded together. In particular, modules must be installed either with the same toolchain as the modules that</strong> are already loaded, or with a compatible (sub)toolchain.
</p><p>For example, once you have loaded one or more modules that were installed with the <code>intel/2016a</code> toolchain, all other modules that you load should have been installed with the same toolchain.
</p><p>In addition, only <strong>one single version</strong> of each software package can be loaded at a particular time. For example, once you have the <code>Python/2.7.11-intel-2016a</code>
	module loaded,
you can not load a different version of Python in the same session/job 
script; neither directly, nor indirectly as a dependency of another 
module you want to load.
</p><p>See also <a href=\"#module_conflicts\">the topic \"module conflicts\" in the list of key differences with the previously used module system</a>.
</p><hr><h3>Unloading modules: module unload &lt;modname(s)&gt; (or ml -&lt;modname(s)&gt;)</h3><p>To revert the changes to the environment that were made by a particular module, you can use <code>module unload</code> or <code>ml -&lt;modname&gt;</code>.
</p><p>For example:
</p><pre>$ ml
Currently Loaded Modules:
  1) cluster/golett (S)
$ which gcc
/usr/bin/gcc
$ ml GCC/4.9.3
$ ml
Currently Loaded Modules:
  1) cluster/golett (S)   2) GCC/4.9.3
$ which gcc
/apps/gent/CO7/haswell-ib/software/GCC/4.9.3/bin/gcc
$ ml -GCC/4.9.3
$ ml
Currently Loaded Modules:
  1) cluster/golett (S)
$ which gcc
/usr/bin/gcc
</pre><hr><h3>Resetting by unloading all modules: ml purge (module purge)</h3><p>To reset your environment back to a clean state, you can use <code>module purge</code> or <code>ml purge</code>:
</p><pre>$ ml
Currently Loaded Modules:
  1) cluster/delcatty                                    (S)  11) bzip2/1.0.6-intel-2016a
  2) GCCcore/4.9.3                                          12) zlib/1.2.8-intel-2016a
  3) binutils/2.25-GCCcore-4.9.3                            13) libreadline/6.3-intel-2016a
  4) icc/2016.1.150-GCC-4.9.3-2.25                          14) ncurses/6.0-intel-2016a
  5) ifort/2016.1.150-GCC-4.9.3-2.25                        15) Tcl/8.6.4-intel-2016a
  6) iccifort/2016.1.150-GCC-4.9.3-2.25                     16) SQLite/3.9.2-intel-2016a
  7) impi/5.1.2.150-iccifort-2016.1.150-GCC-4.9.3-2.25      17) Tk/8.6.4-intel-2016a-no-X11
  8) iimpi/8.1.5-GCC-4.9.3-2.25                             18) GMP/6.1.0-intel-2016a
  9) imkl/11.3.1.150-iimpi-8.1.5-GCC-4.9.3-2.25             19) Python/2.7.11-intel-2016a
 10) intel/2016a
$ ml purge
The following modules were not unloaded:
   (Use \"module --force purge\" to unload all):
  1) cluster/delcatty
[15:21:20] vsc40023@node2626:~ $ ml
Currently Loaded Modules:
  1) cluster/delcatty (S)
</pre><p>Note that, on HPC-UGent, the <code>cluster</code> module will always remain loaded, since
 it defines some important environment variables that point to the 
location of centrally installed software/modules,
and others that are required for submitting jobs and interfacing with 
the cluster resource manager (
	<code>qsub</code>, <code>qstat</code>, ...).
</p><p>As such, you should <strong>not</strong> (re)load the <code>cluster</code> module anymore after running <code>ml purge</code>.
See also 
	<a href=\"#module_load_cluster\">the topic on the purge command in the list of key differences with the previously used module implementation</a>.
</p><hr><h3>Module collections: ml save, ml restore</h3><p>If you have a set of modules that you need to load often, you can save these in a <i>collection</i> (only works with Lmod).
</p><p>First, load all the modules you need, for example:
</p><pre>ml HDF5/1.8.16-intel-2016a GSL/2.1-intel-2016a Python/2.7.11-intel-2016a
</pre><p>Now store them in a collection using <code>ml save</code>:
</p><pre>$ ml save my-collection
</pre><p>Later, for example in a job script, you can reload all these modules with <code>ml restore</code>:
</p><pre>$ ml restore my-collection
</pre><p>With <code>ml savelist</code> you can get a list of all saved collections:
</p><pre>$ ml savelist
Named collection list:
  1) my-collection
  2) my-other-collection
</pre><p>To inspect a collection, use <code>ml describe</code>.
</p><p>To remove a module collection, remove the corresponding entry in <code>$HOME/.lmod.d</code>.
</p><hr><p><span id=\"lmod_vs_tmod\"></span>
</p><h2><a name=\"Advantages-differences\"></a>Lmod vs Tcl-based environment modules</h2><p>In August 2016, we switched to <a rel=\"nofollow\" href=\"https://www.tacc.utexas.edu/research-development/tacc-projects/lmod\">Lmod</a> as a modules tool,
a modern alternative to the outdated & no longer actively maintained 
	<a rel=\"nofollow\" href=\"http://modules.sourceforge.net\">Tcl-based environment modules tool</a>.
</p><p>Consult the <a href=\"http://lmod.readthedocs.io/en/latest/\">Lmod documentation web site</a> for more information.
</p><hr><h3>Benefits</h3><ul>
	<li> significantly more responsive module commands, in particular <code>module avail</code></li>
	<li> a better and easier to use interface (e.g. case-insensitive <code>avail</code>, the <code>ml</code> command, etc.)</li>
	<li> additional useful features, like defining & restoring module collections</li>
	<li> drop-in replacement for Tcl-based environment modules (existing Tcl module files do not need to be modified to work)</li>
	<li> module files can be written in either Tcl or Lua syntax (and both types of modules can be mixed together)</li>
</ul><hr><h3>Key differences</h3><p>The switch to Lmod should be mostly transparent, i.e. <i><strong>you should not have to change your existing job scripts</strong></i>.
</p><p>Existing <code>module</code> commands should keep working as they were before the switch to Lmod.
</p><p>However, there are a couple of minor differences between Lmod & the old modules tool you should be aware of:
</p><ul>
	<li> module conflicts are <i>strictly</i> enforced</li>
	<li> <code>module purge</code> does not unload the <code>cluster</code> module</li>
	<li> <code>modulecmd</code> is not available anymore (only relevant for EasyBuild)</li>
</ul><p><br>
	See below for more detailed information.
</p><hr><p><span id=\"module_conflicts\"></span>
</p><h4>Module conflicts are strictly enforced</h4><p><i>Conflicting modules can no longer be loaded together</i>.
</p><p>Lmod has been configured to report an error if any module conflict occurs
(as opposed to the default behaviour which is to unload the conflicting module and replace it with the one being loaded).
</p><p>Although it seemed like the old modules did allow for conflicting modules to be loaded together, this was highly
discouraged already since it usually resulted in a broken environment. Lmod will ensure no changes are made to your
existing environment if a module that conflicts with an already module is loaded.
</p><p>If you do try to load conflicting modules, you will run into an error message like:
</p><pre>$ module load Python/2.7.11-intel-2016a
$ module load Python/3.5.1-intel-2016a 
Lmod has detected the following error:  Your site prevents the automatic swapping of modules with same name.
You must explicitly unload the loaded version of \"Python\" before you can load the new one. Use swap (or an unload
followed by a load) to do this:
   $ module swap Python  Python/3.5.1-intel-2016a
Alternatively, you can set the environment variable LMOD_DISABLE_SAME_NAME_AUTOSWAP to \"no\" to re-enable same name
</pre><p>Note that although Lmod suggests to unload or swap, we recommend to try and make sure you <i>only load compatible</i>
	modules together<i>, and certainly <strong>not</strong> to define <code>$LMOD_DISABLE_SAME_NAME_AUTOSWAP</code>.</i>
</p><hr><p><span id=\"module_load_cluster\"></span>
</p><h4>module purge does not unload the cluster module</h4><p>Using <code>module purge</code> effectively resets your environment to a pristine <i>working</i> state, i.e. the <code>cluster</code> module <i>stays loaded</i> after the <code>purge</code>.<br>
	As such, it is no longer required to run <code>module load cluster</code> to restore your environment to a working state.
</p><p>When you do run <code>module load cluster</code> when a <code>cluster</code> is already loaded, you will see the following warning message:
</p><pre>WARNING: 'module load cluster' has no effect when a 'cluster' module is already loaded.
For more information, please see https://www.vscentrum.be/cluster-doc/software/modules/lmod#module_load_cluster
</pre><p>To change to another cluster, use <code>module swap</code> or <code>ml swap</code>;
for example, to change your environment for the 
	<code>golett</code> cluster, use <code>ml swap cluster/golett</code>.
</p><p>If you are frequently see the warning above pop up, you may have something like this in your <code>$VSC_HOME/.bashrc</code>
	file:
</p><pre>. /etc/profile.d/modules.sh
module load cluster
</pre><p>If you do, please remove that, and include this <i>at the top</i> of your <code>~/.bashrc</code> file:
</p><pre>if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
</pre><hr><h4>modulecmd is not available anymore</h4><p>The <code>modulecmd</code> command is not available anymore, and has been replacd by the <code>lmod</code> command.
</p><p>This is only relevant for EasyBuild, which has be to configured to use Lmod as a modules tool, since by default
it expects that 
	<code>modulecmd</code> is readily available.
	<br>For example:
</p><pre>export EASYBUILD_MODULES_TOOL=Lmod
</pre><p>See <a href=\"http://easybuild.readthedocs.io/en/latest/Configuration.html#supported-configuration-types\">the EasyBuild
documentation
	</a> for other ways of configuring EasyBuild to use Lmod.
</p><p>You should not be using <code>lmod</code> directly in other circumstances, use either <code>ml</code> or <code>module</code> instead.
</p><h2>Questions or problems</h2><p>In case of questions or problems, please do not hesitate to contact the support HPC team.  HPC-UGent support team can be reached via <strong><a rel=\"nofollow\" href=\"mailto:hpc@ugent.be\">hpc@ugent.be</a></strong>.
The HPC-VUB support team can be reached via 
	<strong><a rel=\"nofollow\" href=\"mailto:hpc@ugent.be\">hpc@vub.ac.be</a></strong>.
</p>"
701,"Job submission and credit reservations","<p>When you submit a job, a reservation is made.  This means that the number of credits required to run your job is marked as reserved.  Of course, this is the number of credits that is required to run the job during the walltime specified, i.e., the reservation is computed based on the requested walltime.
</p><p>Hence, if you submit a largish number of jobs, and the walltime is overestimated, reservation will be made for a total that is potentially much larger than what you'll actually be debited for upon job completion (you're only debited for the walltime used, not the walltime requested).
</p><p>Now, suppose you know that your job will most probably end within 24 hours, but you specify 36 hours to be on the safe side (which is a good idea).  Say, by way of example, that the average cost of a single job will be 300 credits.  You have 3400 credits, so you can probably run at least 10 such jobs, so you submit all 10.
</p><p>Here's the trap: for each job, a reservation is made, not of 300 credits, but of 450.  Hence everything goes well for the first 7 (7*450 = 3150 &lt; 3400), but for the 8th up to the 10th job, your account no longer has sufficient credits to make a reservation.  Those 3 jobs will be blocked by a SystemHold, and never execute (unless additional credits are requested, and a sysadmin releases them as will happen now).
</p><p>We actually have a nice tool to compute the maximum number of credits a job can take.  It is called gquote, and you can use it as follows.  Supose that you submit your job using, e.g.:
</p><pre>$ qsub  -l walltime=4:00:00 my_job.pbs
</pre><p>Then you can compute its cost (before actually doing the qsub) by:
</p><pre>$ module load accounting
$ gquote  -l walltime=4:00:00  my_job.pbs
</pre><p>If this is a worker job, and you submit it as, e.g.:
</p><pre>$ wsub  -data data.csv  -batch my_job.pbs  -l nodes=4:ppn=20
</pre><p>Then you can compute its cost (before actually doing the qsub) by:
</p><pre>$ module load accounting
$ gquote  -l nodes=4:ppn=20  my_job.pbs
</pre><p>As you can see, gquote takes the same arguments as qsub (so if you use wsub, don't use the -batch for the actual job script).  It will use both the arguments on the command line and the PBS directives in your script to compute the cost of the job in the same way PBS torque is computing the resources for your job.
</p><p>You will notice when using gquote that it will give you quotes that are more expensive than you expect.  This typically happens when you don't specify the processor attribute for the nodes resource.  gquote will assume that you job is executed on the most expensive processor type, which inflates prices.
</p><p>The price of a processor is of course proportional to its performance, so when the job finishes, you will be charged approximately the same regardless of the processor type the job ran on.  (It ran for a shorter time on a more faster, and hence more expensive processor.)</p>"
705,"","<p>This page describes the part of the job script that actually does the useful work and runs the programs you want to run.
</p><p>When your job is started by the scheduler and the resource manager, your job script will run as a regular script on the first core of the first node assigned to the job. <em><strong>The script runs in your home directory</strong></em>, which is not the directory where you will do your work, and with the standard user environment. So before you can actually start your program(s), you need to set up a proper environment. On a cluster, this is a bit more involved than on your PC, partly also because multiple versions of the same program may be present on the cluster, or there may be conflicting programs that make it impossible to offer a single set-up that suits all users.
</p><h2>Setting up the environment</h2><h3>Changing the working directory</h3><p>As explained above, the job script will start in your home directory, which is not the place where you should run programs. So the first step will almost always be to switch to the actual working directory (the bash <code>cd</code> command).
</p><ul>
	<li>In most cases, you simply want to start your job in the directory from which you submitted your job. Torque offers the environment variable <code>PBS_O_WORKDIR</code> for that purpose. So for most users, all you need is simply <code>cd $PBS_O_WORKDIR</code> as the first actual command in your job script. </li>
	<li>On all VSC clusters we also define a number of environment variables that point to different file systems that you have access to. They may also be useful in job scripts, and may help to make your job script more portable to other VSC clusters. An overview of environment variables that point to various file systems is given on the page \"<a href=\"https://www.vscentrum.be/cluster-doc/access-data-transfer/where-store-data\">where should which data be stored?</a>\".</li>
</ul><h3>Loading modules</h3><p>The next step consists of loading the appropriate modules. This is no different from loading the modules on the login nodes to prepare for your job or when running programs on interactive nodes, so we refer to the \"<a href=\"/cluster-doc/software/modules\">Modules</a>\" page in the \"<a href=\"/cluster-doc/software\">Running software</a>\" section.
</p><h3>Useful Torque environment variables</h3><p>Torque defines a lot of environment variables on the compute nodes on which your job runs, They can be very useful in your job scripts. Some of the more important ones are:
</p><ul>
	<li><code>PBS_O_WORKDIR</code> : The directory from which your job was submitted.</li>
	<li><code>PBS_JOBNAME</code> : The name of the job</li>
	<li><code>PBS_JOBID</code> : The unique jobid. This is very useful when constructing, e.g., unique file names.</li>
	<li><code>PBS_NUM_NODES</code> : The number of nodes you requested.</li>
	<li><code>PBS_NUM_PPN</code> : The number of cores per node requested.</li>
	<li><code>PBS_NP</code> : The total number of cores requested.</li>
	<li><code>PBS_NODEFILE</code> : This variable is used by several MPI implementation to get the node list from the resource manager when starting a MPI program. It will contain <code>$PBS_NP</code> lines.</li>
</ul><p>There are also some variables that are useful if you use the Torque command <code>pbsdsh</code> to execute a command on another node/core of your allocation. We mention them here for completeness, but they will also be elaborated on in the paragraph on \"<a href=\"#StartingEmbarrasinglyParallel\">Starting a single core program on each assinged core</a>\" further down this page.
</p><ul>
	<li><code>PBS_NODENUM</code> : The number of the node in your allocation. E.g., when starting a job with <code>-l nodes=3:ppn=5</code>, <code>$PBS_NODENUM</code> will be 0, 1 or 2 if the job script has actually been scheduled on three physically distinct nodes. As the job script executes on the first core of the allocation, its value will always be 0 in your job script.</li>
	<li><code>PBS_VNODENUM</code> : The number of the physical core or hyperthread in your allocation. The numbering continues across the nodes in the allocation, so in case of a job started with <code>-l nodes=3:ppn=5</code>, <code>$PBS_VNODENUM</code> will be a number between 0 and 14 (0 and 14 included). In your job script, its value will be 0.</li>
	<li><code>PBS_TASKNUM</code> : Number of the task. The numbering starts from 1 but continues across calls of <code>pbsdsh</code>. The login script runs with <code>PBS_TASKNUM</code> set to 1. The first call to <code>pbsdsh</code> will start its numbering from 2, and so on.</li>
</ul><h2>Starting programs</h2><p>We show some very common start scenarios for programs on a cluster:
</p><ul>
	<li>Shared memory programs with OpenMP as an example</li>
	<li>Distributed memory programs with MPI programs as an example</li>
	<li>An embarrassingly parallel job consisting of independent single-core runs combined in a single job script</li>
</ul><h3>Starting a single multithreaded program (e.g., an OpenMP program)</h3><p>Starting a multithreaded program is easy. In principle, all you need to do is call its executable as you would do with any program at the command line.
</p><p>However, often the program needs to be told how many threads to use. The default behaviour depends on the program. Most programs will either use only one thread unless told otherwise, or use one thread per core it can detect. The problem with programs that do the latter is that if you have requested only a subset of the cores on the node, the program will still detect the total number of cores or hyperthreads on the node and start that number of threads. Depending on the cluster you are using, these threads will swarm out over the whole node and sit in the way of other programs (often the case on older clusters) or will be contained in the set of cores/hyperthreads allocated to the job and sit in each others way (e.g., because they compete for the same limited cache space). In both cases, the program will run way slower than it could.
</p><p>You will also need to experiment a bit with the number of cores that can actually be used in a useful way. This depends on the code and the size of the problem you are trying to solve. The same code may scale to only 4 threads for a small problem yet be able to use all cores on a node well when solving a much larger problem.
</p><p>How to tell the program the number of threads to use, also differs between programs. Typical ways are through an environment variable or a command line option, though for some programs this is actually a parameter in the input file. Many scientific shared memory programs are developed using OpenMP directives. For these programs, the number of threads can be set through the environment variable OMP_NUM_THREADS. The line
</p><pre>export OMP_NUM_THREADS=$PBS_NUM_PPN
</pre><p>will set the number of threads to the value of <code>ppn</code> used in your job script.
</p><h3>Starting a distributed memory program (e.g., a MPI program)</h3><p>Starting a distributed memory program is a bit more involved as they always involve more than one Linux proces. Most distributed memory programs in scientific computing are written using the the Single Program Multiple Data paradigm: A single executable is ran on each core, but each cores works on a different part of the data. And the most popular technique for developing such programs is by using the MPI (Message Passing Interface) library.
</p><p>Distributed memory programs are usually started through a starter command. For MPI programs, this is <code>mpirun</code> or <code>mpiexec</code> (often one is an alias for the other). The command line arguments for mpirun differ between MPI implementations. We refer to the documentation on <a href=\"/cluster-doc/development/toolchains\">toolchains</a> in the \"<a href=\"/cluster-doc/development\">Software development</a>\" section of this web site for more information on the implementations supported at the VSC. As most MPI implementations in use at the VSC recognise our resource manager software and get their information about the number of nodes and cores directly from the resource manager, it is usually sufficient to start your MPI program using
</p><pre>mpirun &lt;mpi-program&gt;
</pre><p>where <code>&lt;mpi-program&gt;</code> is your MPI program and its command line arguments. This will start one instance of your MPI program on each core or hyperthread assigned to the job.
</p><p>Programs using different distributed memory libraries may use a different starter program, and some programs come with a script that will call mpirun for you, so you can start those as a regular program.
</p><p>Some programs use a mix of MPI and OpenMP (or a combination of another distributed and shared memory programming technique). Examples are some programs in Gromacs and QuantumESPRESSO. The rationale is that a single node on a cluster may not be enough, so you need distributed memory, while a shared memory paradigm is often more efficient in exploiting parallelism in the node. You'll need additional implementation-dependent options to mpirun to start such programs and also to define how many threads each instance can use. There is some information specifically for hybrid MPI/OpenMP programs on the \"<a href=\"/cluster-doc/development/hybrid-mpi-openmp\">Hybrid MPI/OpenMP programs</a>\" page in the software development section. We advise you to contact user support to help you figuring out the right options and values for those options if you are not sure which options and values to use.</p><h3><a name=\"StartingEmbarrasinglyParallel\"></a>Starting a single-core program on each assigned core</h3><p>A rather common use case on a cluster is running many copies of the same program independently on a different data set. It is not uncommon that those programs are not or very poorly parallelised and run on only a single core. Rather than submitting a lot of single core jobs, it is easier for the scheduler if those jobs are bundled in a single job that fills a whole node. Our job scheduler will try to fill a whole node using multiple of your jobs, but this doesn't always work right. E.g., assume a cluster with 20-core nodes where some nodes have 3 GB per core available for user jobs and some nodes have 6 GB available. If your job needs 5 GB per core (and you specify that using the <code>mem</code> or <code>pmem</code> parameters), but you don\\t explicitly tell that you want to use the nodes with 6 GB per core, the scheduler may still schedule the first job on a node with only 3 GB per core, then try to fill up that node further with jobs from you, but once half the node is filled discover that there is not enough memory left to start more jobs, leaving half of the CPU capacity unused.
</p><p>To ease combining jobs in a single larger job, we advise to have a look at the <a href=\"/cluster-doc/running-jobs/worker-framework\">Worker framework</a>. It helps you to organise the input to the various instances of your program for many common scenarios.
</p><p>Should you decide to start the instances of your program yourself, we advise to have a look at the Torque <code>pbsdsh</code> command rather than ssh. This assures that all programs will execute under the full control of the resource manager on the cores allocated to your job. The variables <code>PBS_NODENUM</code>, <code>PBS_VNODENUM</code> and <code>PBS_TASKNUM</code> can be used to determine on which core you are running and to select the appropriate input files. Note that in most cases, it will actually be necessary to write a second script besides your job script. That second script then uses these variables to compute the names of the input and the output files and start the actual program you want to run on that core.
</p><p>To further explore the meaning of <code>PBS_NODENUM</code>, <code>PBS_VNODENUM</code> and <code>PBS_TASKNUM</code> and to illustrate the use of <code>pbsdsh,</code> consider the job script
</p><pre>#! /bin/bash
cd $PBS_O_WORKDIR
echo \"Started with nodes=$PBS_NUM_NODES:ppn=$PBS_NUM_PPN\"
echo \"First call of pbsdsh\"
pbsdsh bash -c 'echo \"Hello from node $PBS_NODENUM ($HOSTNAME) vnode $PBS_VNODENUM task $PBS_TASKNUM\"'
echo \"Second call of pbsdsh\"
pbsdsh bash -c 'echo \"Hello from node $PBS_NODENUM ($HOSTNAME) vnode $PBS_VNODENUM task $PBS_TASKNUM\"'
</pre><p>Save this script as \"testscript.pbs\" and execute it for different numbers of nodes and cores-per-node using
</p><pre>qsub -l nodes=4:ppn=5 testscript.pbs
</pre><p>(so using 4 nodes and 5 cores per node in this example). When calling <code>qsub</code>, it will return a job number, and when the job ends you will find a file testscript.pbs.o&lt;number_of_the_job&gt; in the directory where you executed <code>qsub</code>.
</p><p>For more information on the pbsdsh command, we refer to the the Torque manual on the <a href=\"http://www.adaptivecomputing.com/support/documentation-index/\" target=\"_blank\">Adaptive Computing documentation web site</a>.
</p><ul>
	<li><a href=\"http://docs.adaptivecomputing.com/torque/6-0-1/help.htm\" target=\"_blank\">Torque 6.0.1</a> (Antwerp clusters, Hydra and BrENIAC)</li>
	<li><a href=\"http://docs.adaptivecomputing.com/torque/5-1-2/help.htm\" target=\"_blank\">Torque 5.1.X</a> (Thinking, muk)</li>
</ul><p>or to the manual page (\"<code>man pbsdsh</code>\").
</p>"
707,"","<h2>Submitting your job: the qsub command</h2><p>Once your job script is finished, you submit it to the scheduling system using the <code>qsub</code> command:
</p><pre>qsub &lt;jobscript&gt;
</pre><p>places your job script in the queue. As explained on the page on \"<a href=\"cluster-doc/running-jobs/specifying-requirements\">Specifying resources, output files and notifications</a>\", there are several options to tell the scheduler which resources you need or how you want to be notified of events surrounding your job. The can be given at the top of your job script or as additional command line options to <code>qsub</code>. In case both are used, options given on the command line take precedence over the specifications in the job script. E.g., if a different number of nodes and cores is requested through a command line option then specified in the job script, the specification on the command line will be used.
</p><h3>Starting interactive jobs</h3><p>Though our clusters are mainly meant to be used for batch jobs, there are some facilities for interactive work:
</p><ul>
	<li>The login nodes can be used for light interactive work. They can typically run the same software as the compute nodes. Some sites also have special interactive nodes for special tasks, e.g., scientific data visualisation. See the \"<a href=\"/infrastructure/hardware\">Available hardware</a>\" section where each site documents what is available.<br>Examples of work that can be done on the login nodes is running a GUI program that generates the input files for your simulation, a not too long compile, a quick and not very resource intensive visualisation. We have set limits on the amount of time a program can use on the login nodes.</li>
	<li>It is also possible to request one or more compute nodes for interactive work. This is also done through the <code>qsub</code> command. In this case, you can still use a job script to specify the resources, but the most common case is to specify them at the command line.</li>
</ul><p>In the latter scenario, two options of <code>qsub</code> are particularly useful: <code>-I</code> to request an node for interactive use, and <code>-X</code> to add support for X to the request. You would typically also add several <code>-l</code> options to specify for how long you need the node and the amount of resources that you need. E.g.,
</p><pre>qsub -I -l walltime=2:00:00 -l nodes=1:ppn=20
</pre><p>to use 20 cores on a single node for 2 hours. <code>qsub</code> will block until it gets a node and then you get the command prompt for that node. If the wait is too long however, <code>qsub</code> will return with an error message and you'll need to repeat the call.
</p><p>If you want to run programs that use X in your interactive job, you have to add the -X option to the above command. This will set up the forwarding of X traffic to the login node, and ultimately to your terminal if you have set up the connection to the login node properly for X support.
</p><p>Please remain reasonable in your request for interactive resources. On some clusters, a short waltime will give you a higher priority, and on most clusters a request for a multi-day interactive session will fail simply because the cluster cannot give you such a node before the time-out of <code>qsub</code> kicks in. Interactive use of nodes is mostly meant for debugging, for large compiles or larger visualisations on clusters that don't have dedicated nodes for visualisation.</p><h2>Viewing your jobs in the queue: qstat and showq
</h2><p>Two commands can be used to show your jobs in the queue:
</p><ul>
	<li><code>qstat</code> show the queue from the resource manager's perspective. It doesn't know about priorities, only about requested resources and the state of your job: Still idle and waiting for resources, running, finishing, ...</li>
	<li><code>showq</code> shows the queue from the scheduler's perspective, taking priorities and policies into account.</li>
</ul><p>Both commands will also show you the name of the queue (<code>qstat</code>) or class (<code>showq</code>) which in most cases is actually the same as the queue. All VSC clusters have multiple queues. Queues are used to define policies for each cluster. E.g., users may be allowed to have a lot of short jobs running simultaneously as they will finish soon anyway, but may be limited to a few multi-day jobs to avoid long-time monopolisation of a cluster by a single user, and this would typically be implemented by having separate queues with separate policies for short and long jobs. When you submit a job, <code>qsub</code> will put the job in a particular queue based on the resources requested. The <code>qsub</code> command does allow to specify the queue to use, but unless instructed to do so by user support, we strongly advise against using this option. Putting the job in the wrong queue may actually result in your job being refused by the queue manager, and we may also chose to change the available queues on a system to implement new policies.<br>
</p><h3>qstat</h3><p>On the VSC clusters, users will only receive a subset of the options that <code>qsub</code> offers. The output is always restricted to the user's jobs only.
</p><p>To see your jobs in the queue, enter
</p><pre>qstat
</pre><p>This will give you an overview of all jobs including their status, which includes queues but not yet running (Q), running (R) or finishing (C).
</p><pre>qstat &lt;jobid&gt;
</pre><p>where &lt;jobid&gt; is the number of the job, will show you the information about this job only.
</p><p>Several command line options can be specified to modify the output of <code>qstat</code>:
</p><ul>
	<li><code>qstat -i</code> will show you a bit more information.</li>
	<li><code>qstat -n</code> will also show you the nodes allocated to each running job.</li>
	<li><code>qstat -f</code> or <code>qstat -f1</code> produces even more output. In fact, it produces so much output that it is better only used with the job ID as an argument to request information about a specific job.</li>
</ul><h3>showq</h3><p>The <code>showq</code> command will show you information about the queue from the scheduler's perspective. Jobs are subdivided in three categories:
</p><ul>
	<li>The active jobs are the jobs that are actually running, or are being started or terminated.</li>
	<li>Eligible jobs are jobs that are queued and considered eligible for scheduling. </li>
	<li>Blocked jobs are jobs that are ineligible to run or to be queued for scheduling. There are multiple reasons why a job might be in the blocked state.
	<ul>
		<li>If the status is marked as idle, your job most likely violates a fairness policy, i.e., you've used too many resources recently.</li>
		<li>BatchHold: Either the cluster has repeatedly failed to start the job (which typically is a problem with the cluster, so contact user support if you see this happen) or your resource request cannot be granted on the cluster. This is also the case if you try to put more jobs in a queue than you are allowed to have queued or running at any particular moment.</li>
	</ul>
	<ul>
		<li>deferred: a temporary hold after a failed start attempt, but the system will have another try at starting the job.</li>
	</ul>
	<ul>
		<li>UserHold or SystemHold: The user or the system administrator has put a hold on the job (and it is is up to him/her to also release that hold again).</li>
	</ul>
	<ul>
		<li>NotQueued: The job has not been queued for some other reason.</li>
	</ul></li>
</ul><p>The <code>showq</code> command will split its output according to the three major categories. Active jobs are sorted according to their expected end time while eligible jobs are sorted according to their current priority.
</p><p>There are also some useful options:
</p><ul>
	<li><code>showq -r</code> will show you the running jobs only, but will also give more information about these jobs, including an estimate about how efficiently they are using the CPU.</li>
	<li><code>showq -i </code>will give you more information about your eligible jobs.</li>
</ul><h2>Getting detailed information about a job: qstat -f and checkjob </h2><p>We've discussed the Torque <code>qstat -f</code> command already in the previous section. It gives detailed information about a job from the resource manager's perspective.
</p><p>The <code>checkjob</code> command does the same, but from the perspective of the scheduler, so the information that you get is different.
</p><pre>checkjob 323323
</pre><p>will produce information about the job with jobid 323323.
</p><pre>checkjob -v 323323
</pre><p>where -v stands for verbose produces even more information.
</p><p>For a running job, checkjob will give you an overview of the allocated resources and the wall time consumed so far. For blocked jobs, the end of the output typically contains clues about why a job is blocked.
</p><h2>Deleting a job that is queued or running</h2><p>This is easily done with <code>qdel</code>:</p><pre>qdel 323323</pre><p>will delete the job with job ID 323323. If the job is already running, the processes will be killed and the resources will be returned to the scheduler for another job.</p><h2>Getting an estimate for the start time of your job: showstart</h2><p>This is a very simple tool that will tell you, based on the current status of the cluster, when your job is scheduled to start. Note however that this is merely an estimate, and should not be relied upon: jobs can start sooner if other jobs finish early, get removed, etc., but jobs can also be delayed when other jobs with higher priority are submitted.
</p><pre>$ showstart 20030021
job 20030021 requires 896 procs for 1:00:00
Earliest start in       5:20:52:52 on Tue Mar 24 07:36:36
Earliest completion in  5:21:52:52 on Tue Mar 24 08:36:36
Best Partition: DEFAULT
</pre><p>Note however that this is only an estimate, starting from the jobs that are currently running or in the queue and the wall time that users gave for these jobs. Jobs may always end earlier than predicted based on the requested wall time, so your job may start earlier. But other jobs with a higher priority may also enter the queue and delay the start from your job.</p><h2>See if there is are free resources that you might use for a short job: showbf</h2><p>When the scheduler performs its scheduling task, there is bound to be some gaps between jobs on a node. These gaps can be back filled with small jobs. To get an overview of these gaps, you can execute the command <code>showbf</code>:</p><pre>$ showbf
backfill window (user: 'vsc30001' group: 'vsc30001' partition: ALL) Wed Mar 18 10:31:02
323 procs available for      21:04:59
136 procs available for   13:19:28:58</pre><p>There is however no guarantee that if you submit a job that would fit in the available resources, it will also run immediately. Another user might be doing the same thing at the same time, or you may simply be blocked from running more jobs because you already have too many jobs running or have made heavy use of the cluster recently.</p><p><br></p>"
709,"","<h2>The basics of the job system</h2><ul>
	<li><a href=\"/cluster-doc/running-jobs/job-system-what-why\">What is a job system and why do we need it?</a></li>
	<li><a href=\"/cluster-doc/running-jobs/specifying-requirements\">Specifying job requirement</a></li>
	<li><a href=\"/cluster-doc/running-jobs/starting-programs-in-job\">Starting programs in your job</a></li>
	<li><a href=\"/cluster-doc/running-jobs/submitting-managing-jobs\">Submitting and managing jobs</a></li>
</ul><h2>Common problems</h2><ul>
	<li><a href=\"/cluster-doc/running-jobs/job-start-failure\">Why doesn't my job start immediately?</a> A story about scheduling policies and priorities.</li>
	<li><a href=\"/cluster-doc/running-jobs/job-failure-after-start\">Job failure after a successful start</a></li>
</ul><span></span><h2>Advanced topics<br></h2><ul>
	<li><a href=\"/cluster-doc/running-jobs/credit-system-basics\">Credit system basics</a>: credits are used on all clusters at the KU Leuven (including the Tier-1 system BrENIAC) to control your compute time allocation</li><li><a href=\"/cluster-doc/running-jobs/monitoring-memory-and-cpu-usage-of-programs\">Monitoring memory and CPU usage of programs</a>, which helps to find the right parameters to improve your specification of the job requirements</li>
	<li><a href=\"/cluster-doc/running-jobs/worker-framework\">Worker framework</a>: To manage lots of small jobs on a cluster. The cluster scheduler isn't meant to deal with tons of small jobs. Those create a lot of overhead, so it is better to bundle those jobs in larger sets.</li>
	<li>The <a href=\"/cluster-doc/running-jobs/checkpointing-framework\">checkpointing framework</a> can be used to run programs that take longer than the maximum time allowed by the queue. It can break a long job in shorter jobs, saving the state at the end to automatically start the next job from the point where the previous job was interrupted.</li><li>Running jobs on GPU or Xeon Phi nodes: The procedure is not standardised across the VSC, so we refer to the pages for each cluster in the \"<a href=\"/infrastructure/hardware\">Available hardware</a>\" section of this web site<ul><li><a href=\"/infrastructure/hardware/k20x-phi-hardware\">KU Leuven GPU and Xeon Phi nodes</a></li></ul></li>
</ul>"
711,"","<h2 style=\"font-family: Arial, Helvetica, Verdana, Tahoma, sans-serif;\">Access restriction</h2><p>Once your project has been approved, your login on the Tier-1 cluster will be enabled. You use the same vsc-account (vscXXXXX) as at your home institutions and you use the same $VSC_HOME and $VSC_DATA directories, though the Tier-1 does have its own scratch directories.
</p><p>You can log in to the following login nodes:
</p><ul>
	<li>login1-tier1.hpc.kuleuven.be</li>
	<li>login2-tier1.hpc.kuleuven.be</li>
</ul><p>These nodes are also accessible from outside the KU Leuven. Unless for the Tier-1 system muk, it is not needed to first log on to your home cluster to then proceed to BrENIAC. Have a look at the <a href=\"https://www.vscentrum.be/assets/1155\">quickstart guide</a> for more information.
</p><h2>Hardware details</h2><p>The tier-1 cluster <em>BrENIAC</em> is primarily aimed at large parallel computing jobs that require a high-bandwidth low-latency interconnect, but jobs that require a multitude of small independent tasks are also accepted.
</p><p>The main architectural features are:
</p><ul>
	<li>580 compute nodes with two Xeon E5-2680v4 processors (2,4GHz, 14 cores per processor, Broadwell architecture). 435 nodes are equiped with 128 GB RAM and 135 nodes with 256 GB. The total number of cores is 16,240, the total memory capacity is 90.6 TiB and the peak performance is more than 623 TFlops (Linpack result 548 TFlops).<br>The Broadwell CPU supports the 256-bits AVX2 vector instructions with fused-multiply-add operations. Each core can execute up to 16 double precision floating point operations per cycle (2 4-number FMAs), but to be able to use the AVX2 instructions, you need to recompile your program for the Haswell or Broadwell architecture.<br>The CPU also uses what Intel calls the \"Clustter-on-Die\"-approach, which means that each processor chip internally has two groups of 7 cores. For hybrid MPI/OpenMP processes (or in general distributed/shared memory programs), 4 MPI processes per node each using 7 cores might be a good choice.</li>
	<li>EDR Infiniband interconnect with a fat tree topology (blocking factor 2:1)</li>
	<li>A storage system with a net capacity of approximately 634 TB and a peak bandwidth of 20 GB/s, using the GPFS file system.</li>
	<li>2 login nodes with a similar configuration as the compute nodes</li>
</ul><p>Compute time on <em>BrENIAC</em> is only available upon approval of a project. Information on requesting projects is available <a href=\"https://www.vscentrum.be/nl/systemen-en-toegang/projecttoegang-tier1\">in Dutch</a> and <a href=\"https://www.vscentrum.be/en/access-and-infrastructure/project-access-tier1\">in English</a>.<br>
</p><h2>Accessing your data<br></h2><p>BrENIAC supports the standard VSC directories.
</p><ul>
	<li>$VSC_HOME points to your VSC home directory. It is your standard home directory which is accessed over the VSC network, and available as /user/&lt;institution&gt;/XXX/vscXXXYY, e.g., /user/antwerpen/201/vsc20001. So the quota on this directory is set by your home institution.</li>
	<li>$VSC_DATA points to your standard VSC data directory, accessed over the VSC network. It is available as /data/&lt;institution&gt;/XXX/vscXXXYY. The quota on this directory is set by your home institution. The directory is mounted via NFS which lacks some of the feature of the parallel file system which may be available at your home institution. Certain programs using parallel I/O may fail when running from this directory, you are strongly encouraged to only run programs from $VSC_SCRATCH.</li>
	<li>$VSC_SCRATCH is a Tier-1 specific fast parallel file system using the GPFS file system. The default quota is 1 TiB but may be changed depending on your project request. The directory is also available as /scratch/leuven/XXX/vscXXXYY (and note \"leuven\" in the name, not your own institutions as this directory is physically located on the Tier-1 system at KU Leuven). The variable $VSC_SCRATCH_SITE points to the same directory.</li>
	<li>$VSC_NODE_SCRATCH points to a small (roughly 70 GB) local scratch directory on the SSD of each node. It is also available as /node_scratch/&lt;jobid&gt;. The contents is only accessible from a particular node and during the job.  </li>
</ul><h2>Running jobs and specifying node characteristics</h2><p>The cluster uses Torque/Moab as all other clusters at the VSC, so the generic documentation applies to BrENIAC also.
</p><ul>
	<li>BrENIAC uses a single job per node policy. So if a user submits single core jobs, the nodes will usually be used very inefficiently and you will quickly run out of your compute time allocation. Users are strongly encouraged to use the Worker framework (e.g., module worker/1.6.7-intel-2016a) to group such single-core jobs. Worker makes the scheduler's task easier as it does not have to deal with too many jobs. It has <a href=\"/cluster-doc/running-jobs/worker-framework\">a documentation page on this user portal</a> and a <a href=\"http://worker.readthedocs.io/en/latest/\">more detailed external documentation site</a>.</li>
	<li>The maximum regular job duration is 3 days.</li>
	<li>Take into account that each node has 28 cores. These are logically grouped in 2 sets of 14 (socket) or 4 sets of 7 (NUMA-on-chip domains). Hence for hybrid MPI/OpenMP programs, 4 MPI processes per node with 7 threads each (or two with 14 threads each) may be a better choice than 1 MPI process per node with 28 threads.</li>
</ul><p>Several \"MOAB features\" are defined to select nodes of a particular type on the cluster. You can specify them in your job scirpt using, e.g.,
</p><pre>#PBS -l feature=mem256
</pre><p>to request only nodes with the mem256 feature. Some important features:
</p><table>
<thead>
<tr>
	<th>feature
	</th>
	<th>explanation
	</th>
</tr>
</thead>
<tbody>
<tr>
	<td>mem128
	</td>
	<td>Select nodes with 128 GB of RAM (roughly 120 GB available to users)</td>
</tr>
<tr>
	<td>mem256
	</td>
	<td>Select nodes with 256 GB of RAM (roughly 250 GB available to users)</td>
</tr>
<tr>
	<td>rXiY
	</td>
	<td>Request nodes in a specific InfiniBand island. X ranges from 01 to 09, Y can be 01, 11 or 23. The islands RxI01 have 20 nodes each, the islands rXi11 and rXi23 with i = 01, 02, 03, 04, 06, 07, 08 or 09 have 24 nodes each and the island r5i11 has 16 nodes. This may be helpful to make sure that nodes used by a job are as close to each other as possible, but in general will increase waiting time before your job starts.
	</td>
</tr>
</tbody>
</table><h3>Compile and debug nodes</h3><p>8 nodes with 256 GB of RAM are set aside for compiling or debugging small jobs. You can run jobs on them by specifying</p><pre>#PBS -lqos=debugging</pre><p>in your job script. </p><p>The following limitation apply:</p><ul><li>Maximum 1 job per user at a time</li><li>Maximum 8 nodes for the job</li><li>Maximum accumulated wall time is 1 hour. e.g., a job using 1 node for 1 hour or a job using 4 nodes for 15 minutes.</li></ul><h3>Credit system</h3><p>BrENIAC uses Moab Accounting Manager for accounting the compute time used by a user. Tier-1 users have a credit account for each granted Tier-1 project. When starting a job, you need to specify which credit account to use via</p><pre>#PBS -A lpt1_XXXX-YY</pre><p>or with lpt1_XXXX-YY the name of your project account. You can also specify the -A option at the command line of qsub.</p><p>Further information</p><ul><li><a href=\"/assets/1155\">BrENIAC Quick Start Guide</a></li></ul><h2>Software specifics</h2><p>BrENIAC uses the standard VSC toolchains. However, not all VSC toolchains are made available on BrENIAC. For now, only the 2016a toolchain is available. The Intel toolchain has slightly newer versions of the compilers, MKL library and MPI library than the standard VSC 2016a toolchain to be fully compatible with the machine hardware and software stack.</p><h2>Some history</h2><p>BrENIAC was installed during the spring of 2016, followed by several months of testing, first by the system staff and next by pilot users. The system was officially launched on October 17 of that year, and by the end of the month new Tier-1 projects started computing on the cluster.
</p><p>We have a time lapse movie of the construction of BrENIAC:
</p><p align=\"center\">
	<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/WPVOWdCbpro\" frameborder=\"0\" allowfullscreen=\"\">
	</iframe>
</p><h2>Documentation</h2><ul>
	<li><a href=\"/assets/1155\">BrENIAC Quick Start Guide (PDF)</a><span></span></li>
</ul>"
713,"","<p>(Testtekst) The Flemish Supercomputer Centre (<strong>VSC</strong>) is a virtual centre making supercomputer infrastructure available for both the <strong>academic</strong> and <strong>industrial</strong> world. This centre is managed by the Research Foundation - Flanders (FWO) in partnership with the five Flemish university associations.<br>
</p>"
715,"HPC for industry (testversie)","<p>The collective expertise, training programs and infrastructure of VSC together with participating university associations have the potential to create significant added value to your business.</p>"
717,"HPC for academics (testversie)","<p>With HPC-technology you can refine your research and gain new insights to take your research to new heights. </p>"
719,"What is supercomputing? (testversie)","<p>Supercomputers have an immense impact on our daily lives. Their scope extends far beyond the weather forecast after the news.</p>"
739,"","<h2>Basic job system use</h2><ul>
	<li><a href=\"/cluster-doc/running-jobs/job-system-basics\">Job system basics</a></li>
	<li><a href=\"/cluster-doc/running-jobs/credit-system-basics\">Credit system basics</a> (KU Leuven-only currently)</li>
	<li><a href=\"/cluster-doc/running-jobs/job-start-failure\">Why doesn't my job start immediately?</a> A story about scheduling policies and priorities.</li>
	<li><a href=\"/cluster-doc/running-jobs/job-failure-after-start\">Job failure after a successful start</a></li>
</ul><h2>Advanced job system use</h2><ul>
	<li><a href=\"/cluster-doc/running-jobs/advanced-torque\">Advanced PBS/Torque options</a></li>
	<li><a href=\"/cluster-doc/running-jobs/moab-commands\">Moab commands</a></li>
	<li><a href=\"/cluster-doc/running-jobs/worker-framework\">Worker framework</a>: To manage lots of small jobs on a cluster. The cluster scheduler isn't meant to deal with tons of small jobs. Those create a lot of overhead, so it is better to bundle those jobs in larger sets.</li>
</ul><h2>Miscellaneous topics</h2><ul>
	<li><a href=\"/cluster-doc/running-jobs/monitoring-memory-and-cpu-usage-of-programs\">Monitoring memory and CPU usage of programs</a>, which helps to find the right parameters to use in the job scripts.</li>
	<li>The <a href=\"/cluster-doc/running-jobs/checkpointing-framework\">checkpointing framework</a> can be used to run programs that take longer than the maximum time allowed by the queue. It can break a long job in shorter jobs, saving the state at the end to automatically start the next job from the point where the previous job was interrupted.</li>
</ul>"
741,"","<h2>Access</h2><pre>qsub  -l partition=gpu,nodes=1:K20Xm &lt;jobscript&gt;
</pre><p>or
</p><pre>qsub  -l partition=gpu,nodes=1:K40c &lt;jobscript&gt;
</pre><p>depending which GPU node you would like to use if you don't 
'care' on which type of GPU node your job ends up you can just submit it
 like this:
</p><pre>qsub  -l partition=gpu &lt;jobscript&gt;
</pre><h3>Submit to a Phi node:</h3><pre>qsub -l partition=phi &lt;jobscript&gt;
</pre>"
745,"","<h2>The application</h2><p>The designated way to get access to the Tier-1 for research purposes is through a project application.
</p><p>You have to submit a proposal to get compute time on the Tier-1 cluster Muk.
</p><p>You should include a realistic estimate of the compute time needed in the project in your application. These estimations can best be endorsed by Tier-1 benchmarks. To be able to perform these tests for new codes, you can request a <a href=\"/en/access-and-infrastructure/tier1-starting-grant\">starting grant</a> through a short and quick procedure.
</p><p>You can submit proposals continuously, but they will be gathered, evaluated and resources allocated at a number of cut-off dates. There are 3 cut-off dates in 2016 :
</p><ul>
	<li>February 1, 2016</li>
	<li>June 6, 2016</li>
	<li>October 3, 2016</li>
</ul><p>Proposals submitted since the last cut-off and before each of these dates are reviewed together.
</p><p>The FWO appoints an evaluation commission to do this.
</p><p>Because of the international composition of the <a href=\"/en/about-vsc/organisation-structure#tier1-evaluation\">evaluation commission</a>, the preferred language for the proposals is English. If a proposal is in Dutch, you must also sent an English translation. Please have a look at the documentation of standard terms like: CPU, core, node-hour, memory, storage, and use these consistently in the proposal.
</p><p>For applications in 2014 or 2015, costs for resources used will be invoiced, with various discounts for Flemish-funded academic researchers. You should be aware that the investments and operational costs for the Tier-1 infrastructure are considerable.
</p><p>You can submit you application <a href=\"#easychair\">via EasyChair</a> using the application forms below.
</p><h2>Relevant documents - 2016</h2><p>On October 26 the Board of Directors of the Hercules foundation decided to make a major adjustment to the regulations regarding applications to use the Flemish supercomputer.
</p><p>For applications for computing time on the Tier-1 granted in 2016 and coming from researchers at universities, the Flemish SOCs and the Flemish public knowledge institutions, applicants will no longer have to pay a contribution in the cost of compute time and storage. Of course, the applications have to be of outstanding quality. The evaluation commission remains responsible for te review of the applications.
</p><p>For applications granted in 2015 the current pricing structure remains in place and contributions will be asked.
</p><p>The adjusted Regulations for 2016 can be found in the links below.
</p><p>From January 1, 2016 on the responsibility for the funding of HPC and the management of the Tier-1 has been transferred to the FWO, including all current decisions and ongoing contracts.
</p><ul>
	<li><a href=\"/assets/1043\">Reglement betreffende aanvragen voor het gebruik van de Vlaamse supercomputer (Dutch only, applicable as of 1 January 2016) (PDF, 213 kB)</a><i></i></li>
	<li>Enclosure 1: <a href=\"/assets/1041\">The application form for Category 1 Applications</a> (research project of which the scientific quality has already been evaluated, see §1 of the Regulations for the criteria) (docx, 60 kB)<i></i></li>
	<li>Enclosure 2: <a href=\"/assets/1045\">The application form for Category 2 projects</a> (research projects that have not yet been evaluated scientifically) (docx, 71 kB)<i></i></li>
	<li>Enclosure 3: <a href=\"#easychair\">The procedure for the use of EasyChair</a><i></i></li>
	<li><a href=\"https://www.vscentrum.be/assets/1029\">The official version of the letter highlighting the 2016 changes is only available in Dutch by following this link</a>.</li>
	<li><a href=\"/cluster-doc/software/tier1-muk\">An overview of the available (scientific) software</a><i></i></li>
	<li><a href=\"/support/tut-book/hpc-glossary\">An overview of standard terms used in HPC</a><i></i></li>
	<li><a href=\"/en/access-and-infrastructure/project-access-tier1/domains\">The list of scientific domains</a></li>
</ul><p>If you need help to fill out the application, please consult your local support team.
</p><h2>Relevant documents - 2015</h2><ul>
	<li><a href=\"/assets/207\">Regulations regarding applications to use the Flemish supercomputer (applicable as of 1 January 2015)</a><i></i></li>
	<li>Enclosure 1: <a href=\"#pricing\">Cost price computing time and \"SCRATCH\" disk storage</a></li>
</ul><h2><a name=\"pricing\"></a>Pricing - applications in 2015</h2><p>When you  receive compute time through a Tier-1 project application, we expect a contribution in the cost of compute time and storage.
</p><table>
<thead>
<tr>
	<td>
		<p><strong>Summary of Rates:</strong>
		</p>
	</td>
	<td>
		<p><strong>CPU/nodeday</strong>
		</p>
	</td>
	<td>
		<p><strong>Private Disk/TB/mo</strong>
		</p>
	</td>
</tr>
</thead>
<tbody>
<tr>
	<td>
		<p><strong>Universities, VIB and iMINDS</strong>
		</p>
	</td>
	<td>
		<p><strong>0.68€ (5%)</strong><strong></strong><sup></sup>
		</p>
	</td>
	<td>
		<p><strong>2€ (5%)</strong>
		</p>
	</td>
</tr>
<tr>
	<td>
		<p><strong>other SOCs and other flemish public research institutes</strong>
		</p>
	</td>
	<td>
		<p><strong>1.35€ (10%)</strong>
		</p>
	</td>
	<td>
		<p><strong>4€ (10%)</strong>
		</p>
	</td>
</tr>
<tr>
	<td>
		<p><strong>Flemish public research institutes - contract research with possibility of full cost accounting (*)</strong>
		</p>
	</td>
	<td>
		<p><strong>13,54€</strong>
		</p>
	</td>
	<td>
		<p><strong>46,8€</strong>
		</p>
	</td>
</tr>
<tr>
	<td>
		<p><strong>Flemish public research institutes - European projects with possibility of full cost accounting (*)</strong>
		</p>
	</td>
	<td>
		<p><strong>13,54€</strong>
		</p>
	</td>
	<td>
		<p><strong>46,8€</strong>
		</p>
	</td>
</tr>
</tbody>
</table><p>(*) The price for one nodeday is 13.54 euro (incl. overhead and support of Tier-1 technical support team, but excl. advanced support by specialized staff). The price for 1TB storage per month is 46.80 euro (incl. overhead and support of TIER1 technical support team, but excl. advanced support by specialized staff). Approved Tier-1 projects get a default quota of 1TB. Only storage request higher then 1TB will be charged for the amount above 1TB.
</p><h2><a name=\"easychair\"></a>EasyChair procedure</h2><p>You have to submit your proposal on <a href=\"https://easychair.org/conferences/?conf=tier12016\">EasyChair for the conference Tier12016</a>. This requires the following steps:
</p><ol>
	<li>If you do not yet have an EasyChair account, you first have to create one:
	<ol>
		<li>Complete the CAPTCHA</li>
		<li>Provide first name, name, e-mail address</li>
		<li>A confirmation e-mail will be sent, please follow the instructions in this e-mail (click the link)</li>
		<li>Complete the required details.</li>
		<li>When the account has been created, a link will appear to log in on the TIER1 submission page.</li>
	</ol></li>
	<li>Log in onto the EasyChair system.</li>
	<li>Select ‘New submission’.</li>
	<li>If asked, accept the EasyChair terms of service.</li>
	<li>Add one or more authors; if they have an EasyChair account, they can follow up on and/or adjust the present application.</li>
	<li>Complete the title and abstract.</li>
	<li>You must specify at least three keywords: Include the institution of the promoter of the present project and the field of research.</li>
	<li><span></span>As a paper, submit a PDF version of the completed Application form. You must submit the complete proposal, including the enclosures, as 1 single PDF file to the system.</li>
	<li>Click \"Submit\".</li>
	<li>EasyChair will send a confirmation e-mail to all listed authors.</li>
</ol>"
747,"","<p>From version 2017a on of the Intel toolchains, the setup on the UAntwerp is different from the one on some other VSC clusters:
</p><ul>
	<li>A full install of all Intel tools for which we have a license at UAntwerp has been performed in a single directory tree as intended by Intel. There is a single module for the C/C++/Fortran compilers, Intel MPI, the libraries MKL (Math Kernel Library), IPP (Integrated Performance Primitives), TBB (Threading Building Blocks) and DAAL (Data Analytics Acceleration Library) and Intel-provided GDB-based debuggers. The Intel tools for code and performance analysis VTune Amplifier XE, Intel Trace Analyzer and Collector (ITAC), Intel Advisor XE and Intel Inspector XE are also installed, but these still have separate module files as they rely on overloading libraries in some cases.</li>
	<li>There should be no need to run any of the configuration scripts provided by Intel, all variables should be set correctly by the module file. Contact user support if this is not the case. The configuration scripts should work as intended by Intel though should you want to use the compilers without loading the module.</li>
	<li>Several variables specific for the way software is set up at the VSC are defined the way they would be defined if the Intel toolchain was defined in the standard VSC way through the module tree. As such, we expect that you should be able to use any Makefile developed for the standard VSC-setup.</li>
	<li>All compiler components needed to develop applications with offload to a Xeon Phi expansion board are also provided in anticipation of the installation of such a cluster node for testing purposes in Leibniz.</li>
</ul><h2>Compilers</h2><ul>
	<li>The compilers work exactly in the way described on <a href=\"/cluster-doc/development/toolchain-intel\">the regular Intel toolchain web page</a>, including the MPI compiler wrappers. All links to the documentation on that page are also relevant.</li>
	<li>Man pages for all commands have also been installed.</li>
</ul><h2>Debuggers</h2><ul>
	<li>Intel-adapted GDB debuggers have been installed
	<ul>
		<li>Debugging regular Intel64 applications: <code>gdb-ia</code></li>
		<li>Debugging applications with offload to Xeon Phi: <code>gdb-mic</code></li>
	</ul></li>
	<li>Manual pages and GNU info pages are available for both commands</li>
</ul><h2>Libraries</h2><h3>Math Kernel Library (MKL)</h3><p>MKL works exactly as in the regular VSC Intel toolchain. See <a href=\"https://www.vscentrum.be/cluster-doc/development/toolchain-intel#intel-mathematical-libraries\">the MKL section of web page on the VSC Intel toolchain</a> for more information.
</p><h3>Integrated Performance Primitives (IPP)</h3><ul>
	<li>What? The Intel Integrated Performance Primitives is a software library that provides a broad range of functionality, including general signal and image processing, computer vision, data compression, cryptography, and string manipulation. The functions are heavily optimised for the Intel processors.</li>
	<li>Documentation
	<ul>
		<li><a href=\"https://software.intel.com/en-us/get-started-with-ipp-for-linux\">Getting started with Intel® Integrated Performance Primitives 2017 for Linux* OS</a></li>
		<li>Developer guide and various tutorials in <a href=\"https://software.intel.com/en-us/documentation\">the Intel Software Documentation Library</a></li>
	</ul></li>
</ul><h3>Threading Building Blocks (TBB)</h3><ul>
	<li>What? A C++ template library for task parallelism. The library is developed by Intel, but now available under the Apache 2.0 license and runs also with non-Intel compilers, so you don't have to be concerned about vendor lock-in.</li>
	<li>Documentation
	<ul>
		<li><a href=\"https://www.threadingbuildingblocks.org/\">The Intel Threading Building Blocks web site</a> </li>
		<li>Documentation in the <a href=\"https://software.intel.com/en-us/documentation\">Intel Software Documentation Library</a></li>
	</ul></li>
</ul><h3>Data Analytics Acceleration Library (DAAL)</h3><ul>
	<li>What? A library with building blocks covering all stages of data analytics: data acquisition from a data source, preprocessing, transformation, data mining, modeling, validation, and decision making. The implementation is heavily optimised for Intel processors.</li>
	<li>Documentation
	<ul>
		<li><a href=\"https://software.intel.com/en-us/get-started-with-daal-for-linux\">Getting Started with Intel® Data Analytics Acceleration Library for Linux</a></li>
		<li>Documentation in the <a href=\"https://software.intel.com/en-us/documentation\">Intel Software Documentation Library</a></li>
	</ul></li>
</ul><h2>Code and performance analysis</h2><h3>VTune Amplifier XE</h3><ul>
	<li>What? VTune Amplifier is a tool for runtime analysis of serial and shared memory applications. For a list of features, please consult the <a href=\"https://software.intel.com/en-us/get-started-with-vtune\">Getting Started with VTune web page</a>.</li>
	<li>Module: <code>VTune/&lt;toolchain version&gt;</code>, e.g., <code>VTune/2017a</code>.</li>
	<li>How? VTune Amplifier is started through the <code>amplxe-gui</code> (GUI version) or <code>amplxe-cl</code> (command line version) command.</li>
	<li>Documentation:
	<ul>
		<li><a href=\"https://software.intel.com/en-us/get-started-with-vtune\">Getting started with VTune web page on the Intel web site</a></li>
		<li><a href=\"https://software.intel.com/en-us/intel-vtune-amplifier-xe-support/training\">VTune Amplifier training page by Intel</a></li>
		<li>Intel VTune Amplifier documentation in the <a href=\"https://software.intel.com/en-us/documentation\">Intel Software Documentation Library</a></li>
		<li>Man page for <code>amplxe-cl</code> (after loading the appropriate VTune module)</li>
	</ul></li>
</ul><h3>ITAC - Intel Trace Analyzer and Collector</h3><ul>
	<li>What? ITAC is a graphical tool to understand MPI application behaviour. A full set of features can be found on the <a href=\"https://software.intel.com/en-us/get-started-with-itac\">Getting Started with Intel Trace Analyzer and Colllector page</a>.</li>
	<li>How? Analysis is a two-step operation:
	<ol>
		<li>Run your application using the <code>-trace</code> option of mpirun to collect data in a <code>.stf</code> file.</li>
		<li>Analyse the data using <code>traceanalyzer</code></li>
	</ol></li>
	<li>Module: <code>itac/&lt;toolchain version&gt;</code>, e.g., <code>itac/2017a</code>.</li>
	<li>Documentation:
	<ul>
		<li><a href=\"https://software.intel.com/en-us/get-started-with-itac\">Getting started with Intel Trace Analyzer and Collector on the Intel web site</a></li>
		<li><a href=\"https://software.intel.com/en-us/intel-trace-analyzer-support/training\">Training videos on the Intel site</a></li>
		<li>Intel Trace Analyzer and Collector documentation in the <a href=\"https://software.intel.com/en-us/documentation\">Intel Software Documentation Library</a></li>
		<li>Manual pages for the library functions and some commands</li>
	</ul></li>
</ul><h3>Advisor</h3><ul>
	<li>What? Advisor is a code analysis tool that works with the compilers to give advise on vectorization and threading for both the Xeon and Xeon Phi processors.</li>
	<li>How? Advisor uses output generated by the compiler when building a full optimized release build and as such expects that some additional options are specified when compiling the application. The resulting compiler output can then be analized using the <code>advixe-gui</code> command. </li>
	<li>Module: <code>Advisor/&lt;toolchain version&gt;</code>, e.g., <code>Advisor/2017a</code>.</li>
	<li>Documentation
	<ul>
		<li><a href=\"https://software.intel.com/en-us/get-started-with-advisor\">Getting started with Intel Advisor</a></li>
		<li>Intel Advisor documentation in the <a href=\"https://software.intel.com/en-us/documentation\">Intel Software Documentation Library</a></li>
		<li>Manual page for <code>advixe-cl</code></li>
	</ul></li>
</ul><h3>Inspector</h3><ul>
	<li>What? Inspector is a run-time analysis error checking tool for dynamic memory and threading errors.</li>
	<li>How? Inspector uses a debug build of you application. The application is run from within Inspector. Inspector can be started with the <code>inspxe-gui</code> (GUI version) or <code>inspxe-cl</code> (Command Line version) command.</li>
	<li>Module: <code>Inspector/&lt;toolchain version&gt;</code>, e.g., <code>Inspector/2017a</code>.</li>
	<li>Documentation:
	<ul>
		<li><a href=\"https://software.intel.com/en-us/node/595380\">Getting started with Intel Inspector - Linux OS</a></li>
		<li><a href=\"https://software.intel.com/en-us/intel-inspector-xe-support/training\">Intel Online Training material</a></li>
		<li><a href=\"https://software.intel.com/en-us/inspector-user-guide-linux\">Intel Inspector Help - Linux OS</a></li>
		<li>Further Intel Inspector documentation in the <a href=\"https://software.intel.com/en-us/documentation\">Intel Software Documentation Library</a></li>
		<li>Manual page for <code>inspxe-cl</code></li>
	</ul></li>
</ul>"
749,"","<p>The third VSC Users Day was held at the \"Paleis der Academiën\", the seat of the \"Royal Flemish Academy of Belgium for Science and the Arts\", in the Hertogstraat 1, 1000 Brussels, on June 2, 2017.
</p><h2>Program</h2><ul>
	<li>  9u50 : welcome</li>
	<li>10u00: dr. Achim Basermann, German Aerospace Center, High Performance Computing with Aeronautics and Space Applications [<a href=\"/assets/1225 \">slides PDF - 4,9MB</a>] </li>
	<li>11u00: coffee break</li>
	<li>11u30: workshop sessions – part 1
	<ul>
		<li>VSC for starters (by VSC personnel) [<a href=\"/assets/1215\">slides PDF - 5.3MB</a>]</li>
		<li>Profiler and Debugger (by VSC personnel) [<a href=\"/assets/1217 \">slides PDF - 2,2MB</a>]</li>
		<li>Programming GPUs (dr. Bart Goossens - dr. Vule Strbac)</li>
	</ul></li>
	<li>12u45: lunch</li>
	<li>14u00: dr. Ehsan Moravveji, KU Leuven A Success Story on Tier-1: A Grid of Stellar Models [<a href=\"/assets/1219\">slides - PDF 11,9MB</a>]</li>
	<li>14u30: ‘1-minute’ poster presentations</li>
	<li>15u00: workshop sessions – part 2
	<ul>
		<li>VSC for starters (by VSC personnel)</li>
		<li>Profiler and Debugger (by VSC personnel)</li>
		<li>Feedback from Tier-1 Evaluation Committee (dr. Walter Lioen, chairman) [<a href=\"/assets/1221\">slides - PDF 0.5MB</a>]</li>
	</ul></li>
	<li>16u00: coffee and poster session</li>
	<li>17u00: drink</li>
</ul><p><i><strong>Abstracts of workshops</strong></i>
</p><ul>
	<li><strong>VSC for starters</strong> [<a href=\"/assets/1215\">slides PDF - 5.3MB</a>]<br> The workshop provides a smooth introduction to supercomputing for new users. Starting from common concepts in personal computing the similarities and differences with supercomputing are highlighted and some essential terminology is introduced. It is explained what users can expect from supercomputing and what not, as well as what is expected from them as users</li>
	<li><strong><strong></strong>Profiler and Debugger</strong> [<a href=\"/assets/1217 \">slides PDF - 2,2MB</a>]<br>Both profiling and debugging play an important role in the software development process, and are not always appreciated. In this session we will introduce profiling and debugging tools, but the emphasis is on methodology.  We will discuss how to detect common performance bottlenecks, and suggest some approaches to tackle them. For debugging, the most important point is avoiding bugs as much as possible.</li>
	<li><strong>Programming GPUs</strong>
	<ul>
		<li><strong>Quasar, a high-level language and a development environment to reduce the complexity of heterogeneous programming of CPUs and GPUs</strong>,<em> Prof dr. Bart Goosens, UGent </em>[<a href=\"/assets/1227 \">slides PDF - 2,1MB</a>]<br>In this workshop we present Quasar, a new programming framework that takes care of many common challenges for GPU programming, e.g., parallelization, memory management, load balancing and scheduling. Quasar consists of a high-level programming language with a similar abstraction level as Python or Matlab, making it well suited for rapid prototyping. We highlight some of the automatic parallelization strategies of Quasar and show how high-level code can efficiently be compiled to parallel code that takes advantage of the available CPU and GPU cores, while offering a computational performance that is on a par with a manual low-level C++/CUDA implementation. We explain how multi-GPU systems can be programmed from Quasar and we demonstrate some recent image processing and computer vision results obtained with Quasar.</li>
		<li><strong>GPU programming opportunities and challenges: nonlinear finite element analysis</strong>,<em> dr. Vule Strbac, KU Leuven </em>[<a href=\"/assets/1223 \">slides PDF - 2,1MB</a>]<br>From a computational perspective, finite element analysis manifests substantial internal parallelism. Exposing and exploiting this parallelism using GPUs can yield significant speedups against CPU execution. The details of the mapping between a requested FE scenario and the hardware capabilities of the GPU device greatly affect this resulting speedup. Factors such as: (1) the types of materials present (elasticity), (2) the local memory pool and (3) fp32/fp64 computation impact GPU solution times differently than their CPU counterparts.<br>We present results of both simple and complex FE analyses scenarios on a multitude of GPUs and show an objective estimation of general performance. In doing so, we detail the overall opportunities, challenges as well as the limitations of the GPU FE approach.
		</li>
	</ul></li>
</ul><p><strong><em>Poster sessions</em></strong>
</p><p>An overview of the posters that were presented during the poster session is available <a href=\"/events/userday-2017/posters\">here</a>.
</p>"
751,"","<p><iframe src=\"https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d5930.298065013993!2d4.360466631251976!3d50.84147856619101!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x47c3c48409f9587f%3A0xb74dfdbd0226f3dd!2sPaleis+der+Academi%C3%ABn!5e0!3m2!1snl!2sus!4v1493214716676\" width=\"600\" height=\"450\" frameborder=\"0\" allowfullscreen=\"\" style=\"border-width: 0px; border-style: initial;\"></iframe><br></p><ul>
	<li><strong>By train:</strong> The closest railway station is Brussel-Centraal/Bruxelles Central. From there it is a ten minutes walk to the venue, or you can take the metro.</li>
	<li><strong>By metro (MIVB):</strong> Metro station Troon
	<ul>
		<li>From the Central Station: Line 1 or 5 till Kunst-Wet, then line 2 or 6.</li>
		<li>From the North Station: metro Rogier, line 2 or 6 towards \"Koning Boudewijn\" or \"Simonis (Leopold II)\".</li>
		<li>From the South Station: Line 2 or 6 towards Simonis (Elisabeth)</li>
	</ul></li>
	<li><strong>By car:</strong>
	<ul>
		<li>The \"Paleis der Academiën\" has two free parking areas at the side of the \"Kleine ring\". Access is via the Regentlaan which you should enter at the Belliardstraat.</li>
		<li>There are limited non-free parking spots at the Regentlaan or the Paleizenplein</li>
		<li>Two nearby parking garages are:
		<ul>
			<li>Parking 2 Portes: Waterloolaan 2a, 1000 Brussel</li>
			<li>Parking Industrie: Industriestraat 26-38, 1040 Brussel</li>
		</ul></li>
	</ul></li>
</ul>"
753,"","<h2>Important changes</h2><p>The 2017a toolchain is the toolchain that
will be carried forward to Leibniz and will be available after the operating
system upgrade of Hopper. Hence it is meant to be as complete as possible. We
will only make a limited number of programs available in the 2016b toolchain
(basically those that show much better performance with the older compiler or
that do not compile with the compilers in the 2017a toolchains).
</p><p>Important changes
in the 2017a toolchain:
</p><ul>
	<li>The Intel compilers have been installed in
a single directory tree, much the way Intel intends the install to be done. The
intel/2017a module loads fewer submodules and instead sets all required
variables. The install now also contains the Thread Building Blocks (TBB),
Integrated Performance Primitives (IPP) and Data Analytics Acceleration Library
(DAAL). All developer tools (debugger, Inspector, Advisor, Vtune
Amplifier, ITAC) are enabled by loading the inteldevtools/2017a module rather
than independent modules for each tool. More information is available on 
	<a href=\"/infrastructure/hardware/hardware-ua/intel\">the documentation page on the Intel compilers @ UAntwerp</a>.</li>
	<li>The Python install now also contains a
number of packages that previously where accessed via separate modules:
	<ul>
		<li>matplotlib, so there is no longer a
separate module to load matplotlib. 
		</li>
		<li>lxml</li>
	</ul>
	</li>
	<li>The R install now also contains a selection
of the Bioconductor routines, so no separate module is needed to enable the
latter.
	</li>
	<li>netCDF is now a single module containing
all 4 interfaces rather than 4 separate modules that installed each interface
in a different directory tree (three of which all relied on the module for the
fourth). This should ease the installation of code that uses the netCDF Fortran
or one of the C++ interfaces and expects all netCDF libraries to be installed
in the same directory.
	</li>
</ul><p>We will skip the 2017b toolchain as defined by the VSC as we have already upgraded the 2017a toolchain to a more recent update of the Intel 2017 compilers to avoid problems with certain applications.
</p><h2>Available toolchains</h2><p>There are
currently three major toolchains on the UAntwerp clusters:
</p><ul>
	<li>The Intel toolchain,
which includes the Intel compilers and tools, matching versions of the GNU
compilers, and all software compiled with them.
	<ul>
		<li><a href=\"/infrastructure/hardware/hardware-ua/toolchain-2017a-intel\">Modules in the intel/2017a toolchain</a></li>
	</ul></li>
	<li>The FOSS toolchain,
built out of open-source components. It is mostly used for programs that don’t
install with the Intel compilers, or by users who want to do development with
Open MPI and other open-source libraries.
	<br>
	The FOSS-toolchain has a number of subtoolchains: Gompi, GCC and GCCcore, and
some programs are installed in these subtoolchains because they don’t use the
additional components that FOSS offers.
	<ul>
		<li><a href=\"/infrastructure/hardware/hardware-ua/toolchain-2017a-foss\">Modules in the foss/2017a toolchain and its subtoolchains</a></li>
	</ul></li>
	<li>The system toolchain
(sl6 or centos7), containing programs that only use system libraries or other
tools from this toolchain.
	<ul>
		<li><a href=\"/infrastructure/hardware/hardware-ua/toolchain-system-centos7\">Modules in the system toolchain for CentOS 7</a></li>
	</ul></li>
</ul><p>The tables below
list the last available module for a given software package and the
corresponding version in the 2017a toolchain. Older versions can only be
installed on demand with a very good motivation, as older versions of packages
also often fail to take advantage of advances in supercomputer architecture and
offer lower performance. Packages that have not been used recently will
only be installed on demand.
</p><p>Several of the
packages in the system toolchain are still listed as “on demand” since they
require licenses and interaction with their users is needed before we can
install them.
</p>"
755,"","<p>Several of the software packages running on the UAntwerp cluster have restrictions in their licenses and cannot be used by all users. If a module does not load, it is very likely that you have no access to the package.</p><p>Access to such packages is managed by UNIX groups. You can request membership to the group, but that membership will only be granted if you are eligible for use of the package.</p><h2>ANSYS</h2><h2>CPMD</h2><p><a href=\"http://www.cpmd.org/\">CPMD</a> can be used for free for non-commercial research in education institutions under the <a href=\"http://cpmd.org/download/cpmd-licence\">CPMD Free License</a>.  </p><p>To get access:</p><ul><li><a href=\"http://cpmd.org/download/accept-license/cpmd-license-application\">Fill in the form for downloading CPMD</a> on the CPMD website. </li><li>You'll see a \"Thank You\" page confirming your submission. Somewhat later - it may take up to a week but usually it is quite fast - you'll receive a mail with download instructions. Please forward that mail to <a href=\"mailto:hpc@uantwerpen.be\">hpc@uantwerpen.be</a>. We then have to check with IBM for confirmation.</li><li>Apply for membership of the acpmd group via the <a href=\"https://account.vscentrum.be/\">VSC account managment webpage</a>.</li><li>As soon as we have confirmation from IBM that your license application has been accepted, your membership application for acpmd will be granted and you will be able to use CPMD.</li></ul><h2>COMSOL</h2><h2>FINE/Marine</h2><p>FINE/Marine is commercial CFD software from NUMECA International for simulation of flow around ships etc. The license has been granted for use of the Solar Boat Team as sponsoring from NUMECA and cannot be used by others. </p><h2>Gaussian</h2><p>To use Gaussian, you should work or study at the University of Antwerp and your research group should contribute to the cost of the license.</p><p>Contact <a href=\"https://www.uantwerpen.be/en/staff/wouter-herrebout/\">Wouter Herrebout</a> for more information.</p><h2>Gurobi</h2><h2>MATLAB</h2><p>We do not encourage the use of Matlab on the cluster as it is neither designed for use of HPC (despite a number of toolboxes that support parallel computing) nor efficient. </p><p>Matlab on the UAntwerp clusters can be used by everybody who can legally use Matlab within the UAntwerp Campus Agreement with The Mathworks. You should have access to the modules if you are eligible. If you cannot load the Matlab modules yet think you are allowed to use Matlab under the UAntwerp license, please <a href=\"/support/contact-support\">contact support</a>. </p><h2>TurboMole</h2><h2>VASP</h2>"
759,"","<p>You may notice that leibniz is not always faster than hopper, and this is a trend that we expect to continue for the following clusters also. In the past five years, individual cores did not become much more efficient on a instructions-per-clock cycle basis. Instead, faster chips were build by including more cores, though at a lower clock speed to stay within the power budget for a socket, and new vectori instructions.</p><p>Compared to hopper,</p><ul><li>The clock speed of each core is a bit lower (2.4 GHz base frequency instead of 2.8 GHz), and this is not for all applications compensated by the slightly higher instructions-per-clock,</li><li>But there are now 14 cores per socket rather than 10 (so 28 per node rather than 20),</li><li>And there are some new vector instructions that were not present on hopper (AVX2 with Fused Multiply-Add rather than AVX)..</li></ul><p>For programs that manage to use all of this, the peak performance of a node is effectively about twice as high as for a node on hopper. But single core jobs with code that does not use vectorization may very well run slower. </p><h2>Module system</h2><p>We use different software for managing modules on leibniz (Lmod instead of TCL-based modules). The new software supports the same commands as the old software, and more.
</p><ul>
	<li>Lmod does not allow loading multiple modules with the same name, while in the TCL-based modules it was up to the writer of each module file to determine if this was possible or not. As a consequence, while on hopper one could in principle customize the module path by loading multiple hopper modules, this is not the case on leibniz. Instead, we are providing leibniz modules for loading a specific toolchain (e.g., leibniz/2017a), one to load only software compiled against OS libraries (leibniz/centos7) and one to load all supported modules (leibniz/all).<br>As we are still experimenting with the setup of the module system, the safest thing to do is to always explicitly load the right version of the leibniz module.</li>
	<li>An interesting new command is \"module spider Python\" which will show all modules named Python, or with Python as part of their name. Moreover, this search is not case-sensitive so it is a very good way to figure out which module name to use if you are not sure about the capitalization.</li>
</ul><h2>Job submission</h2><p>One important change is that the new version of the operating system (CentOS 7.3, based on Red Hat 7) combined with our job management software allows much better control of the amount of memory that a job uses. Hence we can better protect the cluster against jobs that use more memory than requested. This is particularly important since leibniz does not support swapping on the nodes. This choice was made deliberatly as swapping to hard disk slows down a node to a crawl while SSDs that are robust enough to be used for swapping also cost a lot of money (memory cells on cheap SSDs can only be written a few 100 times, sometimes as little as 150 times). Instead, we increased the amount of memory available to each core. The better protection of jobs against each other may also allow us to consider to set apart some nodes for jobs that cannot fill a node and then allow multiple users on that node, rather than have those nodes used very inefficiently while other users are waiting for resources as is now the case.</p><ul><li> Torque distinguishes between two kinds of memory:<ul><li>Resident memory, essentially RAM, which is requested through pmem and mem.</li><li>Virtual memory, which is the total amount of memory space requested, consisting of both resident memory and swap space, is requested through pvmem and vmem.</li></ul></li><li>mem and vmem specify memory for the job as a whole. The Torque manual discourages to use it for multi-node jobs though in the current version it works fine in most but not all cases, and evenly distributes the requested memory pool across cores.</li><li>pmem and pvmem specify memory per core.</li><li>It is better not to mix pmem/pvmem with mem/vmem as this can lead to confusing situations, though it does work.Torque will generally use the least restrictive of (mem,pmem) and of (vmem,pvmem) respectively for resident and virtual memory.</li><li>Of course pvmem should not be smaller than pmem, and vmem should not be smaller than mem. Otherwise the job will be refused.</li><li>We will set small defaults for users who do not specify this (to protect the system) but we have experienced that qsub will hang if a user makes a request that conflicts with the defaults (e.g., if we set a default for pvmem and a user uses a value for pmem which is larger than this value but does not specify pvmem, qsub will hang without producing the error message that one would expect).</li><li>Our advise for now: Use both pmem and pvmem and set both to the same value, as we do not support swapping anyway.</li></ul><h2>MPI jobs</h2><ul><li>We are still experiencing problems with Open MPI (FOSS toolchain).</li><li>With respect to Intel MPI: We are experimenting with a different way to let Intel MPI start programs (though through the same commands as before). The problem with Intel MPI on hopper was that processes on other nodes than the start node did not run under the control of the job management system. As a result, the CPU times and efficiencies computed by Torque and Moab were wrong, cleanup of failed jobs did not always fully work and resource use in general was not properly monitored. However, we are not sure yet that the new mechanism is robust enough, so let us know if large jobs do not start so that we can investigate what happened.<br>Technical note: The basic idea is that we let mpirun start processes on other nodes through the Torque job management library and not through ssh, but this is all accomplished through a number of environment variables set in the intel modules.</li></ul>"
761,"","<p><strong></strong><em><strong>Poster sessions</strong></em></p><ol><li><em>Computational study of the properties of defects at grain boundaries in CuInSe2<br></em>R. Saniz, J. Bekaert, B. Partoens, and D. Lamoen<br>CMT and EMAT groups, Dept. of Physics, U Antwerpen</li><li><em>First-principles study of superconductivity in atomically thin MgB2<br></em>J. Bekaert, B. Partoens, M. V. Milosevic, A. Aperis, P. M. Oppeneer<br>CMT group, Dept. of Physics, U Antwerpen & Dept. of Physics and Astronomy, Uppsala University</li><li><em>Molecular Spectroscopy : Where Theory Meets Experiment<br></em>C. Mensch, E. Van De Vondel, Y. Geboes, J. Bogaerts, R. Sgammato, E. De Vos, F. Desmet, C. Johannessen, W. Herrebout<br>Molecular Spectroscopy group, Dept. Chemistry, U Antwerpen</li><li><em>Bridging time scales in atomistic simulations: from classical models to density functional theory<br></em>Kristof M. Bal and Erik C. Neyts<br>PLASMANT, Department of Chemistry, U Antwerpen</li><li><em>Bimetallic nanoparticles: computational screening for chirality-selective carbon nanotube growth<br></em>Charlotte Vets and Erik C. Neyts<br>PLASMANT, Department of Chemistry, U Antwerpen</li><li><em>Ab initio molecular dynamics of aromatic sulfonation with sulfur trioxide reveals its mechanism<br></em>Samuel L.C. Moors, Xavier Deraet, Guy Van Assche, Paul Geerlings, Frank De Proft<br>Quantum Chemistry Group, Department of Chemistry, VUB</li><li><em>Acceleration of the Best First Search Algorithm by using predictive analytics<br></em>J.L. Teunissen, F. De Vleeschouwer, F. De Proft<br>Quantum Chemistry Group, VUB, Department of Chemistry, VUB</li><li><em>Investigating molecular switching properties of octaphyrins using DFT<br></em>Tatiana Woller, Paul Geerlings, Frank De Proft, Mercedes Alonso<br>Quantum Chemistry Group, VUB, Department of Chemistry, VUB</li><li><em>Using the Tier-1 infrastructure for high-resolution climate modelling over Europe and Central Asia<br></em>Lesley De Cruz, Rozemien De Troch, Steven Caluwaerts, Piet Termonia, Olivier Giot, Daan Degrauwe, Geert Smet, Julie Berckmans, Alex Deckmyn, Pieter De Meutter, Luc Gerard, Rafiq Hamdi, Joris Van den Bergh, Michiel Van Ginderachter, Bert Van Schaeybroeck<br>Department of Physics and Astronomy, U Gent</li><li><em>Going where the wind blows – Fluid-structure interaction simulations of a wind turbine<br></em>Gilberto Santo, Mathijs Peeters, Wim Van Paepegem, Joris Degroote<br>Dept. of Flow, Heat and Combustion Mechanics, U Gent</li><li><em>Towards Crash-Free Drones – A Large-Scale Computational Aerodynamic Optimization<br></em>Jolan Wauters, Joris Degroote, Jan Vierendeels<br>Dept. of Flow, Heat and Combustion Mechanics, U Gent</li><li><em>Characterisation of fragment binding to TSLPR using molecular dynamics<br></em>Dries Van Rompaey, Kenneth Verstraete, Frank Peelman, Savvas N. Savvides, Pieter Van Der Veken, Koen Augustyns, Hans De Winter<br>Medicinal Chemistry, UAntwerpen and Center for Inflammation Research , VIB-UGent</li><li><em>A hybridized DG method for unsteady flow problems<br></em>Alexander Jaust, Jochen Schütz<br>Computational Mathematics (CMAT) group, U Hasselt</li><li><em>HPC-based materials research: From Metal-Organic Frameworks to diamond<br></em>Danny E. P. Vanpoucke, Ken Haenen<br>Institute for Materials Research (IMO), UHasselt & IMOMEC, IMEC</li><li><em>Improvements to coupled regional climate model simulations over Antarctica<br></em>Souverijns Niels, Gossart Alexandra, Demuzere Matthias, van Lipzig Nicole<br>Dept. of Earth and Environmental Sciences, KU Leuven</li><li><em>Climate modelling of Lake Victoria thunderstorms<br></em>Wim Thiery, Edouard L. Davin, Sonia I. Seneviratne, Kristopher Bedka, Stef Lhermitte, Nicole van Lipzig<br>Dept. of Earth and Environmental Sciences, KU Leuven</li><li><em>Improved climate modeling in urban areas in sub Saharan Africa for malaria epidemiological studies<br></em>Oscar Brousse, Nicole Van Lipzig, Matthias Demuzere, Hendrik Wouters, Wim Thiery<br>Dept. of Earth and Environmental Sciences, KU Leuven</li><li><em>Adaptive Strategies for Multi-Index Monte Carlo<br></em>Dirk Nuyens, Pieterjan Robbe, Stefan Vandewalle<br>NUMA group, Dept. of Computer Science, KU Leuven</li><li><em>SP-Wind: A scalable large-eddy simulation code for simulation and optimization of wind-farm boundary layers<br></em>Wim Munters, Athanasios Vitsas, Dries Allaerts, Ali Emre Yilmaz, Johan Meyers<br>Turbulent Flow Simulation and Optimization (TFSO) group, Dept. of Mechanics, KU Leuven</li><li><em>Control Optimization of Wind Turbines and Wind Farms<br></em>Ali Emre Yilmaz, Wim Munters, Johan Meyers<br>Turbulent Flow Simulation and Optimization (TFSO) group, Dept. of Mechanics, KU Leuven</li><li><em>Simulations of large wind farms with varying atmospheric complexity using Tier-1 Infrastructure<br></em>Dries Allaerts, Johan Meyers<br>Turbulent Flow Simulation and Optimization (TFSO) group, Dept. of Mechanics, KU Leuven</li><li><em>Stability of relativistic, two-component jets<br></em>Dimitrios Millas, Rony Keppens, Zakaria Meliani<br>Plasma-astrophysics, Dept. Mathematics, KU Leuven</li><li><em>HPC in Theoretical and Computational Chemistry <br></em><i>Jeremy Harvey, Eliot Boulanger, Andrea Darù, Milica Feldt, Carlos Martín-Fernández, Ana Sanz Matias, Ewa Szlapa </i><br>Quantum Chemistry and Physical Chemistry Section, <i></i>Dept. of Chemistry, KU Leuven</li></ol>"
765,"","<p>The third VSC Users Day was held at the \"Paleis der Academiën\", the seat of the \"Royal Flemish Academy of Belgium for Science and the Arts\", in the Hertogstraat 1, 1000 Brussels, on June 2, 2017.
</p><h2>Program</h2><ul>
	<li>  9u50 : welcome</li>
	<li>10u00: dr. Achim Basermann, German Aerospace Center, High Performance Computing with Aeronautics and Space Applications [<a href=\"/assets/1225 \">slides PDF - 4,9MB</a>] </li>
	<li>11u00: coffee break</li>
	<li>11u30: workshop sessions – part 1
	<ul>
		<li>VSC for starters (by VSC personnel) [<a href=\"/assets/1215\">slides PDF - 5.3MB</a>]</li>
		<li>Profiler and Debugger (by VSC personnel) [<a href=\"/assets/1217 \">slides PDF - 2,2MB</a>]</li>
		<li>Programming GPUs (dr. Bart Goossens - dr. Vule Strbac)</li>
	</ul></li>
	<li>12u45: lunch</li>
	<li>14u00: dr. Ehsan Moravveji, KU Leuven A Success Story on Tier-1: A Grid of Stellar Models [<a href=\"/assets/1219\">slides - PDF 11,9MB</a>]</li>
	<li>14u30: ‘1-minute’ poster presentations</li>
	<li>15u00: workshop sessions – part 2
	<ul>
		<li>VSC for starters (by VSC personnel)</li>
		<li>Profiler and Debugger (by VSC personnel)</li>
		<li>Feedback from Tier-1 Evaluation Committee (dr. Walter Lioen, chairman) [<a href=\"/assets/1221\">slides - PDF 0.5MB</a>]</li>
	</ul></li>
	<li>16u00: coffee and poster session</li>
	<li>17u00: drink</li>
</ul><p><i><strong>Abstracts of workshops</strong></i>
</p><ul>
	<li><strong>VSC for starters</strong> [<a href=\"/assets/1215\">slides PDF - 5.3MB</a>]<br> The workshop provides a smooth introduction to supercomputing for new users. Starting from common concepts in personal computing the similarities and differences with supercomputing are highlighted and some essential terminology is introduced. It is explained what users can expect from supercomputing and what not, as well as what is expected from them as users</li>
	<li><strong><strong></strong>Profiler and Debugger</strong> [<a href=\"/assets/1217 \">slides PDF - 2,2MB</a>]<br>Both profiling and debugging play an important role in the software development process, and are not always appreciated. In this session we will introduce profiling and debugging tools, but the emphasis is on methodology.  We will discuss how to detect common performance bottlenecks, and suggest some approaches to tackle them. For debugging, the most important point is avoiding bugs as much as possible.</li>
	<li><strong>Programming GPUs</strong>
	<ul>
		<li><strong>Quasar, a high-level language and a development environment to reduce the complexity of heterogeneous programming of CPUs and GPUs</strong>,<em> Prof dr. Bart Goosens, UGent </em>[<a href=\"/assets/1227 \">slides PDF - 2,1MB</a>]<br>In this workshop we present Quasar, a new programming framework that takes care of many common challenges for GPU programming, e.g., parallelization, memory management, load balancing and scheduling. Quasar consists of a high-level programming language with a similar abstraction level as Python or Matlab, making it well suited for rapid prototyping. We highlight some of the automatic parallelization strategies of Quasar and show how high-level code can efficiently be compiled to parallel code that takes advantage of the available CPU and GPU cores, while offering a computational performance that is on a par with a manual low-level C++/CUDA implementation. We explain how multi-GPU systems can be programmed from Quasar and we demonstrate some recent image processing and computer vision results obtained with Quasar.</li>
		<li><strong>GPU programming opportunities and challenges: nonlinear finite element analysis</strong>,<em> dr. Vule Strbac, KU Leuven </em>[<a href=\"/assets/1223 \">slides PDF - 2,1MB</a>]<br>From a computational perspective, finite element analysis manifests substantial internal parallelism. Exposing and exploiting this parallelism using GPUs can yield significant speedups against CPU execution. The details of the mapping between a requested FE scenario and the hardware capabilities of the GPU device greatly affect this resulting speedup. Factors such as: (1) the types of materials present (elasticity), (2) the local memory pool and (3) fp32/fp64 computation impact GPU solution times differently than their CPU counterparts.<br>We present results of both simple and complex FE analyses scenarios on a multitude of GPUs and show an objective estimation of general performance. In doing so, we detail the overall opportunities, challenges as well as the limitations of the GPU FE approach.
		</li>
	</ul></li>
</ul><p><strong><em>Poster sessions</em></strong>
</p><p><a href=\"/events/userday-2017/posters\">An overview of the posters that were presented during the poster session is available here</a>.
</p>"
769,"","<p><a href=\"/events/userday-2017/photos\">Other pictures of the VSC User Day 2017</a>.</p>"
773,"","<p>Below is a selection of photos from the user day 2017. A larger set of photos at a higher resolution can be <a href=\"/assets/1291\">downloaded as a zip file (23MB)</a>.
</p>"
777,"","<p>The UAntwerp clusters have limited features for remote visualization on the login nodes of hopper and the visualization node of leibniz using a <a href=\"https://en.wikipedia.org/wiki/Virtual_Network_Computing\">VNC-based remote display technology</a>. On the regular login nodes of hopper, there is no acceleration of 3D graphics, but the visualisation node of leibniz is equipped with a NVIDIA M5000 card that when used properly will offer accelerated rendering of OpenGL applications. The setup is similar to the setup of <a href=\"/client/multiplatform/turbovnc\">the visualization nodes at the KU Leuven</a>.
</p><p><em>Using VNC turns out to be more complicated than one would think and things sometimes go wrong. It is a good solution for those who absolutely need a GUI tool or a visualization tool on the cluster rather than on your local desktop; it is not a good solution for those who don't want to invest in learning Linux properly and are only looking for the ease-of-use of a PC.</em>
</p><h2>The idea behind the setup</h2><h3>2D and 3D graphics on Linux</h3><p>Graphics (local and remote) on Linux-machines is based on the <a href=\"https://en.wikipedia.org/wiki/X_Window_System\">X Window System version 11</a>, shortly X11. This technology is pretty old (1987) and nor really up to the task anymore with todays powerful computers yet has so many applications that support it that it is still the standard in practice (though there are efforts going on to replace it with Wayland on modern Linux systems).
</p><p>X11 applications talk to a X server which draws the commands on your screen. These commands can go over a network so applications on a remote machine can draw on your local screen. Note also the somewhat confusing terminology: The server is the program that draws on the screen and thus runs on your local system (which for other applications will usually be called the client) while the application is called the client (and in this scenario runs on a computer which you will usually call the server). However, partly due to the way the X11 protocol works and partly also because modern applications are very graphics-heavy, the network has become a bottleneck and graphics-heavy applications (e.g., the Matlab GUI) will work sluggish on all but the fastest network connections.
</p><p>X11 is a protocol for 2D-graphics only. however, it is extensible. Enter <a href=\"https://en.wikipedia.org/wiki/OpenGL\">OpenGL</a>, a standard cross-platform API for professional 3D-graphics. Even though its importance on Windows and macOS platforms had decreased as Microsoft and Apple both promote their own APIs (DirectX and Metal respectively), it is still very popular for professional applications and in the Linux world. It is supported by X11 servers through <a href=\"https://en.wikipedia.org/wiki/GLX\">the GLX-extension</a> (OpenGL for the X Window System). When set up properly, OpenGL commands can be passed to the X server and use any OpenGL graphics accelerator available on the computer running the X server. In principle, if you have a X server with GLX extension on your desktop, you should be able to run OpenGL programs on the cluster and use the graphics accelerator of your desktop to display the graphics. In practice however this works well when the application and X server run on the same machine, but the typical OpenGL command stream is to extensive to work well over a network connection and performance will be sluggish.
</p><h3>Optimizing remote graphics</h3><p>The solution offered on the visualization node of leibniz (and in a reduced setting on the login nodes of hopper) consists of two elements to deal with the issues of network bandwidth and, more importantly, network latency.
</p><p><a href=\"https://en.wikipedia.org/wiki/VirtualGL\">VirtualGL</a> is a technology that redirects OpenGL commands to a 3D graphics accelerator on the computer where the application is running or to a sofware rendering library. It then pushes the rendered image to the X server. Instead of a stream of thousands or millions of OpenGL commands, one large image is now passed over the network to the X server, reducing the effect of latency. These images can be large though, but with an additional piece of software on your client, called the VGL client, VirtualGL can send the images in compressed form which strongly reduces the bandwidth requirements. To use VirtualGL, you have to start the OpenGL application through the vglrun-command. That command will set up the application to redirect OpenGL calls to the VirtualGL libraries.
</p><p>VirtualGL does not solve the issue of slow 2D-rendering because of network latency and also requires the user to set up a VGL client and an X server on the local desktop, which is cumbersome for less experienced users. We solve this problem through <a href=\"https://en.wikipedia.org/wiki/Virtual_Network_Computing\">VNC (Virtual Network Computing)</a>. VNC consists of three components: a server on the computer where your application runs, a client on your desktop, and a standardized protocol for the communication between server and client. The server renders the graphics on the computer on which it runs and sends compressed images to the client. The client of course takes care of keyboard and mouse input and sends this to the server. A VNC server for X applications will in fact emulate a X server. Since the protocol between client and server is pretty standard, most clients will work with most servers, though some combinations of client and server will be more efficient because they may support a more efficient compression technology. Our choice of server is <a href=\"https://www.turbovnc.org/\">TurboVNC</a> which is maintained by the same group that also develops VirtualGL and has an advanced implementation of a compression algorithm very well suited for 3D graphics. TurboVNC has clients for Windows, macOS and Linux. However, our experience is that it also works with several other VNC clients (e.g., Apple Remote Desktop), though it may be a bit less efficient as it may not be able to use the best compression strategies.
</p><h3>The concept of a Window Manager</h3><p>When working with Windows or macOS, we're used to seeing a title bar for most windows with buttons to maximize or hide the window, and borders that allow to resize a window. You'd think this functionality is provided by the X server, but in true UNIX-spirit of having separate components for every bit of functionality, this is not the case. On X11, this functionality is provided by the Window Manager, a separate software package that you start after starting the X server (or may be started for you automatically by the startup script that is run when starting the X server). The basic window managers from the early days of X11 have evolved into feature-rich desktop enviroments that do not only offer a window manager, but also a task bar etc. Gnome and KDE are currently the most popular desktop environments (or Unity on Ubuntu, but future editions of Ubuntu will return to Gnome). However, these require a lot of resources and are difficult to install on top of TurboVNC. Examples of very basic old-style window managers are the <a href=\"https://en.wikipedia.org/wiki/Twm\">Tab Window Manager</a> (command <code>twm</code>) and the <a href=\"https://en.wikipedia.org/wiki/Motif_Window_Manager\">Motif Window Manager</a> (command <code>mwm</code>). (Both are currently available on the login nodes of hopper.)
</p><p>For the remote visualization setup on the UAntwerp clusters, we have chosen to use the Xfce Desktop Environment which is definitely more user-friendly than the rather primitive Tab Window Manager and Motif Window Manager, yet requires less system resources and is easier to set up than the more advanced Gnome and KDE desktops.
</p><h2>Prerequisites</h2><p>You'll need a ssh client on your desktop that provides port forwarding functionality on your desktop. We refer to the \"<a href=\"/cluster-doc/access-data-transfer\">Access and data transfer</a>\" section of the documentation on the user portal for information about ssh clients for various client operating systems. PuTTY (Windows) and OpenSSH (macOS, Linux, unix-compatibility environment on Windows) both provide all required functionality.
</p><p>Furthermore, you'll need a VNC client, preferably the TurboVNC client.
</p><h3>Windows</h3><p>We have tested the setup with three different clients:
</p><ul>
	<li>The <a href=\"https://turbovnc.org/\">TurboVNC</a> client can be downloaded by following the Download link on <a href=\"https://www.turbovnc.org/\">the TurboVNC web site</a> (which at the moment of writing this documentation takes you to <a href=\"https://sourceforge.net/projects/turbovnc/files/\">a sourceforge page</a>). Binaries are available for both 32-bit and 64-bit windows systems. This client is made by the same people as the server we use so in theory one should expect the least problems with this setup.</li>
	<li><a href=\"http://tigervnc.org/\">TigerVNC</a> is a client whose development is supported by the Swedish company Cendio who makes a remote display server product (ThinLinc) based on TigerVNC. Binaries for 32-bit and 64-bit windows (vncviewr-*.*.*.exe) can be downloaded by following the link on <a href=\"https://github.com/TigerVNC/tigervnc/releases\">the GitHub Releases page</a>. These binaries are ready-to-run.</li>
	<li><a href=\"http://www.tightvnc.com/\">ThightVNC</a> is also a popular free VNC implementation. 32-bit and 64-bit Windows installers can be downloaded from <a href=\"http://www.tightvnc.com/download.php\">the download page on their website</a>. When installing on your PC or laptop, make sure to chose the \"custom install\" and only install the TightVNC Viewer.</li>
</ul><p>All three viewers are quite fast and offer good performance, even when run from home over a typical broadband internet connection. TigerVNC seems to be a bit quicker than the other two, while TightVNC doesn't allow you to resize your window. With the other two implementations, when you resize your desktop window, the desktop is also properly resized.
</p><h3>macOS</h3><p>Here also there are several possible setups:</p><ul><li>The TurboVNC client can be downloaded from <a href=\"https://www.turbovnc.org/\">the TurboVNC web site</a>. The macOS client is Java-based. Packages are available for both Apple Java on older versions of OS X and Oracle Java (which you will need to install if it is not yet on your system). We advise to use the Oracle Java version as Java needs frequent security updates and Apple Java is no longer maintained.</li><li><a href=\"https://tigervnc.org/\">TigerVNC</a>, a client whose development is supported by the Swedish company Cendio who makes a remote display server product (ThinLinc) based on TigerVNC, is a native macOS client. At the time of writing (version 1.9.0), it is still only distributed as a 32-bit binary so you may get warnings on some versions of macOS. However, there already exist 64-bit pre-release builds so future versions will certainly fully support future macOS versions. Some places report that this client is a lot slower than the the TurboVNC one on macOS. <br><a href=\"https://bintray.com/tigervnc/stable/tigervnc/\">Binaries are available</a>. Look for the tigervnc-*.dmg files, which contrary to those for Windows and Linux, only contain the viewer software.</li><li>A not-so-good alternative is to use the Apple Screen Sharing feature which is available through the Finder (command-K key combination) or Safari (URL bar) by specifying the server as a URL starting with svn://. This VNC client is considerably slower though than the TurboVNC client, partly because it doesn't support some of the TurboVNC-specific compression algorithms.</li></ul><h3>Linux</h3><p>RPM and Debian packages for TurboVNC can be downloaded from <a href=\"https://www.turbovnc.org/\">the TurboVNC web site</a> and are available in some Linux distributions. You can also try another VNC client provided by your Linux distribution at your own risk as we cannot guarantee that all VNC viewers (even recent ones) work eficiently with TurboVNC.
</p><h2>How do I run an application with TurboVNC?</h2><p>Running an application with TurboVNC requires 3 steps:
</p><ul>
	<li>Start the VNC server on the cluster</li>
	<li>Start the VNC client on your desktop/laptop and connect to the server</li>
	<li>Start your application</li>
</ul><h3>Starting the server</h3><ol>
	<li>Log on in the regular way to one of the login nodes of hopper or to the visualization node of Leibniz. Note that the latter should only be used for running demanding visualizations that benefit from the 3D acceleration. The node is not meant for those who just want to run some lightweight 2D Gui application, e.g., an editor with GUI. </li>
	<li>Load the module vsc-vnc:<br><code>module load vsc-vnc</code><br>This module does not only put the TurboVNC server in the path, but also provides wrapper scripts to start the VNC server with a supported window manager / dekstop environment. Try <code>module help vsc-vnc</code> for more info about the specific wrappers.</li>
	<li>Use your wrapper of choice to start the VNC server. We encourage to use the one for the Xfce desktop environment:<br><code>vnc-xfce</code></li>
	<li>The first time you use VNC, it will ask you to create a password. For security reasons, please use a password that you don't use for anything else. If you have forgotten your password, it can easily be changed with the <code>vncpasswd</code> command and is stored in the file <code>~/.vnc/passwd</code> in encrypted form. It will also ask you for a viewer-only password. If you don't know what this is, you don't need it.</li>
	<li>Among other information, the VNC server will show a line similar to:<br><code>Desktop 'TurboVNC: viz1.leibniz:2 (vsc20XXX)' started on display viz1.leibniz:2</code><br>Note the number after TurboVNC:viz1.leibniz, in this case 2. This is the number of your VNC server, and it will in general be the same as the X display number which is the last number on the line. You'll need that number to connect to the VNC server.</li>
	<li>It is in fact safe though not mandatory to log out now from your SSH session as the VNC server will continue running in the background.</li>
</ol><p>The standard way of starting a VNC server as described in the TurboVNC documentation is by using the <code>vncserver</code> command. However, you should only use this command if you fully understand how it works and what it does. Also, please don't forget to kill the VNC server when you have finished using it as it will not be killed automatically when started through this command  (or use the <code>-autokill</code> command line option at startup). The default startup script (<code>xstartup.turbovnc</code>) which will be put in the <code>~/.vnc</code> directory on first use does not function properly on our systems. We know this and we have no intent to repair this as we prefer to install the vncserver command unmodified from the distribution and provide wrapper scripts instead that use working startup files.
</p><h3>Connecting to the server</h3><ol>
	<li>In most cases, you'll not be able to connect directly to the TurboVNC server (which runs on port 5900 + the server number, 5902 in the above example) but you will need to create a SSH tunnel to forward traffic to the VNC server. The exact procedure is explained in length in the pages \"<a href=\"/client/windows/creating-an-ssh-tunnel\">Creating a SSH tunnel using PuTTY</a>\" (for Windows) and \"<a href=\"/client/linux/creating-an-ssh-tunnel\">Creating a SSH tunnel using OpenSSH</a>\" (for or Linux and macOS) . <br>You'll need to tunnel port number (5900 + server number) (5902 in the example above) on you local machine to the same port number on the node on which the VNC server is running. You cannot use the generic login names (such as login.hpc.uantwerpen.be) for that as you may be assigned a different login node as you were assigned just minutes ago. Instead, use the full names for the specific nodes, e.g., login1-hopper.uantwerpen.be, login2-leibniz.uantwerpen.be or viz1-leibniz.uantwerpen.be.<br>
	<ol>
		<li>In brief:With OpenSSH, your command will look like<br><code>ssh -L 5902:viz1-leibniz.uantwerpen.be:5902 -N vsc20XXX@viz1-leibniz.uantwerpen.be</code></li>
		<li>In PuTTY, select \"Connections - SSH - Tunnel\" in the left pane. As \"Source port\", use 5900 + the server number (5902 in our example) and as destination the full name of the node on which the VNC server is running, e.g., viz1-leibniz.uantwerpen.be.</li>
	</ol></li>
	<li>Once your tunnel is up-and-running, start your VNC client. The procedure depends on the precise client you are using. However in general, the client will ask for the VNC server. That server is localhost:x where x is the number of your VNC server, 2 in the above example. It will then ask you for the password that you have assigned when you first started VNC.</li>
	<li>If all went well, you will now get a window with the desktop environment that you have chosen when starting the VNC server</li>
	<li>Do not forget to close your tunnel when you log out from the VNC server. Otherwise the next user might not be able to connect.</li>
</ol><p><em>Note that the first time that you start a Xfce session with TurboVNC, you'll see a panel \"Welcome to the first start of the panel\". Please select \"Use default config\" as otherwise you get a very empty desktop.</em>
</p><h3>Starting an application</h3><ol>
	<li>Open a terminal window (if one was not already created when you started your session).<br>In the default Xfce-environment, you can open a terminal by selecting \"Terminal Emulator\" in the \"Applications\" menu in the top left. The first time it will let you chose between selected terminal applications.</li>
	<li>Load the modules that are required to start your application of choice.</li>
	<li>2D applications or applications that use a sofware renderer for 3D start as usual. However, to start an application using the hardware-accelerated OpenGL, you'll need to start it through <code>vglrun</code>. Usually adding <code>vglrun</code> at the start of the command line is sufficient.<br>This however doesn't work with all applications. Some applications require a special setup.
	<ol>
		<li>Matlab: start matlab with the <code>-nosoftwareopengl</code> option to enable accelerated OpenGL:<br><code>vglrun matlab -nosoftwareopengl</code><br>The Matlab command <code>opengl info</code> will then show that you are indeed using the GPU.</li>
	</ol></li>
	<li>When you've finished, don't forget to log out (when you use one of our wrapper scripts) or kill the VNC server otherwise (using <code>vncserver -kill :x</code> with <code>x</code> the number of the server).</li>
</ol><p>Note: For a quick test of your setup, enter
</p><pre>vglrun glxinfo
vglrun glxgears
</pre><p>The first command will print some information about the OpenGL functionality that is supported. The second command will display a set of rotating gears. Don't be fooled if they appear to stand still but look at the \"frames per second\" printed in the terminal window.
</p><h3>Common problems</h3><ul>
	<li>Authentication fails when connecting to the server: This happens occasionaly when switching between different versions of TurboVNC. The easiest solution is to simply kill the VNC server using <code>vncserver -kill :x</code> (with x the display number), set a new VNC password using <code>vncpasswd</code> and start over again.</li>
	<li>Xfce doesn't show the task bar at the top of the screen: This too happens sometimes when switching between versions of Xfce4, or you may have screwed up your configuration in another way. Remove the <code>.config/xfce-centos7</code> directory (<code>rm -r .config/xfce-centos7</code>) or the <code>.config/xfce-sl6</code> directory depending on whether you are working on a CentOS7 system (Leibniz curently) or Scientific Linux 6 system (/hopper currently), kill the VNC server and start again.</li>
</ul><span></span><h2>Links</h2><h3>Components used in the UAntwerp setup</h3><ul>
	<li><a href=\"https://www.turbovnc.org/\">The TurboVNC web site</a>, where you'll find downloads for Linux, Windows and macOS</li>
	<li><a href=\"https://www.turbovnc.org/\">The VirtualGL web site</a></li>
	<li><a href=\"https://xfce.org/\">The Xfce web site</a> and some <a href=\"https://en.wikipedia.org/wiki/Xfce\">background material in WikiPedia</a></li>
</ul><h3>Related technologies</h3><ul>
	<li><a href=\"https://www.gnome.org/\">The Gnome web site</a> and <a href=\"https://en.wikipedia.org/wiki/GNOME\">some background in WikiPedia</a></li>
	<li><a href=\"https://www.kde.org/\">The KDE web site</a> and <a href=\"https://en.wikipedia.org/wiki/KDE\">some background in WikiPedia</a></li>
	<li><a href=\"https://en.wikipedia.org/wiki/Twm\">The Tab Window Manager (sometimes called Tom's Window Manager) on WikiPedia</a>, currently available on hopper without support.</li>
	<li><a href=\"https://en.wikipedia.org/wiki/Motif_Window_Manager\">The Motif Window Manager on Wikipedia</a>, currently available on hopper without support.</li>
</ul>"
779,"","<p>Leibniz has one compute node equipped with a Xeon Phi coprocessor from the Knights Landing generation (the first generation with support for the AVX-512 instruction set). For cost reasons we have opted for the PCIe coprocessor model rather than an independent node based on that processor. Downside is the lower memory capacity directly available to the Xeon Phi processor though.
</p><p>The goals for the system are:
</p><ul>
	<li>Having a test device for AVX-512 code as it was too early to purchase Sky Lake Xeon CPUs.</li>
	<li>Assessing the performance of the Xeon Phi compared to regular compute nodes to determine whether it is interesting to further invest in this technology for a later cluster or cluster update.</li>
</ul><p>The system is set up in such a way that once you have access to the Xeon Phi node, you can also log on to the Xeon Phi card itself and use it as an independent system. Your regular VSC directories will be mounted (at least for UAntwerp users, others on request). As such you can also test code to run on independent Xeon Phi systems, the kind of setup that Intel is currently promoting.</p><p>The module system is not yet implemented on the Xeon Phi coprocessor, but modules do work on the host. It does imply though that some setup may be required when running native programs on the Xeon Phi.</p><h2>Getting access</h2><p>Contact <a href=\"/support/contact-support\">the UAntwerp support team</a> to get access to the Xeon Phi node.</p><p>Users of the Xeon Phi node are expected to report back on their experiences. We are most interested in users who can also compare with running on regular nodes as we will use this information for future purchase decisions.</p><p>Currently the node is not yet in the job system, you can log on manually to the node but need to check if noone else is using the node.</p><h2>Compiling for the Xeon Phi</h2><p>We currently support compiling code for the Xeon Phi with the Intel compilers included in the 2017a and later toolchains (i.e., Intel compiler version 17 and higher).
</p><p>Compared to the earlier Knights Corner based Xeon Phi system installed in the Tier-2 infrastructure at the KU Leuven, there are a number of changes. All come down to the fact that the Knights Landing Xeon Phi has much more in common with the regular Intel CPUs than was the case for the earlier generation.
</p><ul>
	<li>Don't use the -mmic compiler option to compile code for the Xeon Phi. This option generates code for the Knights Corner instruction set which is not compatible with the Knights Landing processors. Instead,
	<ul>
		<li>Use -xMIC-AVX512 to compile code that runs natively on the Xeon Phi</li>
		<li>Use -qoffload-arch:mic-avx512 (in combination met -xHost) for programs that run on the host but offload sections to the Xeon Phi.</li>
	</ul>In most cases you'll also want to use -qopenmp to enable OpenMP, the primary programming model for the Xeon Phi.</li>
	<li>Similarly,
environment variables that start with MIC are for KNC only. KNL uses the same
libraries as regular x86-64 code.
	</li><li>Mind the meaning of the _MIC__
preprocessor macro in old Xeon Phi code. It is set when compiling for the KNC
generation cards, but some code may use it wrongly for conditional compilation
of parts of offloaded code, which really should have been done through
__TARGET_ARCH_MIC which works for both KNC and KNL. For conditional compilation
of code for KNL in both offload and native routines, one should use the
__AVX512F__ feature macro.</li>
</ul><h2>Running applications on the Xeon Phi</h2><ul><li>Programs that use offloading are started in the same way as regular host programs. Nothing special needs to be done. The offloaded code runs on the coprocessor under the userid micuser.</li><li>Simple native programs can be started from the host using the micnativeloadex command followed by the name of the executable and other arguments. The micnativeloadex command will look up all shared libraries used by the executable and make sure that they are uploaded to the Xeon Phi. To find the libraries, it uses the environment variable SINK_LD_LIBRARY_PATH. For programs that only rely on a compiler module, our compiler modules take care of the proper definition of this variable. Your program will run on the coprocessor under a special userid, micuser, which also implies that you cannot acces your own files!<br><em>According to the Xeon Phi manuals, certain requests are send automatically to the host but it is not clear at the moment what this implies.</em></li><li>The second way to start native programs is to log on to the Xeon Phi using ssh (ssh mic0) and work the way you would on a regular cluster node. You will see the same directories that you also see on the regular Xeon Phi node (minus the /small file system at the moment) and will have access to the same data in the same way.<br><em>The module system has not yet been implemented on the Xeon Phi.</em></li></ul>"
781,"","<p>Leibniz has two compute nodes each equipped with two NVIDIA Tesla P100 GPU compute cards, the most powerful cards available at the time of installation of the system. We run the regular NVIDIA software stack on those systems
</p><p>The main goal of the system is to assess the performance of GPUs for applications used by our researchers. We want to learn for which applications GPU computing is economically viable. Users should realise that these nodes carry three times the cost of a regular compute node and might also be shorter lived (in the past, some NVIDA GPUs have shown to be pretty fragile). So these nodes are only interesting and should only be used for applications that run three times faster than a regular CPU-based equivalent.
</p><p>As such we offer precedence to users who want to work with us towards this goal and either develop high-quality GPU software or are willing to benchmark their application on GPU and regular CPUs.
</p><h2>Getting access</h2><p>Contact <a href=\"/support/contact-support\">the UAntwerp support team</a> to get access to the Xeon Phi node.
</p><p>Users of the GPU compute nodes are expected to report back on their experiences. We are most interested in users who can also compare with running on regular nodes as we will use this information for future purchase decisions.
</p><p>Currently the nodes are not yet integrated in the job system, you can log on manually to the node but need to check if noone else is using the node.
</p><h2>Monitoring GPU nodes</h2><p>Monitoring of CPU use by jobs running on the GPU nodes can be done in the same way as for regular compute nodes.
</p><p>One useful command to monitor the use of the GPUs is <code>nvidia-smi</code>. It will show information on both GPUs in the GPU node, and among others lets you easily verify if the GPUs are used by the job.
</p><h2>Software on the GPU</h2><p>Software is installed on demand. As these systems are new to us also, we do expect some collaboration of the user to get software running on the GPUs.
</p><table>
<thead>
<tr>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">Package
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\"><strong>Module</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">Description
	</th>
</tr>
</thead>
<tbody>
<tr>
	<td style=\"border: solid 1pt;\"><a href=\"https://www.cp2k.org/\">CP2K</a>
	</td>
	<td style=\"border: solid 1pt;\">CP2K/5.1-intel-2017a-bare-GPU-noMPI
	</td>
	<td style=\"border: solid 1pt;\">GPU-accelerated version of CP2K. The -GPU-noMPI-versions are ssmp binaries without support for MPI, so they can only be used on a single GPU node. The binaries are compiled with equivalent options to the corresponding -bare-multiver modules for CPU-only computations.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\"><a href=\"https://developer.nvidia.com/cuda-zone\">CUDA</a>
	</td>
	<td style=\"border: solid 1pt;\">CUDA/8.0.61<br>CUDA/9.0.176<br>CUDA/9.1.85
	</td>
	<td style=\"border: solid 1pt;\">Various versions of the CUDA development kit
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\"><a href=\"https://developer.nvidia.com/cudnn\">cuDNN</a>
	</td>
	<td style=\"border: solid 1pt;\">cuDNN/6.0-CUDA-8.0.61<br>cuDNN/7.0.5-CUDA-8.0.61<br>cuDNN/7.0.5-CUDA-9.0.176<br>cuDNN/7.0.5-CUDA-9.1.85<br>
	</td>
	<td style=\"border: solid 1pt;\">The CUDA Deep Neural Network library, version 6.0 and 7.0, both installed from standard NVIDA tarbals but in the directory structure of our module system.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\"><a href=\"http://www.gromacs.org/\">GROMACS</a>
	</td>
	<td style=\"border: solid 1pt;\">GROMACS/2016.4-foss-2017a-GPU-noMPI<br>GROMACS/2016.4-intel-2017a-GPU-noMPI<br>
	</td>
	<td style=\"border: solid 1pt;\">GROMACS with GPU acceleration. The -GPU-noMPI-versions are ssmp binaries without support for MPI, so they can only be used on a single GPU node.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\"><a href=\"https://keras.io/\">Keras</a>
	</td>
	<td style=\"border: solid 1pt;\">Keras/2.1.3-intel-2017c-GPU-Python-3.6.3
	</td>
	<td style=\"border: solid 1pt;\">Keras with TensorFlow as the backend (1.4 for Keras 2.1.3), using the GPU-accelerated version of Tensorflow.<br>For comparison purposes there is a identical version using the CPU-only version of TensorFlow 1.4.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\"><a href=\"http://www.ks.uiuc.edu/Research/namd/\">NAMD</a>
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">Work in progress
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\"><a href=\"https://www.tensorflow.org/\">TensorFlow</a>
	</td>
	<td style=\"border: solid 1pt;\">Tensorflow/1.3.0-intel-2017a-GPU-Python-3.6.1<br>Tensorflow/1.4.0-intel-2017c-GPU-Python-3.6.3<br>
	</td>
	<td style=\"border: solid 1pt;\">GPU versions of Tensorflow 1.3 and 1.4. Google-provided binaries were used for the installation.<br>There are CPU-only equivalents of those modules for comparison. The 1.3 version was installed from the standard PyPi wheel which is not well optimized for modern processors, the 1.4 version was installed from a Python wheel compiled by Intel engineers and should be well-optimized for all our systems.
	</td>
</tr>
</tbody>
</table>"
783,"","<h2>HPC Tutorial</h2>
<p>This is our standard introduction to the VSC HPC systems. It is complementary to the information in this user portal, the latter being more the reference manual.
</p>
<p>We have separate versions depending on your home institution and the operating system from which you access the cluster:
</p>
<table style=\"width: 100%;\" rel=\"width: 100%;\" border=\"1\" cellpadding=\"1\" cellspacing=\"1\">
<thead>
<tr>
	<td>
	</td>
	<td><strong>Windows</strong>
	</td>
	<td><strong>macOS</strong>
	</td>
	<td><strong>Linux</strong>
	</td>
</tr>
</thead>
<tbody>
<tr>
	<td><strong>UAntwerpen</strong>
	</td>
	<td>[<a href=\"http://hpcugent.github.io/vsc_user_docs/pdf/intro-HPC-windows-antwerpen.pdf\">PDF</a>]
	</td>
	<td>[<a href=\"http://hpcugent.github.io/vsc_user_docs/pdf/intro-HPC-mac-antwerpen.pdf\">PDF</a>]
	</td>
	<td>[<a href=\"http://hpcugent.github.io/vsc_user_docs/pdf/intro-HPC-linux-antwerpen.pdf\">PDF</a>]
	</td>
</tr>
<tr>
	<td><strong>VUB</strong>
	</td>
	<td>[<a href=\"http://hpcugent.github.io/vsc_user_docs/pdf/intro-HPC-windows-brussel.pdf\">PDF</a>]
	</td>
	<td>[<a href=\"http://hpcugent.github.io/vsc_user_docs/pdf/intro-HPC-mac-brussel.pdf\">PDF</a>]
	</td>
	<td>[<a href=\"http://hpcugent.github.io/vsc_user_docs/pdf/intro-HPC-linux-brussel.pdf\">PDF</a>]
	</td>
</tr>
<tr>
	<td><strong>UGent</strong>
	</td>
	<td>[<a href=\"http://hpcugent.github.io/vsc_user_docs/pdf/intro-HPC-windows-gent.pdf\">PDF</a>]
	</td>
	<td>[<a href=\"http://hpcugent.github.io/vsc_user_docs/pdf/intro-HPC-mac-gent.pdf\">PDF</a>]
	</td>
	<td>[<a href=\"http://hpcugent.github.io/vsc_user_docs/pdf/intro-HPC-linux-gent.pdf\">PDF</a>]
	</td>
</tr>
<tr>
	<td><strong>KU Leuven/UHasselt</strong>
	</td>
	<td>[<a href=\"http://hpcugent.github.io/vsc_user_docs/pdf/intro-HPC-windows-leuven.pdf\">PDF</a>]
	</td>
	<td>[<a href=\"http://hpcugent.github.io/vsc_user_docs/pdf/intro-HPC-mac-leuven.pdf\">PDF</a>]
	</td>
	<td>[<a href=\"http://hpcugent.github.io/vsc_user_docs/pdf/intro-HPC-linux-leuven.pdf\">PDF</a>]
	</td>
</tr>
</tbody>
</table>"
785,"","<h2>Important changes</h2><p>The 2017a toolchain is the toolchain that
will be carried forward to Leibniz and will be available after the operating
system upgrade of Hopper. Hence it is meant to be as complete as possible. We
will only make a limited number of programs available in the 2016b toolchain
(basically those that show much better performance with the older compiler or
that do not compile with the compilers in the 2017a toolchains).
</p><p>Important changes
in the 2017a toolchain:
</p><ul>
	<li>The Intel compilers have been installed in
a single directory tree, much the way Intel intends the install to be done. The
intel/2017a module loads fewer submodules and instead sets all required
variables. The install now also contains the Thread Building Blocks (TBB),
Integrated Performance Primitives (IPP) and Data Analytics Acceleration Library
(DAAL). All developer tools (debugger, Inspector,Advisor,  Vtune
Amplifier, ITAC) are enabled by loading the inteldevtools/2017a module rather
than independent modules for each tool. More information is available on 
	<a href=\"/infrastructure/hardware/hardware-ua/intel\">the documentation page on the Intel compilers @ UAntwerp</a>.</li>
	<li>The Python install now also contains a
number of packages that previously where accessed via separate modules:
	<ul>
		<li>matplotlib, so there is no longer a
separate module to load matplotlib. 
		</li>
		<li>lxml</li>
	</ul>
	</li>
	<li>The R install now also contains a selection
of the Bioconductor routines, so no separate module is needed to enable the
latter.
	</li>
	<li>netCDF is now a single module containing
all 4 interfaces rather than 4 separate modules that installed each interface
in a different directory tree (three of which all relied on the module for the
fourth). This should ease the installation of code that uses the netCDF Fortran
or one of the C++ interfaces and expects all netCDF libraries to be installed
in the same directory.
	</li>
</ul><p>We will skip the 2017b toolchain as defined by the VSC as we have already upgraded the 2017a toolchain to a more recent update of the Intel 2017 compilers to avoid problems with certain applications.
</p><h2>Available toolchains</h2><p>There are
currently three major toolchains on the UIAntwerp clusters:
</p><ul>
	<li>The Intel toolchain,
which includes the Intel compilers and tools, matching versions of the GNU
compilers, and all software compiled with them. 
	</li>
	<li>The FOSS toolchain,
built out of open-source components. It is mostly used for programs that don’t
install with the Intel compilers, or by users who want to do development with
Open MPI and other open-source libraries.
	<br>
	The FOSS-toolchain has a number of subtoolchains: Gompi, GCC and GCCcore, and
some programs are installed in these subtoolchains because they don’t use the
additional components that FOSS offers.
	</li>
	<li>The system toolchain
(sl6 or centos7), containing programs that only use system libraries or other
tools from this toolchain.
	</li>
</ul><p>The tables below
list the last available module for a given software package and the
corresponding version in the 2017a toolchain. Older versions can only be
installed on demand with a very good motivation, as older versions of packages
also often fail to take advantage of advances in supercomputer architecture and
offer lower performance. Packages that have not been used recently will
only be installed on demand.
</p><p>Several of the
packages in the system toolchain are still listed as “on demand” since they
require licenses and interaction with their users is needed before we can
install them.
</p><h3>Intel toolchain</h3><table style=\"border: solid 1px; border-collapse: collapse;\">
<thead>
<tr>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Latest
  pre-2017a
		</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>2017a</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Comments</strong>
	</th>
</tr>
</thead>
<tbody>
<tr>
	<td style=\"border: solid 1pt;\">
		ABINIT/8.0.7-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Work in progress</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Advisor/2016_update4
	</td>
	<td style=\"border: solid 1pt;\">
		inteldevtools/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in a
  new module with the other Intel development tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\"><em>ANTs/2.1.0-foss-2015a</em>
	</td>
	<td style=\"border: solid 1pt;\">ANTs/2.2.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">Not yet available on Leibniz due to compile problems.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		augustus/3.0.1-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Autoconf/2.69-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Autoconf/2.69</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the system
  toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		AutoDock/1.1.2
	</td>
	<td style=\"border: solid 1pt;\">
		AutoDock_Vina/1.1.2
	</td>
	<td style=\"border: solid 1pt;\">
		Naming modified
  to the standard naming used in our build tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Automake/1.15-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Automake/1.15</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the system
  toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Autotools/20150215-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Autotools/2016123</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the system
  toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">BAli-Phy/2.3.8-intel-2017a-OpenMP<br>BAli-Phy/2.3.8-intel-2017a-MPI<br>
	</td>
	<td style=\"border: solid 1pt;\">By Ben Redelings, <a href=\"http://www.bali-phy.org/docs.php\">documentation</a> on <a href=\"http://www.bali-phy.org/\">the software web site</a>. This package supports either OpenMP or MPI, but not both together in a hybrid mode.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		beagle-lib/2.1.2-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		beagle-lib/2.1.2-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Beast/2.4.4-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Beast/2.4.5-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Version with beagle-lib
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Biopython/1.68-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		Biopython/1.68-intel-2017a-Python-2.7.13<br>Biopython/1.68-intel-2017a-Python-3.6.1
	</td>
	<td style=\"border: solid 1pt;\">
		Builds for Python
  2,7 and Python 3.6
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		bismark/0.13.1-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		Bismark/0.17.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Naming modified
  to the standard naming used in our build tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Bison/3.0.4-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Bison/3.0.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		BLAST+/2.6.0-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		BLAST+/2.6.0-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Boost/1.63.0-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		Boost/1.63.0-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Bowtie2/2.2.9-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		Bowtie2/2.2.9-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		byacc/20160606-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>byacc/20170201</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the system
  toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		bzip2/1.0.6-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		bzip2/1.0.6-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		cairo/1.15.2-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		cairo/1.15.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		CASINO/2.12.1-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		CASM/0.2.0-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on demand, compiler problems.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">CGAL/4.9-intel-2017a-forOpenFOAM
	</td>
	<td style=\"border: solid 1pt;\">Installed without the components that require Qt and/or OpenGL.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		CMake/3.5.2-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		CMake/3.7.2-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		CP2K/4.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">CP2K/4.1-intel-2017a-bare<br>CP2K/4.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\"><br>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		CPMD/4.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		cURL/7.49.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		cURL/7.53.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">DIAMOND/0.9.12-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		DLCpar/1.0-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		DLCpar/1.0-intel-2017a-Python-2.7.13<br>DLCpar/1.0-intel-2017a-Python-3.6.1<br>
	</td>
	<td style=\"border: solid 1pt;\">
		Installed for
  Python 2.7.13 and Pyton 3.6.1
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Doxygen/1.8.11-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Doxygen/1.8.13</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the
  system toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		DSSP/2.2.1-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">DSSP/2.2.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\"><br>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Eigen/3.2.9-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Eigen/3.3.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		elk/3.3.17-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		Elk/4.0.15-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Naming modified
  to the standard naming used in our build tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		exonerate/2.2.0-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		Exonerate/2.4.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Naming modified
  to the standard naming used in our build tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		expat/2.2.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		expat/2.2.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">FastME/2.1.5.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		FFTW/3.3.4-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		FFTW/3.3.6-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		There is also a
  FFTW-compatible interface in intel/2017a, but it does not work for all
  packages.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		file/5.30-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		fixesproto/5.0-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		flex/2.6.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		flex/2.6.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		fontconfig/2.12.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		fontconfig/2.12.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		freeglut/3.0.0-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		freeglut/3.0.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Not yet
  operational on CentOS 7
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		freetype/2.7-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		freetype/2.7.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		FSL/5.0.9-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GAMESS-US/20141205-R1-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		gc/7.4.4-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		gc/7.6.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GDAL/2.1.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		GDAL/2.1.3-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		Does
  not support Python 3.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		genometools/1.5.4-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		GenomeTools/1.5.9-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GEOS/3.5.0-intel-2015a-Python-2.7.9
	</td>
	<td style=\"border: solid 1pt;\">
		GEOS/3.6.1-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		Does
  not support Python 3.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		gettext/0.19.8-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		gettext/0.19.8.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GLib/2.48.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		GLib/2.49.7-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GMAP-GSNAP/2014-12-25-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		GMAP-GSNAP/2017-03-17-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GMP/6.1.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		GMP/6.1.2-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		gnuplot/5.0.0-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		gnuplot/5.0.6-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GObject-Introspection/1.44.0-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		GObject-Introspection/1.49.2-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GROMACS/5.1.2-intel-2016a-hybrid
	</td>
	<td style=\"border: solid 1pt;\">
		GROMACS/5.1.2-intel-2017a-hybrid<br>GROMACS/2016.3-intel-2017a<br>
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GSL/2.3-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		GSL/2.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">gtest/1.8.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">Google C++ Testing Framework
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Guile/1.8.8-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Guile/1.8.8-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Guile/2.0.11-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Guile/2.2.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		hanythingondemand/3.2.0-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		hanythingondemand/3.2.0-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		HarfBuzz/1.3.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		HDF5/1.8.17-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		HDF5/1.8.18-intel-2017a<br>HDF5/1.8.18-intel-2017a-noMPI
	</td>
	<td style=\"border: solid 1pt;\"><a href=\"https://support.hdfgroup.org/HDF5/\">HDF5</a> with and without MPI-support.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">HISAT2/2.0.5-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		HTSeq/0.6.1p1-intel-2016a-Python-2.7.11
	</td>
	<td style=\"border: solid 1pt;\">
		HTSeq/0.7.2-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		Does not support
  Python 3.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		icc/2016.3.210-GCC-5.4.0-2.26
	</td>
	<td style=\"border: solid 1pt;\">
		intel/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Intel compiler
  components in a single module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		iccifort/2016.3.210-GCC-5.4.0-2.26
	</td>
	<td style=\"border: solid 1pt;\">
		intel/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Intel compiler
  components in a single module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		ifort/2016.3.210-GCC-5.4.0-2.26
	</td>
	<td style=\"border: solid 1pt;\">
		intel/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Intel compiler
  components in a single module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		imkl/11.3.3.210-iimpi-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		intel/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Intel compiler
  components in a single module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		impi/5.1.3.181-iccifort-2016.3.210-GCC-5.4.0-2.26
	</td>
	<td style=\"border: solid 1pt;\">
		intel/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Intel compiler
  components in a single module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		inputproto/2.3.2-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Inspector/2016_update3
	</td>
	<td style=\"border: solid 1pt;\">
		inteldevtools/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in a
  new module with the other Intel development tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		ipp/8.2.1.133
	</td>
	<td style=\"border: solid 1pt;\">
		intel/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Intel compiler
  components in a single module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		itac/9.0.2.045
	</td>
	<td style=\"border: solid 1pt;\">
		inteldevtools/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in a
  new module with the other Intel development tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		JasPer/2.0.12-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">Julia/0.6.0-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\"><a href=\"https://julialang.org/\">Julia</a>, command line version (so without the Juno IDE).
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		kbproto/1.0.7-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		kwant/1.2.2-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">kwant/1.2.2-intel-2017a-Python-3.6.1
	</td>
	<td style=\"border: solid 1pt;\">Built with single-threaded libraries as advised in the documentation which implies that kwant is not exactly a HPC program.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		LAMMPS/14May16-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		LAMMPS/31Mar2017-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		libcerf/1.5-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libffi/3.2.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libffi/3.2.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		libgd/2.2.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Libint/1.1.6-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Libint/1.1.6-intel-2017a<br>Libint/1.1.6-intel-2017a-CP2K
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libint2/2.0.3-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libjpeg-turbo/1.5.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libjpeg-turbo/1.5.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libmatheval/1.1.11-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libmatheval/1.1.11-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libpng/1.6.26-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libpng/1.6.28-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libpthread-stubs/0.3-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\"><i>Installed on demand.</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libreadline/6.3-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libreadline/7.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		LibTIFF/4.0.6-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		LibTIFF/4.0.7-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libtool/2.4.6-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>libtool/2.4.6</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the
  system toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libunistring/0.9.6-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libunistring/0.9.7-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libX11/1.6.3-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libXau/1.0.8-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libxc/2.2.3-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libxc/3.0.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libxcb/1.12-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libXdmcp/1.1.2-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libXext/1.3.3-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libXfixes/5.0.1-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libXi/1.7.6-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libxml2/2.9.4-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libxml2/2.9.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libXrender/0.9.9-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libxslt/1.1.28-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">
		libxslt/1.1.29-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libxsmm/1.6.4-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libxsmm/1.7.1-intel-2017a<br>libxsmm/1.8-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libyaml/0.1.6-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\"><em>Installed on demand</em>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">LLVM/3.9/.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\"><a href=\"https://llvm.org/\">LLVM compiler backend</a> with libLLVM.so.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		lxml/3.5.0-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
		Python/3.6.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 2.7 and 3.6 modules.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		M4/1.4.17-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>M4/1.4.18</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the
  system toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">MAFFT/7.312-intel-2017a-with-extensions
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MAKER-P/2.31.8-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MAKER-P-mpi/2.31.8-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		matplotlib/1.5.3-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a<br>Python/3.6.1-intel-2017a<br>
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 2.7 and 3.6 modules
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MCL/14.137-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		MCL/14.137-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		mdust/1.0-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		mdust/1.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		METIS/5.1.0-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		METIS/5.1.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MITE_Hunter/11-2011-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		molmod/1.1-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">molmod/1.1-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Work
  in progress, compile problems with newer compilers.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Mono/4.6.2.7-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Mono/4.8.0.495-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Mothur/1.34.4-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\"><em>Installed on demand</em>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MUMPS/5.0.1-intel-2016a-serial<br>MUMPS/5.0.0-intel-2015a-parmetis<br>
	</td>
	<td style=\"border: solid 1pt;\">
		MUMPS-5.1.1-intel-2017a-openmp-noMPI<br>MUMPS-5.1.1-intel-2017a-openmp-MPI<br>MUMPS-5.1.1-intel-2017a-noOpenMP-noMPI<br>
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MUSCLE/3.8.31-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		MUSCLE/3.8.31-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		NASM/2.12.02-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>NASM/2.12.02</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the systemtoolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		ncbi-vdb/2.8.2-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		ncurses/6.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		ncurses/6.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">NEURON/7.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\"><a href=\"http://www.neuron.yale.edu/neuron/\">Yale NEURON code</a>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		netaddr/0.7.14-intel-2015a-Python-2.7.9
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 2.7 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		netCDF/4.4.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		netCDF/4.4.1.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		All netCDF
  interfaces integrated in a single module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		netCDF-Fortran/4.4.4-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		netCDF/4.4.1.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		All netCDF
  interfaces integrated in a single module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		netifaces/0.10.4-intel-2015a-Python-2.7.9
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 2.7 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		NGS/1.3.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		numpy/1.9.2-intel-2015b-Python-2.7.10
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 2.7 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		numpy/1.10.4-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">
		Python/3.6.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 3.6 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		NWChem/6.5.revision26243-intel-2015b-2014-09-10-Python-2.7.10
	</td>
	<td style=\"border: solid 1pt;\">
		NWChem/6.6.r27746-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		<i>On demand on Hopper.</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">OpenFOAM/4.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">Installed without the components that require OpenGL and/or Qt (which should only be in the postprocessing)
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		OpenMX/3.8.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		OpenMX/3.8.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">OrthoFinder/1.1.10-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		Pango/1.40.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		ParMETIS/4.0.3-intel-2015b
	</td>
	<td style=\"border: solid 1pt;\">
		ParMETIS/4.0.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		pbs-drmaa/1.0.18-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\"><em>Installed on demand</em>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		pbs_PRISMS/1.0.1-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		Python
  interfaces for Torque/PBS used by CASM
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		pbs_python/4.6.0-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		pbs_python/4.6.0-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		Python
  interfaces for Torque/PBS used by hanythingondemand
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		PCRE/8.38-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		PCRE/8.40-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Perl/5.20.1-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		Perl/5.24.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		pixman/0.34.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		pixman/0.34.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		pkg-config/0.29.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>pkg-config/0.29.1</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the
  system toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		PLUMED/2.3.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		PLUMED/2.3.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		PROJ/4.9.2-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		PROJ/4.9.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">protobuf/3.4.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\"><a href=\"https://developers.google.com/protocol-buffers/\">Google Protocol Buffers</a>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Pysam/0.9.1.4-intel-2016a-Python-2.7.11
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 2.7 module. Also load SAMtools to use.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Pysam/0.9.1.2-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">
		Python/3.6.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 3.6 module. Also load SAMtools to use.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Python/2.7.12-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Python/3.5.1-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		Python/3.6.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		QuantumESPRESSO/5.2.1-intel-2015b-hybrid
	</td>
	<td style=\"border: solid 1pt;\">QuantumESPRESSO/6.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Work in progress.</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		R/3.3.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		R/3.3.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		RAxML/8.2.9-intel-2016b-hybrid-avx
	</td>
	<td style=\"border: solid 1pt;\">RAxML/8.2.10-intel-2017a-hybrid
	</td>
	<td style=\"border: solid 1pt;\">We suggest users try RAxML-ng (still beta) which is supposedly much faster and better adapted to new architectures and can be installed on demand.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">RAxML-NG/0.4.1-intel-2017a-pthreads<br>
		RAxML-NG/0.4.1-intel-2017a-hybrid
	</td>
	<td style=\"border: solid 1pt;\"><a href=\"https://github.com/amkozlov/raxml-ng/wiki\">RAxML Next Generation beta</a>, compiled for shared memory (pthreads) and hybrid
distributed-shared memory (hybrid, uses MPI and pthreads).
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		R-bundle-Bioconductor/3.3-intel-2016b-R-3.3.1
	</td>
	<td style=\"border: solid 1pt;\">
		R/3.3.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard R module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		renderproto/0.11.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		RepeatMasker/4.0.5-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand; compiler problems.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		RMBlast/2.2.28-intel-2015a-Python-2.7.9
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand; compiler problems.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SAMtools/0.1.19-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		SAMtools/1.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		scikit-umfpack/0.2.1-intel-2015b-Python-2.7.10
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\"><em>Installed on demand</em>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		scikit-umfpack/0.2.1-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">scikit-umfpack/0.2.3-intel-2017a-Python-3.6.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		scipy/0.15.1-intel-2015b-Python-2.7.10
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 2.7 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		scipy/0.16.1-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">
		Python/3.6.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 3.6 module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SCons/2.5.1-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		SCons/2.5.1-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		<i>On demand on
  CentOS 7; also in the system toolchain.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SCOTCH/6.0.4-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		SCOTCH/6.0.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Siesta/3.2-pl5-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		Siesta/4.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SNAP/2013-11-29-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		spglib/1.7.4-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\"><em>Installed on demand</em>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SQLite/3.13.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		SQLite/3.17.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SuiteSparse/4.4.5-intel-2015b-ParMETIS-4.0.3
	</td>
	<td style=\"border: solid 1pt;\">SuiteSparse/4.5.5-intel-2015b-ParMETIS-4.0.3
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SuiteSparse/4.4.5-intel-2016a-METIS-5.1.0
	</td>
	<td style=\"border: solid 1pt;\">SuiteSparse/4.4.5-intel-2017a-METIS-5.1.0<br>SuiteSparse/4.5.5-intel-2017a-METIS-5.1.0<br>
	</td>
	<td style=\"border: solid 1pt;\">Older version as it is known to be compatible with our Python packages.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SWIG/3.0.7-intel-2015b-Python-2.7.10
	</td>
	<td style=\"border: solid 1pt;\">
		SWIG/3.0.12-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SWIG/3.0.8-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">
		SWIG/3.0.12-intel-2017a-Python-3.6.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Szip/2.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Szip/2.1.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		tbb/4.3.2.135
	</td>
	<td style=\"border: solid 1pt;\">
		intel/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Intel compiler
  components in a single module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Tcl/8.6.5-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Tcl/8.6.6-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		TELEMAC/v7p2r0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\"><em>Work in progress.</em>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		TINKER/7.1.3-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand; compiler problems.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Tk/8.6.5-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Tk/8.6.6-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		TopHat/2.1.1-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		TopHat is no
  longer developed, its developers advise considering switching to 
		<a href=\"http://ccb.jhu.edu/software/hisat2/index.shtml\">HISAT2</a> which is more accurate and more efficient. It does not compile with the intel/2017a compilers.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">VASP
	</td>
	<td style=\"border: solid 1pt;\">VASP/5.4.4-intel-2016b<br>VASP/5.4.4-intel-2016b-vtst-173
	</td>
	<td style=\"border: solid 1pt;\">VASP has not been installed in the 2017a toolchain due to performance regressions and occasional run time errors with the Intel 2017 compilers and hence has been made available in the intel/2016b toolchain.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Voro++/0.4.6-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Voro++/0.4.6-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		vsc-base/2.5.1-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		vsc-install/0.10.11-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		vsc-install/0.10.25-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		Does not support
  Python 3.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		vsc-mympirun/3.4.3-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		vsc-mympirun/3.4.3-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		VTune/2016_update3
	</td>
	<td style=\"border: solid 1pt;\">
		inteldevtools/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in a
  new module with the other Intel development tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		worker/1.5.1-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		worker-1.6.7-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		X11/20160819-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		xcb-proto/1.12
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		xextproto/7.3.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		xorg-macros/1.19.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		xproto/7.0.29-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		xtrans/1.3.5-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		XZ/5.2.2-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		XZ/5.2.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		zlib/1.2.8-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		zlib/1.2.11-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
</tbody>
</table><h3>Foss toolchain</h3><table style=\"border: solid 1px; border-collapse: collapse;\">
<thead>
<tr>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Latest pre-2017a</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>2017a</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Comments</strong>
	</th>
</tr>
</thead>
<tbody>
<tr>
	<td style=\"border: solid 1pt;\">
		ANTs/2.1.0-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\"><em>ANTs/2.2.0-intel-2017a</em>
	</td>
	<td style=\"border: solid 1pt;\">Moved to the Intel toolchain.<br>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		ATLAS/3.10.2-foss-2015a-LAPACK-3.4.2
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		CMake/3.5.2-foss-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		CMake/3.7.2-foss-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Cufflinks/2.2.1-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		cURL/7.41.0-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Cython/0.22.1-foss-2015a-Python-2.7.9
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated into
  the standard Python module for the intel toolchains
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		FFTW/3.3.4-gompi-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		FFTW/3.3.6-gompi-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GSL/2.1-foss-2015b
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		HDF5/1.8.14-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libpng/1.6.16-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libreadline/6.3-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		makedepend/1.0.5-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MaSuRCA/2.3.2-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		ncurses/6.0-foss-2016b
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		pbs-drmaa/1.0.18-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Perl/5.20.1-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Python/2.7.9-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		Python is
  available in the Intel toolchain.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SAMtools/0.1.19-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		Newer versions
  with intel toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SPAdes/3.10.1-foss-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		SPAdes/3.10.1-foss-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Szip/2.1-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		zlib/1.2.8-foss-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		zlib/1.2.11-foss-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
</tbody>
</table><h3>Gompi</h3><table style=\"border: solid 1px; border-collapse: collapse;\">
<thead>
<tr>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Latest
  pre-GCC-6.3.0 (2017a)
		</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>gompi-2017a</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Comments</strong>
	</th>
</tr>
</thead>
<tbody>
<tr>
	<td style=\"border: solid 1pt;\">
		ScaLAPACK/2.0.2-gompi-2016b-OpenBLAS-0.2.18-LAPACK-3.6.1
	</td>
	<td style=\"border: solid 1pt;\">
		ScaLAPACK/2.0.2-gompi-2017a-OpenBLAS-0.2.19-LAPACK-3.7.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
</tbody>
</table><h3>GCC</h3><table style=\"border: solid 1px;\">
<thead>
<tr>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Latest pre-gompi-2017a</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>GCC-6.3.0  (2017a)</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Comments</strong>
	</th>
</tr>
</thead>
<tbody>
<tr>
	<td style=\"border: solid 1pt;\">
		OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1
	</td>
	<td style=\"border: solid 1pt;\">
		OpenBLAS/0.2.19-GCC-6.3.0-2.27-LAPACK-3.7.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		numactl/2.0.11-GCC-5.4.0-2.26
	</td>
	<td style=\"border: solid 1pt;\">
		numactl/2.0.11-GCC-6.3.0-2.27
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		OpenMPI/1.10.3-GCC-5.4.0-2.26
	</td>
	<td style=\"border: solid 1pt;\">
		OpenMPI/2.0.2-GCC-6.3.0-2.27
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MPICH/3.1.4-GCC-4.9.2
	</td>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
</tbody>
</table><h3>GCCcore</h3><table style=\"border: solid 1px; border-collapse: collapse;\">
<thead>
<tr>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Latest
  pre-GCCcore-6.3.0  (2017a)
		</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>GCCcore-6.3.0 
  (2017a)
		</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Comments</strong>
	</th>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		binutils/2.26-GCCcore-5.4.0
	</td>
	<td style=\"border: solid 1pt;\">
		binutils/2.27-GCCcore-6.3.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		flex/2.6.0-GCCcore-5.4.0
	</td>
	<td style=\"border: solid 1pt;\">
		flex/2.6.3-GCCcore-6.3.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Lmod/7.0.5
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		Default
  module tool on CentOS 7
	</td>
</tr>
</thead>
</table><h3>System toolchain</h3><table style=\"border: solid 1px; border-collapse: collapse;\">
<thead>
<tr>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Pre-2017</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Latest module</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Comments</strong>
	</th>
</tr>
</thead>
<tbody>
<tr>
	<td style=\"border: solid 1pt;\">
		ant/1.9.4-Java-8
	</td>
	<td style=\"border: solid 1pt;\">
		ant/1.10.1-Java-8
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		Autoconf/2.69
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		AutoDock_Vina/1.1.2
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		Automake/1.15
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		Autotools/2016123
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">Bazel/0.5.3
	</td>
	<td style=\"border: solid 1pt;\"><a href=\"https://bazel.build/\">Google's software installer</a>. Not installed on the Scientific Linux 6 nodes of hopper.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		binutils/2.26
	</td>
	<td style=\"border: solid 1pt;\">
		binutils/2.27
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Bison/3.0.4
	</td>
	<td style=\"border: solid 1pt;\">
		Bison/3.0.4
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		BRATNextGen/20150505
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		byacc/20170201
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		CMake/3.7.2
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		core-counter/1.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		CPLEX/12.6.3
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		DFTB+/1.2.2
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		Doxygen/1.8.13
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		EasuBuild/…
	</td>
	<td style=\"border: solid 1pt;\">
		EasyBuild/3.1.2
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		FastQC/0.11.5-Java-8
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		FINE-Marine/5.2
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		flex/2.6.0<br>flex/2.6.3<br>
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GATK/3.5-Java-8
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Gaussian16/g16_A3-AVX
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Work in progress.</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Gurobi/6.5.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Hadoop/2.6.0-cdh5.4.5-native
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		help2man/1.47.4
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Java/8
	</td>
	<td style=\"border: solid 1pt;\">
		Java/8
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		JUnit/4.12-Java-8
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		libtool/2.4.6
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		M4/1.4.17
	</td>
	<td style=\"border: solid 1pt;\">
		M4/1.4.18
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MATLAB/R2016a
	</td>
	<td style=\"border: solid 1pt;\">
		MATLAB/R2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Maven/3.3.9
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MGLTools/1.5.7rc1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MlxLibrary/1.0.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		Lixoft Simulx
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MlxPlore/1.1.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		Lixoft MLXPlore
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		monitor/1.1.2
	</td>
	<td style=\"border: solid 1pt;\">
		monitor/1.1.2
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Monolix/2016R1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		NASM/2.12.02
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Newbler/2.9
	</td>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		On request, has
  not been used recently.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Novoalign/3.04.02
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		ORCA/3.0.3
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		p4vasp/0.3.29
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		parallel/20160622
	</td>
	<td style=\"border: solid 1pt;\">
		parallel/20170322
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		pkg-config/0.29.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		protobuf/2.5.0
	</td>
	<td style=\"border: solid 1pt;\">
		protobuf/2.6.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Ruby/2.1.10
	</td>
	<td style=\"border: solid 1pt;\">
		Ruby/2.4.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		SCons/2.5.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		scripts/4.0.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
	</td>"
787,"","<table style=\"border: solid 1px; border-collapse: collapse;\">
<thead>
<tr>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Latest
  pre-2017a
		</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>2017a</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Comments</strong>
	</th>
</tr>
</thead>
<tbody>
<tr>
	<td style=\"border: solid 1pt;\">
		ABINIT/8.0.7-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Work in progress</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Advisor/2016_update4
	</td>
	<td style=\"border: solid 1pt;\">
		inteldevtools/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in a
  new module with the other Intel development tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\"><em>ANTs/2.1.0-foss-2015a</em>
	</td>
	<td style=\"border: solid 1pt;\">ANTs/2.2.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">Not yet available on Leibniz due to compile problems.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		augustus/3.0.1-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Autoconf/2.69-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Autoconf/2.69</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the system
  toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		AutoDock/1.1.2
	</td>
	<td style=\"border: solid 1pt;\">
		AutoDock_Vina/1.1.2
	</td>
	<td style=\"border: solid 1pt;\">
		Naming modified
  to the standard naming used in our build tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Automake/1.15-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Automake/1.15</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the system
  toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Autotools/20150215-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Autotools/2016123</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the system
  toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">BAli-Phy/2.3.8-intel-2017a-OpenMP<br>BAli-Phy/2.3.8-intel-2017a-MPI<br>
	</td>
	<td style=\"border: solid 1pt;\">By Ben Redelings, <a href=\"http://www.bali-phy.org/docs.php\">documentation</a> on <a href=\"http://www.bali-phy.org/\">the software web site</a>. This package supports either OpenMP or MPI, but not both together in a hybrid mode.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		beagle-lib/2.1.2-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		beagle-lib/2.1.2-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Beast/2.4.4-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Beast/2.4.5-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Version with beagle-lib
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Biopython/1.68-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		Biopython/1.68-intel-2017a-Python-2.7.13<br>Biopython/1.68-intel-2017a-Python-3.6.1
	</td>
	<td style=\"border: solid 1pt;\">
		Builds for Python
  2,7 and Python 3.6
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		bismark/0.13.1-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		Bismark/0.17.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Naming modified
  to the standard naming used in our build tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Bison/3.0.4-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Bison/3.0.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		BLAST+/2.6.0-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		BLAST+/2.6.0-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Boost/1.63.0-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		Boost/1.63.0-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Bowtie2/2.2.9-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		Bowtie2/2.2.9-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		byacc/20160606-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>byacc/20170201</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the system
  toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		bzip2/1.0.6-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		bzip2/1.0.6-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		cairo/1.15.2-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		cairo/1.15.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		CASINO/2.12.1-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		CASM/0.2.0-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on demand, compiler problems.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">CGAL/4.9-intel-2017a-forOpenFOAM
	</td>
	<td style=\"border: solid 1pt;\">Installed without the components that require Qt and/or OpenGL.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		CMake/3.5.2-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		CMake/3.7.2-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		CP2K/4.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">CP2K/4.1-intel-2017a-bare<br>CP2K/4.1-intel-2017a-bare-multiver<br>CP2K/5.1-intel-2017a-bare-multiver<br>CP2K-5.1/intel-2017a-bare-GPU-noMPI<br>
	</td>
	<td style=\"border: solid 1pt;\">The multiver modules contain the sopt, popt, ssmp and psmp binaries.<br>The bare-GPU version only works on a single GPU node, support for MPI was not included. It is a ssmp binary using GPU acceleration.</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		CPMD/4.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">CPMD/4.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\"><a href=\"/infrastructure/hardware/hardware-ua/licensed-software#cpmd\">CPMD is licensed software</a>.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		cURL/7.49.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		cURL/7.53.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">DIAMOND/0.9.12-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		DLCpar/1.0-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		DLCpar/1.0-intel-2017a-Python-2.7.13<br>DLCpar/1.0-intel-2017a-Python-3.6.1<br>
	</td>
	<td style=\"border: solid 1pt;\">
		Installed for
  Python 2.7.13 and Pyton 3.6.1
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Doxygen/1.8.11-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Doxygen/1.8.13</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the
  system toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		DSSP/2.2.1-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">DSSP/2.2.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\"><br>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Eigen/3.2.9-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Eigen/3.3.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		elk/3.3.17-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		Elk/4.0.15-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Naming modified
  to the standard naming used in our build tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		exonerate/2.2.0-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		Exonerate/2.4.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Naming modified
  to the standard naming used in our build tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		expat/2.2.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		expat/2.2.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">FastME/2.1.5.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		FFTW/3.3.4-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		FFTW/3.3.6-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		There is also a
  FFTW-compatible interface in intel/2017a, but it does not work for all
  packages.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		file/5.30-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		fixesproto/5.0-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		flex/2.6.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		flex/2.6.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		fontconfig/2.12.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		fontconfig/2.12.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		freeglut/3.0.0-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		freeglut/3.0.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Not yet
  operational on CentOS 7
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		freetype/2.7-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		freetype/2.7.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		FSL/5.0.9-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GAMESS-US/20141205-R1-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		gc/7.4.4-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		gc/7.6.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\"><i><br></i></td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GDAL/2.1.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		GDAL/2.1.3-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		Does
  not support Python 3.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		genometools/1.5.4-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		GenomeTools/1.5.9-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GEOS/3.5.0-intel-2015a-Python-2.7.9
	</td>
	<td style=\"border: solid 1pt;\">
		GEOS/3.6.1-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		Does
  not support Python 3.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		gettext/0.19.8-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		gettext/0.19.8.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GLib/2.48.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		GLib/2.49.7-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GMAP-GSNAP/2014-12-25-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		GMAP-GSNAP/2017-03-17-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GMP/6.1.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		GMP/6.1.2-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		gnuplot/5.0.0-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		gnuplot/5.0.6-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GObject-Introspection/1.44.0-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		GObject-Introspection/1.49.2-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GROMACS/5.1.2-intel-2016a-hybrid
	</td>
	<td style=\"border: solid 1pt;\">
		GROMACS/5.1.2-intel-2017a-hybrid<br>GROMACS/2016.3-intel-2017a<br>GROMACS/2016.4-intel-2017a-GPU-noMPI<br>
	</td>
	<td style=\"border: solid 1pt;\">The GROMACS -GPU-noMPI binary is a binary for the GPU nodes, without support for MPI, so it can only be used on a single GPU node.</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GSL/2.3-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		GSL/2.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">gtest/1.8.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">Google C++ Testing Framework
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Guile/1.8.8-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Guile/1.8.8-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Guile/2.0.11-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Guile/2.2.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		hanythingondemand/3.2.0-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		hanythingondemand/3.2.0-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		HarfBuzz/1.3.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		HDF5/1.8.17-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		HDF5/1.8.18-intel-2017a<br>HDF5/1.8.18-intel-2017a-noMPI
	</td>
	<td style=\"border: solid 1pt;\"><a href=\"https://support.hdfgroup.org/HDF5/\">HDF5</a> with and without MPI-support.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">HISAT2/2.0.5-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		HTSeq/0.6.1p1-intel-2016a-Python-2.7.11
	</td>
	<td style=\"border: solid 1pt;\">
		HTSeq/0.7.2-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		Does not support
  Python 3.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		icc/2016.3.210-GCC-5.4.0-2.26
	</td>
	<td style=\"border: solid 1pt;\">
		intel/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Intel compiler
  components in a single module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		iccifort/2016.3.210-GCC-5.4.0-2.26
	</td>
	<td style=\"border: solid 1pt;\">
		intel/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Intel compiler
  components in a single module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		ifort/2016.3.210-GCC-5.4.0-2.26
	</td>
	<td style=\"border: solid 1pt;\">
		intel/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Intel compiler
  components in a single module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		imkl/11.3.3.210-iimpi-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		intel/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Intel compiler
  components in a single module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		impi/5.1.3.181-iccifort-2016.3.210-GCC-5.4.0-2.26
	</td>
	<td style=\"border: solid 1pt;\">
		intel/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Intel compiler
  components in a single module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		inputproto/2.3.2-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Inspector/2016_update3
	</td>
	<td style=\"border: solid 1pt;\">
		inteldevtools/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in a
  new module with the other Intel development tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		ipp/8.2.1.133
	</td>
	<td style=\"border: solid 1pt;\">
		intel/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Intel compiler
  components in a single module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		itac/9.0.2.045
	</td>
	<td style=\"border: solid 1pt;\">
		inteldevtools/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in a
  new module with the other Intel development tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		JasPer/2.0.12-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">Julia/0.6.0-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\"><a href=\"https://julialang.org/\">Julia</a>, command line version (so without the Juno IDE).
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		kbproto/1.0.7-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		kwant/1.2.2-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">kwant/1.2.2-intel-2017a-Python-3.6.1
	</td>
	<td style=\"border: solid 1pt;\">Built with single-threaded libraries as advised in the documentation which implies that kwant is not exactly a HPC program.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		LAMMPS/14May16-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		LAMMPS/31Mar2017-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		libcerf/1.5-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libffi/3.2.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libffi/3.2.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		libgd/2.2.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Libint/1.1.6-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Libint/1.1.6-intel-2017a<br>Libint/1.1.6-intel-2017a-CP2K
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libint2/2.0.3-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libjpeg-turbo/1.5.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libjpeg-turbo/1.5.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libmatheval/1.1.11-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libmatheval/1.1.11-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libpng/1.6.26-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libpng/1.6.28-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libpthread-stubs/0.3-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\"><i>Installed on demand.</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libreadline/6.3-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libreadline/7.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		LibTIFF/4.0.6-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		LibTIFF/4.0.7-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libtool/2.4.6-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>libtool/2.4.6</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the
  system toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libunistring/0.9.6-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libunistring/0.9.7-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libX11/1.6.3-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libXau/1.0.8-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libxc/2.2.3-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libxc/3.0.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libxcb/1.12-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libXdmcp/1.1.2-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libXext/1.3.3-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libXfixes/5.0.1-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libXi/1.7.6-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libxml2/2.9.4-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libxml2/2.9.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libXrender/0.9.9-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libxslt/1.1.28-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">
		libxslt/1.1.29-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libxsmm/1.6.4-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		libxsmm/1.7.1-intel-2017a<br>libxsmm/1.8-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libyaml/0.1.6-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\"><em>Installed on demand</em>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">LLVM/3.9/.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\"><a href=\"https://llvm.org/\">LLVM compiler backend</a> with libLLVM.so.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		lxml/3.5.0-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
		Python/3.6.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 2.7 and 3.6 modules.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		M4/1.4.17-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>M4/1.4.18</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the
  system toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">MAFFT/7.312-intel-2017a-with-extensions
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MAKER-P/2.31.8-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MAKER-P-mpi/2.31.8-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		matplotlib/1.5.3-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a<br>Python/3.6.1-intel-2017a<br>
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 2.7 and 3.6 modules
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MCL/14.137-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		MCL/14.137-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		mdust/1.0-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		mdust/1.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		METIS/5.1.0-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		METIS/5.1.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MITE_Hunter/11-2011-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		molmod/1.1-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">molmod/1.1-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Work
  in progress, compile problems with newer compilers.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Mono/4.6.2.7-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Mono/4.8.0.495-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Mothur/1.34.4-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\"><em>Installed on demand</em>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MUMPS/5.0.1-intel-2016a-serial<br>MUMPS/5.0.0-intel-2015a-parmetis<br>
	</td>
	<td style=\"border: solid 1pt;\">
		MUMPS-5.1.1-intel-2017a-openmp-noMPI<br>MUMPS-5.1.1-intel-2017a-openmp-MPI<br>MUMPS-5.1.1-intel-2017a-noOpenMP-noMPI<br>
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MUSCLE/3.8.31-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		MUSCLE/3.8.31-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		NASM/2.12.02-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>NASM/2.12.02</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the systemtoolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		ncbi-vdb/2.8.2-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		ncurses/6.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		ncurses/6.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">NEURON/7.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\"><a href=\"https://neuron.yale.edu/neuron\">Yale NEURON code</a>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		netaddr/0.7.14-intel-2015a-Python-2.7.9
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 2.7 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		netCDF/4.4.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		netCDF/4.4.1.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		All netCDF
  interfaces integrated in a single module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		netCDF-Fortran/4.4.4-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		netCDF/4.4.1.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		All netCDF
  interfaces integrated in a single module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		netifaces/0.10.4-intel-2015a-Python-2.7.9
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 2.7 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		NGS/1.3.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		numpy/1.9.2-intel-2015b-Python-2.7.10
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 2.7 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		numpy/1.10.4-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">
		Python/3.6.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 3.6 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		NWChem/6.5.revision26243-intel-2015b-2014-09-10-Python-2.7.10
	</td>
	<td style=\"border: solid 1pt;\">
		NWChem/6.6.r27746-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		<i>On demand on Hopper.</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">OpenFOAM/4.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">Installed without the components that require OpenGL and/or Qt (which should only be in the postprocessing)
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		OpenMX/3.8.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		OpenMX/3.8.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">OrthoFinder/1.1.10-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		Pango/1.40.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		ParMETIS/4.0.3-intel-2015b
	</td>
	<td style=\"border: solid 1pt;\">
		ParMETIS/4.0.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		pbs-drmaa/1.0.18-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\"><em>Installed on demand</em>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		pbs_PRISMS/1.0.1-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		Python
  interfaces for Torque/PBS used by CASM
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		pbs_python/4.6.0-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		pbs_python/4.6.0-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		Python
  interfaces for Torque/PBS used by hanythingondemand
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		PCRE/8.38-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		PCRE/8.40-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Perl/5.20.1-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		Perl/5.24.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		pixman/0.34.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		pixman/0.34.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		pkg-config/0.29.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		<i>pkg-config/0.29.1</i>
	</td>
	<td style=\"border: solid 1pt;\">
		Moved to the
  system toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		PLUMED/2.3.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		PLUMED/2.3.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		PROJ/4.9.2-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		PROJ/4.9.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">protobuf/3.4.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\"><a href=\"https://developers.google.com/protocol-buffers/\">Google Protocol Buffers</a>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Pysam/0.9.1.4-intel-2016a-Python-2.7.11
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 2.7 module. Also load SAMtools to use.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Pysam/0.9.1.2-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">
		Python/3.6.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 3.6 module. Also load SAMtools to use.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Python/2.7.12-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Python/3.5.1-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		Python/3.6.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		QuantumESPRESSO/5.2.1-intel-2015b-hybrid
	</td>
	<td style=\"border: solid 1pt;\">QuantumESPRESSO/6.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Work in progress.</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		R/3.3.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		R/3.3.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		RAxML/8.2.9-intel-2016b-hybrid-avx
	</td>
	<td style=\"border: solid 1pt;\">RAxML/8.2.10-intel-2017a-hybrid
	</td>
	<td style=\"border: solid 1pt;\">We suggest users try RAxML-ng (still beta) which is supposedly much faster and better adapted to new architectures and can be installed on demand.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">RAxML-NG/0.4.1-intel-2017a-pthreads<br>
		RAxML-NG/0.4.1-intel-2017a-hybrid
	</td>
	<td style=\"border: solid 1pt;\"><a href=\"https://github.com/amkozlov/raxml-ng/wiki\">RAxML Next Generation beta</a>, compiled for shared memory (pthreads) and hybrid
distributed-shared memory (hybrid, uses MPI and pthreads).
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		R-bundle-Bioconductor/3.3-intel-2016b-R-3.3.1
	</td>
	<td style=\"border: solid 1pt;\">
		R/3.3.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard R module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		renderproto/0.11.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		RepeatMasker/4.0.5-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand; compiler problems.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		RMBlast/2.2.28-intel-2015a-Python-2.7.9
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand; compiler problems.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SAMtools/0.1.19-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		SAMtools/1.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		scikit-umfpack/0.2.1-intel-2015b-Python-2.7.10
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\"><em>Installed on demand</em>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		scikit-umfpack/0.2.1-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">scikit-umfpack/0.2.3-intel-2017a-Python-3.6.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		scipy/0.15.1-intel-2015b-Python-2.7.10
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 2.7 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		scipy/0.16.1-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">
		Python/3.6.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  the standard Python 3.6 module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SCons/2.5.1-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		SCons/2.5.1-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		<i>On demand on
  CentOS 7; also in the system toolchain.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SCOTCH/6.0.4-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">
		SCOTCH/6.0.4-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Siesta/3.2-pl5-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		Siesta/4.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SNAP/2013-11-29-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		spglib/1.7.4-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\"><em>Installed on demand</em>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SQLite/3.13.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		SQLite/3.17.0-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SuiteSparse/4.4.5-intel-2015b-ParMETIS-4.0.3
	</td>
	<td style=\"border: solid 1pt;\">SuiteSparse/4.5.5-intel-2015b-ParMETIS-4.0.3
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SuiteSparse/4.4.5-intel-2016a-METIS-5.1.0
	</td>
	<td style=\"border: solid 1pt;\">SuiteSparse/4.4.5-intel-2017a-METIS-5.1.0<br>SuiteSparse/4.5.5-intel-2017a-METIS-5.1.0<br>
	</td>
	<td style=\"border: solid 1pt;\">Older version as it is known to be compatible with our Python packages.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SWIG/3.0.7-intel-2015b-Python-2.7.10
	</td>
	<td style=\"border: solid 1pt;\">
		SWIG/3.0.12-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SWIG/3.0.8-intel-2016a-Python-3.5.1
	</td>
	<td style=\"border: solid 1pt;\">
		SWIG/3.0.12-intel-2017a-Python-3.6.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Szip/2.1-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Szip/2.1.1-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		tbb/4.3.2.135
	</td>
	<td style=\"border: solid 1pt;\">
		intel/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Intel compiler
  components in a single module.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Tcl/8.6.5-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Tcl/8.6.6-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		TELEMAC/v7p2r0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">TELEMAC/v7p2r0-intel-2017a<br>TELEMAC/v7p2r1-intel-2017a<br>TELEMAC/v7p2r2-intel-2017a<br>TELEMAC/v7p3r0-intel-2017a<br></td>
	<td style=\"border: solid 1pt;\"><br></td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		TINKER/7.1.3-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand; compiler problems.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Tk/8.6.5-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Tk/8.6.6-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		TopHat/2.1.1-intel-2016a
	</td>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">
		TopHat is no
  longer developed, its developers advise considering switching to 
		<a href=\"http://ccb.jhu.edu/software/hisat2/index.shtml\">HISAT2</a> which is more accurate and more efficient. It does not compile with the intel/2017a compilers.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">VASP
	</td>
	<td style=\"border: solid 1pt;\">VASP/5.4.4-intel-2016b<br>VASP/5.4.4-intel-2016b-vtst-173
	</td>
	<td style=\"border: solid 1pt;\">VASP has not been installed in the 2017a toolchain due to performance regressions and occasional run time errors with the Intel 2017 compilers and hence has been made available in the intel/2016b toolchain.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Voro++/0.4.6-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		Voro++/0.4.6-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		vsc-base/2.5.1-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		vsc-install/0.10.11-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		vsc-install/0.10.25-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
		Does not support
  Python 3.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		vsc-mympirun/3.4.3-intel-2016b-Python-2.7.12
	</td>
	<td style=\"border: solid 1pt;\">
		vsc-mympirun/3.4.3-intel-2017a-Python-2.7.13
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		VTune/2016_update3
	</td>
	<td style=\"border: solid 1pt;\">
		inteldevtools/2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in a
  new module with the other Intel development tools
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		worker/1.5.1-intel-2015a
	</td>
	<td style=\"border: solid 1pt;\">
		worker-1.6.7-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		X11/20160819-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		xcb-proto/1.12
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		xextproto/7.3.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		xorg-macros/1.19.0-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		xproto/7.0.29-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		xtrans/1.3.5-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		X11/20170129-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated in
  one large X11 module
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		XZ/5.2.2-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		XZ/5.2.3-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		zlib/1.2.8-intel-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		zlib/1.2.11-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
</tbody>
</table>"
789,"","<h3>Foss toolchain</h3>
<table style=\"border: solid 1px; border-collapse: collapse;\">
<thead>
<tr>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Latest pre-2017a</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>2017a</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Comments</strong>
	</th>
</tr>
</thead>
<tbody>
<tr>
	<td style=\"border: solid 1pt;\">
		ANTs/2.1.0-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\"><em>ANTs/2.2.0-intel-2017a</em>
	</td>
	<td style=\"border: solid 1pt;\">Moved to the Intel toolchain.<br>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		ATLAS/3.10.2-foss-2015a-LAPACK-3.4.2
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		CMake/3.5.2-foss-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		CMake/3.7.2-foss-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Cufflinks/2.2.1-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		cURL/7.41.0-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Cython/0.22.1-foss-2015a-Python-2.7.9
	</td>
	<td style=\"border: solid 1pt;\">
		Python/2.7.13-intel-2017a
	</td>
	<td style=\"border: solid 1pt;\">
		Integrated into
  the standard Python module for the intel toolchains
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		FFTW/3.3.4-gompi-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		FFTW/3.3.6-gompi-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GSL/2.1-foss-2015b
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		HDF5/1.8.14-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libpng/1.6.16-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		libreadline/6.3-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		makedepend/1.0.5-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MaSuRCA/2.3.2-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		ncurses/6.0-foss-2016b
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		pbs-drmaa/1.0.18-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Perl/5.20.1-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Python/2.7.9-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		Python is
  available in the Intel toolchain.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SAMtools/0.1.19-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		Newer versions
  with intel toolchain
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		SPAdes/3.10.1-foss-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		SPAdes/3.10.1-foss-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Szip/2.1-foss-2015a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		zlib/1.2.8-foss-2016b
	</td>
	<td style=\"border: solid 1pt;\">
		zlib/1.2.11-foss-2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
</tbody>
</table>
<h3>Gompi</h3>
<table style=\"border: solid 1px; border-collapse: collapse;\">
<thead>
<tr>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Latest
  pre-GCC-6.3.0 (2017a)
		</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>gompi-2017a</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Comments</strong>
	</th>
</tr>
</thead>
<tbody>
<tr>
	<td style=\"border: solid 1pt;\">
		ScaLAPACK/2.0.2-gompi-2016b-OpenBLAS-0.2.18-LAPACK-3.6.1
	</td>
	<td style=\"border: solid 1pt;\">
		ScaLAPACK/2.0.2-gompi-2017a-OpenBLAS-0.2.19-LAPACK-3.7.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
</tbody>
</table>
<h3>GCC</h3>
<table style=\"border: solid 1px;\">
<thead>
<tr>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Latest pre-gompi-2017a</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>GCC-6.3.0  (2017a)</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Comments</strong>
	</th>
</tr>
</thead>
<tbody>
<tr>
	<td style=\"border: solid 1pt;\">
		OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1
	</td>
	<td style=\"border: solid 1pt;\">
		OpenBLAS/0.2.19-GCC-6.3.0-2.27-LAPACK-3.7.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		numactl/2.0.11-GCC-5.4.0-2.26
	</td>
	<td style=\"border: solid 1pt;\">
		numactl/2.0.11-GCC-6.3.0-2.27
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		OpenMPI/1.10.3-GCC-5.4.0-2.26
	</td>
	<td style=\"border: solid 1pt;\">
		OpenMPI/2.0.2-GCC-6.3.0-2.27
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MPICH/3.1.4-GCC-4.9.2
	</td>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
</tbody>
</table>
<h3>GCCcore</h3>
<table style=\"border: solid 1px; border-collapse: collapse;\">
<thead>
<tr>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Latest
  pre-GCCcore-6.3.0  (2017a)
		</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>GCCcore-6.3.0 
  (2017a)
		</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Comments</strong>
	</th>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		binutils/2.26-GCCcore-5.4.0
	</td>
	<td style=\"border: solid 1pt;\">
		binutils/2.27-GCCcore-6.3.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		flex/2.6.0-GCCcore-5.4.0
	</td>
	<td style=\"border: solid 1pt;\">
		flex/2.6.3-GCCcore-6.3.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Lmod/7.0.5
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		Default
  module tool on CentOS 7
	</td>
</tr>
</thead>
</table>"
791,"","<table style=\"border: solid 1px; border-collapse: collapse;\">
<thead>
<tr>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Pre-2017</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Latest module</strong>
	</th>
	<th style=\"background-color:LightGrey; border: solid 1pt;\">
		<strong>Comments</strong>
	</th>
</tr>
</thead>
<tbody>
<tr>
	<td style=\"border: solid 1pt;\">
		ant/1.9.4-Java-8
	</td>
	<td style=\"border: solid 1pt;\">
		ant/1.10.1-Java-8
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		Autoconf/2.69
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		AutoDock_Vina/1.1.2
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		Automake/1.15
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		Autotools/2016123
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">/
	</td>
	<td style=\"border: solid 1pt;\">Bazel/0.5.3
	</td>
	<td style=\"border: solid 1pt;\"><a href=\"https://bazel.build/\">Google's software installer</a>. Not installed on the Scientific Linux 6 nodes of hopper.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		binutils/2.26
	</td>
	<td style=\"border: solid 1pt;\">
		binutils/2.27
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Bison/3.0.4
	</td>
	<td style=\"border: solid 1pt;\">
		Bison/3.0.4
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		BRATNextGen/20150505
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		byacc/20170201
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		CMake/3.7.2
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		core-counter/1.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		CPLEX/12.6.3
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		DFTB+/1.2.2
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		Doxygen/1.8.13
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		EasuBuild/…
	</td>
	<td style=\"border: solid 1pt;\">
		EasyBuild/3.1.2
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		FastQC/0.11.5-Java-8
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		FINE-Marine/5.2
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		flex/2.6.0<br>flex/2.6.3<br>
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		GATK/3.5-Java-8
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Gaussian16/g16_A3-AVX
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Work in progress.</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Gurobi/6.5.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Hadoop/2.6.0-cdh5.4.5-native
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed
  on demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		help2man/1.47.4
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Java/8
	</td>
	<td style=\"border: solid 1pt;\">
		Java/8
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		JUnit/4.12-Java-8
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		libtool/2.4.6
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		M4/1.4.17
	</td>
	<td style=\"border: solid 1pt;\">
		M4/1.4.18
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MATLAB/R2016a
	</td>
	<td style=\"border: solid 1pt;\">
		MATLAB/R2017a
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Maven/3.3.9
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MGLTools/1.5.7rc1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MlxLibrary/1.0.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		Lixoft Simulx
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		MlxPlore/1.1.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		Lixoft MLXPlore
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		monitor/1.1.2
	</td>
	<td style=\"border: solid 1pt;\">
		monitor/1.1.2
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Monolix/2016R1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		NASM/2.12.02
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Newbler/2.9
	</td>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		On request, has
  not been used recently.
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Novoalign/3.04.02
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		ORCA/3.0.3
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		p4vasp/0.3.29
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">
		<i>Installed on
  demand on Leibniz.
		</i>
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		parallel/20160622
	</td>
	<td style=\"border: solid 1pt;\">
		parallel/20170322
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		pkg-config/0.29.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		protobuf/2.5.0
	</td>
	<td style=\"border: solid 1pt;\">
		protobuf/2.6.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		Ruby/2.1.10
	</td>
	<td style=\"border: solid 1pt;\">
		Ruby/2.4.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		/
	</td>
	<td style=\"border: solid 1pt;\">
		SCons/2.5.1
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
</tr>
<tr>
	<td style=\"border: solid 1pt;\">
		scripts/4.0.0
	</td>
	<td style=\"border: solid 1pt;\">
	</td>
	<td style=\"border: solid 1pt;\">On request, has not been used recently.
	</td>
</tr><tr>
	<td style=\"border: solid 1pt;\">setuptools/1.4.2</td>
	<td style=\"border: solid 1pt;\"></td>
	<td style=\"border: solid 1pt;\">On request, has not been used recently.</td>
</tr><tr>
	<td style=\"border: solid 1pt;\">Spark/2.0.2</td>
	<td style=\"border: solid 1pt;\"></td>
	<td style=\"border: solid 1pt;\">On request, has not been used recently.</td>
</tr><tr>
	<td style=\"border: solid 1pt;\">TRF/4.07.b</td>
	<td style=\"border: solid 1pt;\"></td>
	<td style=\"border: solid 1pt;\">On request, has not been used recently.</td>
</tr><tr>
	<td style=\"border: solid 1pt;\">TRIQS/1.2.0</td>
	<td style=\"border: solid 1pt;\"></td>
	<td style=\"border: solid 1pt;\">On request, has not been used recently.</td>
</tr><tr>
	<td style=\"border: solid 1pt;\">viral-ngs/1.4.2</td>
	<td style=\"border: solid 1pt;\"></td>
	<td style=\"border: solid 1pt;\">On request, has not been used recently.</td>
</tr><tr>
	<td style=\"border: solid 1pt;\"></td>
	<td style=\"border: solid 1pt;\">vsc-base/2.5.1</td>
	<td style=\"border: solid 1pt;\">Used to be in compiler toolchains</td>
</tr>
</tbody>
</table>"
793,"","<h2>Introduction</h2><p>Most of the useful R packages come 
in the form of packages that can be installed separatly. Some of those 
are part of the default installtion on VSC infrastructure. Given the astounding number of packages, it is not sustainable to
 install each and everyone system wide. Since it is very easy for a user
 to install them just for himself, or for his research group, that is 
not a problem though. Do not hesitate to contact support whenever you 
encounter trouble doing so.
</p><h2>Installing your own packages using conda</h2><p>The easiest way to install and manage your own R environment is conda.
</p><h3>Installing Miniconda</h3><p> If you have Miniconda already installed, you can skip ahead to the next
section, if Miniconda is not installed, we start with that. Download the
Bash script that will install it from
	<a href=\"https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\" target=\"_blank\">conda.io</a> using, e.g., wget:
</p><pre>$ wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
</pre><p>Once downloaded, run the installation script:
</p><pre>$ bash Miniconda3-latest-Linux-x86_64.sh -b -p $VSC_DATA/miniconda3
</pre><p>Optionally, you can add the path to the   Miniconda 
installation to the PATH environment variable in your .bashrc file.  
This is convenient, but may lead to conflicts when working with the 
module system, so make sure that you know what you are doing in either 
case. The line to add to your .bashrc file would be:
</p><pre>export PATH=\"${VSC_DATA}/miniconda3/bin:${PATH}
</pre><h3>Creating an environment<br></h3><p>First, ensure that the 
Miniconda installation is in your PATH environment variable.  The 
following command should return the full path to the conda command:
</p><pre>$ which conda
</pre><p>If the result is blank, or reports that conda can not be found,
 modify the `PATH` environment variable appropriately by adding  
iniconda's bin directory to PATH.
</p><p>Creating a new conda environment is straightforward:
</p><pre>$ conda create -n science -c r r-essentials r-rodbc
</pre><p>This command creates a new conda environment called science, 
and installs a number of R packages that you will probably want to 
have handy in any case to preprocess, visualize, or postprocess your 
data. You can of course install more, depending on your requirements and
 personal taste.
</p><h3>Working with the environment</h3><p>To work with an environment, you have to activate it.  This is done with, e.g.,
</p><pre>$ source activate science
</pre><p>Here, science is the name of the environment you want to work in.
</p><h3>Install an additional package</h3><p>To install an additional package, e.g., `pandas`, first ensure that the environment you want to work in is activated.
</p><pre>$ source activate science
</pre><p>Next, install the package:
</p><pre>$ conda install -c r r-ggplot2
</pre><p>Note that conda will take care of all independencies, including
 non-R libraries. This
 ensures that you work in a consistent environment.
</p><h3>Updating/removing</h3><p>Using conda, it is easy to keep your packages up-to-date. Updating a single package (and its dependencies) can be done using:
</p><pre>$ conda update r-rodbc
</pre><p>Updating all packages in the environement is trivial:
</p><pre>$ conda update --all
</pre><p>Removing an installed package:
</p><pre>$ conda remove r-mass
</pre><h3>Deactivating an environment</h3><p>To deactivate a conda environment, i.e., return the shell to its original state, use the following command
</p><pre>$ source deactivate
</pre><h3>More information</h3><p>Additional information about conda can be found on its <a href=\"https://conda.readthedocs.io/en/latest/\" target=\"_blank\">documentation site</a>.
</p><h2>Alternatives to conda
</h2><p>Setting up your own package repository for R is straightforward.
</p><ol>
	<li>Load the appropriate R module, i.e., the one you want the R package to be available for:<br>
	<code>$ module load R/3.2.1-foss-2014a-x11-tcl</code></li>
	<li>Start R and install the package :<br><code>&gt; install.packages(\"DEoptim\")</code></li>
	<li>Alternatively you can download the
desired package:
	<br>
	<code>$ wget cran.r-project.org/src/contrib/Archive/DEoptim/DEoptim_2.0-0.tar.gz</code></li>
	And install the package from the command line:
	<code>$ R CMD
INSTALL DEoptim_2.2-3.tar.gz  -l
/$VSC_HOME/R/
	</code>
	<li>These packages might depend on the specific R version, so you may need to reinstall them for the other version.</li>
</ol>"
795,"","<p>The 4th VSC Users Day was held at the \"Paleis der Academiën\", the seat of the \"<a href=\"https://www.vscentrum.be/events/userday-2017/venue\">Royal Flemish Academy of Belgium for Science and the Arts</a>\", in the Hertogstraat 1, 1000 Brussels, on May 22, 2018.
</p><h2>Program</h2><p>The titles in the program link to slides or abstracts of the presentations.
</p><ul>
	<li>9u50 : Welcome</li>
	<li>10u00: <a href=\"https://www.researchgate.net/publication/325285871_Ultra_scalable_Algorithms_for_Complex_Flows\">“Ultrascalable algorithms for complex flows” – Ulrich Rüde, CERFACS and Universitaet Erlangen-Nuernberg</a> </li>
	<li>11u00: Coffee break</li>
	<li>11u25: Workshop sessions part 1 – VSC staff<br>
	<ul>
		<li>Start to VSC</li>
		<li><a href=\"/assets/1361\">Start to GPU</a></li>
		<li><a href=\"/assets/1363\">Code optimization</a></li>
	</ul></li>
	<li>12u15: <a href=\"/events/userday-2018/posters\">1 minute poster presentations</a></li>
	<li>12u45: Lunch</li>
	<li>13h30: <a href=\"/events/userday-2018/posters\">Poster session</a></li>
	<li>14u15: <a href=\"/assets/1359\">“Processing Genomics Data: High Performance Computing meets Big Data” – Jan Fostier, UGent</a> </li>
	<li>14u45: Workshop sessions part 2 – VSC staff<br>
	<ul>
		<li>Start to VSC</li>
		<li><a href=\"/assets/1361\">Start to GPU</a></li>
		<li><a href=\"/assets/1365\">Code debugging</a></li>
	</ul></li>
	<li>15u35: Coffee break</li>
	<li>15u55: “Why HPC and artificial intelligence engineering go hand in hand” – Joris Coddé, CTO Diabatix</li>
	<li>16u15: Tier-1 supercomputing platform as a service – </li>
	<li>16u35: Poster award and closing </li>
	<li>16u45: Reception</li>
	<li>18u: end</li>
</ul><h3>Abstracts of workshops</h3><p><i><strong>VSC for starters</strong></i>
</p><p>The workshop provides a smooth introduction to supercomputing for new users. Starting from common concepts in personal computing the similarities and differences with supercomputing are highlighted and some essential terminology is introduced. It is explained what users can expect from supercomputing and what not, as well as what is expected from them as users.
</p><p><i><strong>Start to GPU</strong></i>
</p><p>GPU’s have become an important resource of computational power. For some workloads they are extremely suited eg. Machine learning frameworks, but also applications vendors are providing more and more support. So it is important to keep track of things happening in your research field. This workshop will provide you with an overview of available GPU power within VSC and will give you guidelines how you can start using it.
</p><p><i><strong>Code debugging</strong></i>
</p><p>All code contains bugs, and that is frustrating. Trying to identify and eliminate them is tedious work. The extra complexity in parallel code makes this even harder.  However, using coding best practices can reduce the number of bugs in your code considerably, and using the right tools for debugging parallel code will simplify and streamline the process of fixing your code.  Familiarizing yourself with best practices will give you an excellent return on investment.
</p><p><i><strong>Code optimization</strong></i>
</p><p>Performance is a key concern in HPC (High Performance Computing). As a developer, but also as an application user you have to be aware of the impact of modern computer architecture on the efficiency of you code. Profilers can help you identify performance hotspots so that you can improve the performance of your code systematically.  Profilers can also help you to find the limiting factors when you run an application, so that you can improve your workflow to try and overcome those as much as possible.
</p><p>Paying attention to efficiency will allow you to scale your research to higher accuracy and fidelity.</p>"
797,"","<ol>
	<li><em></em><em>Doping Diamond with Luminescent Centres: The Electronic Structure of Ge and Eu Defect Complexes</em> <br>Danny E. P. Vanpoucke, Shannon S. Nicley, Emilie Bourgeois, Milos Nesladek, Ken Haenen (U Hasselt)</li>
	<li><em>Impact of observed and future climate change on agriculture and forestry in Central Asia</em> <br>Rozemien De Troch, Steven Caluwaerts, Lesley De Cruz, Piet Termonia and Philippe De Maeyer (U Gent en Koninklijk Meteorologisch Instituut - Institut Royal Météorologique)</li>
	<li>
	<em>Do droughts self-propage and self-intensify?</em><br>Jessica Keune, Hendrik Wouters (U Gent)</li>
	<li><em>Combining Multigrid and Multilevel Monte Carlo with Applications to Uncertainty Quantification</em><br>Pieterjan Robbe, Dirk Nuyens, Stefan Vandewalle (KU Leuven) </li>
	<li><em>Reference Assisted Assembly and Annotation of the Octopus vulgaris Genome</em><br>Koen Herten, Gregory Maes, Eve Seuntjes, Fiorito Graziano, Joris R Vermeesch (KU Leuven)</li>
	<li><em>Going where the wind blows – Aeroelastic simulations of a wind turbine with composite blades</em><br>Gilberto Santo, Mathijs Peeters, Wim Van Paepegem, Joris Degroote (U Gent) </li>
	<li><em>Tailoring superconductivity in lithium-decorated graphene</em>
	<br>	
	Annelinde Strobbe, Jonas Bekaert, Milorad Milošević (U Antwerpen) 
	</li>
	<li><em>Calculating terrain parameters from Digital Elevation Models on multicore processors </em><br>Grethell Castillo Reyes, Dirk Roose (UCI, Havana and KU Leuven)</li>
	<li><em>HPC4Business: Predicting Churn in Telco from Very Large Graphs using Representation Learning</em><br>Sandra Mitrović, Jochen De Weedt (KU Leuven)</li>
	<li>Machine learning and materials science: from ab initio screening to microstructure analysis<br>
	Michiel Larmuseau, Maarten Cools-Ceuppens, Michael Sluydts, Toon Verstraelen, Tom Dhaene, Stefaan Cottenier (U Gent and OCAS) </li>
	<li><em>Generating climate forcing for the Ecotron experiment using HPC
	</em><br>	
	Inne Vanderkelen, F. Rineau, E. Davin, L. Gudmundsson, J. Zscheischler, S. I. Seneviratne, W. Thiery  (VUB, U Hasselt and ETH Zurich) 
	</li>
	<li><em>A hybridized DG method for unsteady flow problems </em><br>Alexander Jaust, Jochen Schütz (U Hasset) </li>
	<li>
	Aromatic sulfonation with SO3: mechanistic and kinetic study
	<br>	
	 Samuel Moors, Xavier Deraet, Guy Van Assche, Paul Geerlings, Frank De Proft (VUB) 
	</li>
	<li>
	<em>Understanding ambident nucleophilicity: a combined activation-strain and conceptual DFT analysis</em>
	<br>	
	Tom Bettens, Trevor A. Hamlin, Mercedes Alonso, F. Matthias Bickelhaupt, Frank De Proft (VUB) 
	</li>
	<li><em>Computational fluid dynamics-based study of novel technologies in the steam cracking process</em>
	<br>	
	Stijn Vangaever, Jens N. Dedeyne, Pieter A. Reyniers, Guy B. Marin, Geraldine J. Heynderickx, Kevin M. Van Geem (U Gent) 
	</li>
	<li><em>HPC for regional climate simulations over Antarctica</em>
	<br>	
	Alexandra Gossart, Niels Souverijns, Matthias Demuzere,  Sam Vanden Broucke, Nicole P.M. van Lipzig (KU Leuven) 
	</li>
	<li><em>Materials microstructure simulation</em><br>Yuri Coutinho, Nele Moelans (KU Leuven)
	<br> </li>
	<li><em>SP-Wind: A scalable large-eddy simulation code for modeling and optimization of wind energy systems</em><br>Wim Munters, Athanasios Vitsas, Thomas Haas, Johan Meyers (KU Leuven) </li>
	<li><em>Simulation of atmospheric flows and their interaction with classical and airborne wind energy systems</em>
	<br>	
	Dries Allaerts, Thomas Haas, Johan Meyers (KU Leuven) 
	</li>
	<li><em>LES based control of wind farms</em>
	<br>	
	Pieter Bauweraerts, Wim Munters, Johan Meyers (KU Leuven) 
	</li>
	<li>
	<em>Surge resistance identification of inland vessels by Computational Fluid Dynamics</em>
	<br>	
	 Arne Eggers, Gerben Peeters (KU Leuven) 
	</li>
	<li><em>Analyzing the epidemic size distributions of an individual-based influenza model</em>
	<br>	
	Pieter Libin, Kristof Theys, Ann Nowé (VUB and KU Leuven) 
	</li>
	<li><em>Studying the adaptation of complex biomolecular systems through mechanistic modeling and in silico evolution</em>
	<br>	
	Jayson Gutiérrez, Steven Maere (VIB-UGent Center for Plant Systems Biology and U Gent) 
	</li>
	<li><em>Fraction of virus individuals with beneficial alleles affects the trajectory of a selective sweep</em>
	<br>	
	Abbas Jariani,  Pieter Libin, Kristof Theys (VIB, VUB and KU Leuven)
	</li>
	<li><em>The ESA Virtual Space Weather Modeling Centre</em><br>S. Poedts, A. Kochanov, A. Lani (KU Leuven), H. Deconinck (VKI), N. Mihalache A. Lani & F. Diet (SAS), D. Heynderickx (DH Consultancy), J. De Keyser, E. De Donder, N.B. Crosby, M. Echim (BISA), L. Rodriguez, R. Vansina, F. Verstringe , B. Mampaey (ROB), R. Horne, S. Glauert, J. Isles (BAS), P. Jiggens, R. Keil, A. Glover, J.-P. </li>
	<li><em>Forecasting space weather with EUHFORIA in ESA’s Virtual Space Weather Modeling Centre</em><br>
	S. Poedts, A. Lani, A. Kochanov, Ch. Verbeke, C. Scolini, A. Isavnin, N. Wijsen - CmPA / KU Leuven<br>J. Pomoell, E. Kilpua, E. Asvestari, E. Lumme - University of Helsinki, Helsinki, Finland
	</li>
	<li><em>Cheese brines harbour both halophilic/halotolerant and cheese ingredient-associated microorganisms</em><br>Louise Vermote, Marko Verce, Luc De Vuyst Stefan Weckx (VUB) </li>
	<li><em>Multiscale Climate Modelling over Africa
	</em><br>	O. Brousse, J. Van de Walle, W. Thiery, H. Wouters, M. Demuzerre, N. P.M. van Lipzig (KU Leuven) </li>
	<li><em>Structural and electronic properties of Naples Yellow pigments
	</em><br>	
	R. Saniz, D. Lamoen, A. Martchetti, K. De Wael, B. Partoens (U Antwerpen) 
	</li>
	<li><em>Hard X-ray sources in solar flares: K-H instability and turbulence
	</em><br>	
	Wenzhi Ruan, Chun Xia, Rony Keppens (KU Leuven) 
	</li>
	<li><em>A Cost-Efficient Workflow for the Whole-Transcriptome Analysis of Xenograft-Derived Tissue
	</em><br>	
	Álvaro Cortés-Calabuig, Magali Verheecke, Vanessa Brys, Jeroen Van Houdt, Frederic Amant, Joris Vermeesch  (KU Leuven)
	</li>
	<li><em>Validation of GenOme Resolution by Density-gradient Isopycnic ANalysis (GORDIAN) for sequence-based microbial community analysis
	</em><br>	
	Sofie Thijs, Nathan Bullen, Sarah Coyotzi, Jaco Vangronsveld, William Holben, Laura Hug, Josh Neufeld (U Hasselt, Waterloo Univ., Univ. Montana)
	</li>
	<li>
	The composition and functional potential of water kefir fermentation microbiota as revealed through shotgun metagenomics       Marko Verce, Luc De Vuyst, and Stefan Weckx (VUB)
	
	<br>
	</li>
</ol>"
